{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 1\n",
    "\n",
    "# Ранжирование вопросов StackOverflow с помощью векторных представлений слов\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: Смирнов Александр Львович\n",
    "\n",
    "## Внимание! Эта версия задания для тех, кто посещает курс в МФТИ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "В этом задании вы научитесь вычислять близость текстов и применить этот метод для поиска похожих вопросов на [StackOverflow](https://stackoverflow.com).\n",
    "\n",
    "### Используемые библиотеки\n",
    "\n",
    "В данном задании потребуются следующие библиотеки:\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — инструмент для решения различных задач NLP (тематическое моделирование, представление текстов, ...).\n",
    "- [Numpy](http://www.numpy.org) — библиотека для научных вычислений.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — библилиотека с многими реализованными алгоритмами машинного обучения для анализа данных.\n",
    "- [Nltk](http://www.nltk.org) — инструмент для работы с естественными языками.\n",
    "- [Pytorch](https://pytorch.org/) — инструмент для обучения нейросетей.\n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "Данные лежат в архиве `StackOverflowData.zip`, который состоит из:\n",
    "- `train.tsv` - обучающая выборка. В каждой строке через табуляцию записаны дублирующие друг друга предложения;\n",
    "- `test.tsv` - тестовая выборка. В каждой строке через табуляцию записаны: *<вопрос>, <похожий вопрос>, <отрицательный пример 1>, <отрицательный пример 2>, ...*\n",
    "\n",
    "Скачать архив можно здесь: [ссылка на google диск](https://drive.google.com/open?id=1QqT4D0EoqJTy7v9VrNCYD-m964XZFR7_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import TaskTests\n",
    "\n",
    "task_tests = TaskTests.from_json(path='test_gt.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вектора слов\n",
    "\n",
    "Для решения вам потребуются предобученная модель векторных представлений слов. Используйте [модель эмбеддингов](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit), которая была обучена с помощью пакета word2vec на данных Google News (100 миллиардов слов). Модель содержит 300-мерные вектора для 3 миллионов слов и фраз. Вы можете скачать их, запустив блок кода ниже."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from download_utils import download_google_vectors\n",
    "\n",
    "\n",
    "download_google_vectors(target_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Предобученные векторные представления слов (2 балла)\n",
    "\n",
    "Скачайте предобученные вектора и загрузите их с помощью функции [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) библиотеки Gensim с параметром *binary=True*. Если суммарный размер векторов больше, чем доступная память, то вы можете загрузите только часть векторов, указав параметр *limit* (рекомендуемое значение: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как пользоваться этими векторами?\n",
    "\n",
    "Как только вы загрузите векторные представления слов в память, убедитесь, что имеете к ним доступ. Сначала вы можете проверить, содержится ли какое-то слово в загруженных эмбедингах:\n",
    "\n",
    "    'word' in wv_embeddings\n",
    "\n",
    "Затем, чтобы получить соответствующий вектор, вы можете использовать оператор доступа по ключу:\n",
    "\n",
    "    wv_embeddings['word']\n",
    "\n",
    "### Проверим, корректны ли векторные представления\n",
    "\n",
    "Чтобы предотвратить возможные ошибки во время первого этапа, можно проверить, что загруженные вектора корректны. Для этого проверьте три пункта:\n",
    "1. Используя метод `.most_similar(positive=..., negative=...)`, найти слово, похожее на `woman`, `king` и непохожее на `man`.\n",
    "2. Используя метод `.doesnt_match(...)`, найти \"белую ворону\" в списке `['breakfast, 'dinner', 'lunch', 'cereal']`.\n",
    "3. Используя метод `.most_similar_to_given(word, [...])`, найти наиболее похожее на `music` слово из списка `['water', 'sound', 'backpack', 'mouse']`.\n",
    "\n",
    "Прокомментируйте полученные результаты: считаете ли вы их верными и почему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is 'kek' exists in embeddings: False\n",
      "\n",
      "is 'lord' exists in embeddings: True\n",
      "\n",
      "words like ['woman', 'king'] and opposite to ['man']:\n",
      "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951)]\n",
      "\n",
      "black sheep word in a ['breakfast', 'dinner', 'lunch', 'cereal'] list:\n",
      "cereal\n",
      "\n",
      "most similar word to 'music' from ['water', 'sound', 'backpack', 'mouse'] list:\n",
      "sound\n"
     ]
    }
   ],
   "source": [
    "word = \"kek\"\n",
    "print(f\"is '{word}' exists in embeddings: {word in wv_embeddings}\\n\")\n",
    "word = \"lord\"\n",
    "print(f\"is '{word}' exists in embeddings: {word in wv_embeddings}\\n\")\n",
    "\n",
    "positive = [\"woman\", \"king\"]\n",
    "negative = [\"man\"]\n",
    "print(f\"words like {positive} and opposite to {negative}:\\n{wv_embeddings.most_similar(positive=positive, negative=negative)[:3]}\\n\")\n",
    "\n",
    "black_sheep = ['breakfast', 'dinner', 'lunch', 'cereal']\n",
    "print(f\"black sheep word in a {black_sheep} list:\\n{wv_embeddings.doesnt_match(black_sheep)}\\n\")\n",
    "\n",
    "word = \"music\"\n",
    "candidates = ['water', 'sound', 'backpack', 'mouse']\n",
    "print(f\"most similar word to '{word}' from {candidates} list:\\n{wv_embeddings.most_similar_to_given(word, candidates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загруженные вектора корректны, так как действительно отражают семантические связи слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ранжирование вопросов StackOverflow\n",
    "\n",
    "Давайте посмотрим на данные, которые мы будем использовать в рамках задания. Выборка уже разбита на обучающую и тестовую. Все файлы используют табуляцию в качестве разделителя, но они имеют разный формат:\n",
    "\n",
    "- *обучающая* выборка (train.tsv) содержит похожие друг на друга предложения в каждой строке;\n",
    "- *тестовая* выборка (validation.tsv) содержит в каждой строке: *вопрос, похожий вопрос, отрицательный пример 1, отрицательный пример 2, ...*\n",
    "\n",
    "Считайте тестовую (валидационную) выборку. Ответьте на следующие вопросы:\n",
    "1. Сколько пар-дубликатов предоставлено в выборке?\n",
    "2. Сколько в среднем на каждую пару предоставлено отрицательных примеров?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data\n",
    "\n",
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = len(validation)\n",
    "amount_of_negatives_per_sample = np.mean(list(map(len, validation))) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_validation_corpus(\n",
    "    num_samples,\n",
    "    amount_of_negatives_per_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторные представления текста\n",
    "\n",
    "Чтобы перейти от отдельных слов к векторным представлениям вопросов, предлагается подсчитать **среднее** векторов всех слов в вопросе. Если для какого-то слова нет предобученного вектоора, то его нужно пропустить. Если вопрос не содержит ни одного известного слова, то нужно вернуть нулевой вектор.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from numpy.linalg import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Embedder:\n",
    "    \n",
    "    def __init__(self, embeddings, dim):\n",
    "        \"\"\"\n",
    "            embeddings: word2vec эмбеддинги\n",
    "            dim: размерность word2vec эмбеддингов. Нужна для задания нулего вектора для пустых вопросов\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.dim = dim\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def __call__(self, text, normalize=False):\n",
    "        \"\"\"\n",
    "            Принимает на вход текст и преобразует его в вектор.\n",
    "            \n",
    "            text: строка с вопросом\n",
    "            normalize: при True нужно перед возвращением нормализовать вектор\n",
    "            \n",
    "            returns: вектор вопроса\n",
    "        \"\"\"\n",
    "        vector = np.zeros(self.dim)\n",
    "    \n",
    "        #text_tokenized = self.tokenizer.tokenize(text.lower())\n",
    "        text_tokenized = text.split(\" \")\n",
    "        phrase_vectors = [self.embeddings[word] for word in text_tokenized if word in self.embeddings]\n",
    "        \n",
    "        if phrase_vectors:\n",
    "            vector = np.mean(phrase_vectors, axis=0)\n",
    "        \n",
    "        if normalize:\n",
    "            vector = self.scaler.fit_transform(vector.reshape(-1, 1))\n",
    "        \n",
    "        return vector.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(wv_embeddings, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_embedder(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть метод для создания векторного представления любого предложения. Оценим, как будет работать это решение.\n",
    "\n",
    "### Оценка близости текстов\n",
    "\n",
    "В качестве метрики схожести вопросов будем использовать косинусную близость.\n",
    "\n",
    "В валидационном датасете для каждой пары вопросов-дубликатов у нас есть случайные отрицательные примеры. Для каждого триплета (вопрос, дубликат, отрицательные примеры) будет ранжировать с помощью нашей модели и косинусной близости дубликат и отрицательные примеры и смотреть на позицию дубликата.\n",
    "\n",
    "#### Hits@K\n",
    "Довольно простой и легко интерпретируемой метрикой будет количество корректных попаданий дубликата в top \"выдачи\" для какого-то *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)],$$\n",
    "где $q_i$ - $i$-ый вопрос, $dup_i$ - его дубликат, $topK(q_i)$ - первые *K* элементов в ранжированном списке, который выдает наша модель.\n",
    "\n",
    "#### Пример оценок\n",
    "\n",
    "Пусть $N = 1$, вопрос $q_1$ это \"Что такое python\", а его дубликат $dup_1$ это \"Что такое язык python\". Пусть модель выдала следующий ранжированный список кандидатов:\n",
    "\n",
    "1. *\"Как узнать с++\"*\n",
    "2. *\"Что такое язык python\"*\n",
    "3. *\"Хочу учить Java\"*\n",
    "4. *\"Не понимаю Tensorflow\"*\n",
    "\n",
    "Вычислим метрику *Hits@K* для *K = 1, 4*:\n",
    "\n",
    "- [K = 1] $\\text{Hits@1} =  [dup_1 \\in top1(q_1)] = 0$\n",
    "- [K = 4] $\\text{Hits@4} =  [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "#### Подсчет метрики Hits@k сразу для нескольких k\n",
    "\n",
    "Чтобы посчитать метрику для нескольких k, не нужно повторно ранжировать нашей моделью вопросы для одного и того же сэмпла. Достаточно посчитать для сэмпла количество **сложных негативов** - отрицательных примеров, оказавшихся в выдаче выше, чем дубликат. Тогда\n",
    "$$Hits@k = \\begin{cases}\n",
    "    1, & N < k \\\\\n",
    "    0, & иначе\n",
    "   \\end{cases},$$\n",
    "где **N** - количество сложных негативов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте подсчет Hits@k для произвольного набора значений k и заданной валидационной выборки, используя предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "    \n",
    "class Scorer:\n",
    "    \n",
    "    def __init__(self, k, embedder):\n",
    "        \"\"\"\n",
    "            k: список значений k, для которых нужно посчитать hits@k\n",
    "            embedder: объект класса Embedder, умеющий преобразовать текст в вектор\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.embedder = embedder\n",
    "        \n",
    "    def _get_hard_negatives(self, q, pos, negs):\n",
    "        \"\"\"\n",
    "            q: текст вопроса\n",
    "            pos: текст дубликата\n",
    "            negs: список из текстов случайных вопросов\n",
    "            \n",
    "            result: количество сложных отрицательных примеров, оказавшихся выше положительного\n",
    "        \"\"\"\n",
    "        q_embedding = self.embedder(q)\n",
    "        pos_embedding = self.embedder(pos)\n",
    "        negs_embeddings = list(map(self.embedder, negs))\n",
    "        \n",
    "        sim_to_pos = cosine_similarity(q_embedding, pos_embedding)\n",
    "        sims_to_negs = np.array(list(map(lambda neg: cosine_similarity(q_embedding, neg), negs_embeddings)))\n",
    "        \n",
    "        result = np.sum(sims_to_negs > sim_to_pos)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def __call__(self, samples, verbose=False):\n",
    "        \"\"\"\n",
    "            samples: список из списков вида [q, pos, neg1, neg2, ...]. Наша валидационная выборка\n",
    "            verbose: выводить progressbar подсчета метрики с помощью tqdm\n",
    "            \n",
    "            result: словарь вида {k: hits@k}\n",
    "        \"\"\"\n",
    "        result = dict(zip(self.k, [0] * len(self.k)))\n",
    "        \n",
    "        for sample in tqdm(samples, disable=not verbose):\n",
    "            hard_negs = self._get_hard_negatives(sample[0], sample[1], sample[2:])\n",
    "            for k_i in self.k:\n",
    "                result[k_i] += k_i > hard_negs\n",
    "            \n",
    "        for k_i in self.k:\n",
    "            result[k_i] /= len(samples)\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 689/3760 [02:20<10:26,  4.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6afa4ea60e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8f29e1393b33>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, samples, verbose)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mhard_negs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_hard_negatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mk_i\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mhard_negs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8f29e1393b33>\u001b[0m in \u001b[0;36m_get_hard_negatives\u001b[0;34m(self, q, pos, negs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msim_to_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msims_to_negs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegs_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims_to_negs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msim_to_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8f29e1393b33>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(neg)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msim_to_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msims_to_negs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegs_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims_to_negs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msim_to_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/base/lib/python3.9/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/base/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/base/lib/python3.9/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    144\u001b[0m                             estimator=estimator)\n\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         X = check_array(X, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[0m\u001b[1;32m    147\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                         estimator=estimator)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/base/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/base/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n",
      "\u001b[0;32m~/.local/share/virtualenvs/base/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;34m\"\"\"Return number of samples in array-like x.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Expected sequence or array-like, got %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Don't get num_samples from an ensembles length!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "hits = scorer(validation, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_scorer(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текста\n",
    "\n",
    "Как вы могли заметить, мы имеем дело с сырыми данными. Это означает, что там присутствует много опечаток, спецсимволов и заглавных букв. В нашем случае это все может привести к ситуации, когда для данных токенов нет предобученных векторов. Поэтому необходима предобработка.\n",
    "\n",
    "Вам требуется:\n",
    "- Перевести символы в нижний регистр;\n",
    "- Заменить символы пунктуации и всевозможные плохие символы на пробелы;\n",
    "- Удалить стопслова.\n",
    "- Удалить слова с длиной меньше трех букв\n",
    "\n",
    "Реализуйте предобработку текста, используя предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "    \n",
    "    \n",
    "class TextPreprocessor:\n",
    "    \n",
    "    def __init__(self, characters, min_word_length=0, stopwords=None):\n",
    "        \"\"\"\n",
    "            characters: список плохих символов\n",
    "            min_word_length: минимальная допустимая длина для слов\n",
    "            stopwords: множество фоновых слов\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.min_word_length = min_word_length\n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "        self.pattern = '[' + re.escape(''.join(self.characters)) + ']'\n",
    "        self.tokenizer = RegexpTokenizer('[a-z0-9+-]+')\n",
    "        self.characters_table = str.maketrans(' ', ' ', \"\".join(characters))\n",
    "        \n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: текст для обработки\n",
    "            \n",
    "            returns: обработанный текст\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "       # text = text.translate(self.characters_table)\n",
    "        \n",
    "        text = re.sub(self.pattern, ' ', text)\n",
    "        #text = self.tokenizer.tokenize(text)\n",
    "        #text = [word for word in text if word not in self.stopwords]\n",
    "        text = [word for word in text.split() if word not in self.stopwords]\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "        #text = ' '.join([word for word in text.lower().split() if word not in self.stopwords])\n",
    "        #text = text.translate(self.characters_table)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TextPreprocessor(\n",
    "    characters=('?', '.', '-', ':'),\n",
    "    stopwords={'not', 'and', 'or'},\n",
    "    min_word_length=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_text_preprocessor(TextPreprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Множество фоновых слов можно взять из **nltk** с помощью `nltk.corpus.stopwords.words`, выкидываемые плохие символы и пунктуацию следует подобрать самостоятельно.\n",
    "\n",
    "Обработайте текст и продемонстрируйте улучшение качества:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/furiousteabag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 3760/3760 [01:02<00:00, 60.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "    \n",
    "preprocessor = TextPreprocessor(\n",
    "    characters=string.punctuation,\n",
    "    min_word_length=1,\n",
    "    stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    ")\n",
    "\n",
    "validation_preprocessed = [[preprocessor(text) for text in row] for row in tqdm(validation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "hits = scorer(validation_preprocessed, verbose=True)\n",
    "\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из критериев получения полных баллов является значение **hits@500** $\\geqslant 0.82$ до предобработки текста и $\\geqslant 0.85$ после предобработки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Представления для неизвестных слов. (8 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, что получить представления для неизвестного слова, воспользуемся следующим подходом:\n",
    "    \n",
    "1. Будем восстанавливать эмбеддинг неизвестного слова как сумму эмбеддингов буквенных триграмм. Например, слово where должно представляться суммой триграмм #wh, whe, her, ere, re#\n",
    "\n",
    "2. В качестве обучающих данных будем использовать слова, для которых есть эмбеддинг в модели. Будем обучать эмбеддинги триграмм по выборке эмбеддингов с помощью функционала MSE:\n",
    "\n",
    "$$L = \\sum_{w \\in W_{known}}\\| f_{\\theta}(w) - v_w \\|^2 \\to \\min_{\\theta}$$\n",
    "\n",
    "где:\n",
    "\n",
    "* $W_{known}$ — множество известных модели слов\n",
    "* $f_{\\theta}(w)$ — сумма эмбеддингов триграмм слова $w$\n",
    "* $v_w$ — эмбеддинг слова $w$\n",
    "* $\\theta$ — веса эмбеддингов триграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание триграммного токенизатора\n",
    "\n",
    "Для начала, нам нужно:\n",
    "1. Пройтись по известным в word2vec словам и составить множество триграмм, для которых будем обучать векторы\n",
    "2. Составить маппинг из триграмм в индексы\n",
    "3. Реализовать преобразование произвольного слова в список триграмм\n",
    "4. Реализовать преобразование произвольного слова в список индексов триграмм\n",
    "\n",
    "Для реализации всех этих пунктов предлагается использовать шаблон, приведенный ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramTokenizer:\n",
    "    \n",
    "    def __init__(self, words):\n",
    "        \"\"\"\n",
    "            Формируем множество всевозможных триграмм, встречающихся в словах из words.\n",
    "            Делаем маппинг триграмм в индексы.\n",
    "            \n",
    "            words: список слов\n",
    "        \"\"\"\n",
    "        self.words = words\n",
    "        \n",
    "        self.trigrams = []\n",
    "        for word in words:\n",
    "            self.trigrams.append(self._get_trigrams(word))\n",
    "        \n",
    "        self.trigrams = set(list(itertools.chain(*self.trigrams)))\n",
    "        self.trigrams = dict(zip(self.trigrams, range(self.vocab_size)))\n",
    "            \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: колчиество триграмм, для которых мы завели индекс.\n",
    "        \"\"\"\n",
    "        return len(self.trigrams)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_trigrams(word):\n",
    "        \"\"\"\n",
    "            word: слово\n",
    "            \n",
    "            returns: список триграмм для word\n",
    "        \"\"\"\n",
    "        trigrams = []\n",
    "        \n",
    "        word = \"#\" + word + \"#\"\n",
    "        for i in range(1, len(word) - 1):\n",
    "            trigrams.append(word[i-1:i+2])\n",
    "        \n",
    "        return trigrams\n",
    "            \n",
    "    def __call__(self, word):\n",
    "        \"\"\"\n",
    "            word: слово\n",
    "            \n",
    "            returns: список индексов триграмм для слова word, которые нашлись в маппинге\n",
    "        \"\"\"\n",
    "        trigrams = self._get_trigrams(word)\n",
    "        trigrams_indexes = [self.trigrams[trigram] for trigram in trigrams if trigram in self.trigrams]\n",
    "        \n",
    "        return trigrams_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_trigram_tokenizer(TrigramTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания токенизатора используйте обработанный с помощью TextProcessor текст. \n",
    "\n",
    "**Важно:** в токенизатор нужно подавать только слова, известные word2vec'у."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in validation_preprocessed: 17635\n",
      "Total number of unique words which have embeddings: 7968\n",
      "Total number of trigrams: 4142\n",
      "CPU times: user 11.4 s, sys: 580 ms, total: 12 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentences_preprocessed = [item for row in validation_preprocessed for item in row]\n",
    "sentences_words_preprocessed = list(map(lambda sentence: sentence.split(), sentences_preprocessed))\n",
    "words_preprocessed = set(list(itertools.chain(*sentences_words_preprocessed)))\n",
    "print(f\"Total number of unique words in validation_preprocessed: {len(words_preprocessed)}\")\n",
    "\n",
    "w2v_vocab = list(filter(lambda word: word in wv_embeddings, words_preprocessed))\n",
    "print(f\"Total number of unique words which have embeddings: {len(w2v_vocab)}\")\n",
    "\n",
    "tri_tokenizer = TrigramTokenizer(w2v_vocab)\n",
    "print(f\"Total number of trigrams: {tri_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание датасета с w2v векторами и списками индексов триграмм\n",
    "\n",
    "Мы будем обучать триграммную модель в нейросетевом фреймворке pytorch. Для этого нам нужно создать свой датасет.\n",
    "\n",
    "Он должен:\n",
    "1. Принимать список слов, word2vec и уже созданный триграммный токенизатор.\n",
    "2. Выдавать пары вида (эмбеддинг для слова из word2vec, список индексов триграмм для этого слова)\n",
    "\n",
    "Реализовать датасет нужно в шаблоне, приведенном ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainTrigramDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, vocab, w2v_embeddings, tri_tokenizer):\n",
    "        \"\"\"\n",
    "            Формируем выборку для обучения триграммной модели.\n",
    "            ЗАРАНЕЕ считаем маппинг в список индексов для всех известных в word2vec слов.\n",
    "            \n",
    "            vocab: список слов\n",
    "            w2v_embeddings: no comments\n",
    "            tri_tokenizer: токенизатор триграмм\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.w2v_embeddings = w2v_embeddings\n",
    "        self.tri_tokenizer = tri_tokenizer\n",
    "        \n",
    "        \n",
    "        self.w2tri_index = {}\n",
    "        for word in self.vocab:\n",
    "            self.w2tri_index[word] = self.tri_tokenizer(word)\n",
    "        \n",
    "                \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            returns: возвращает количество слов, вошедших в маппинг (размер словаря)\n",
    "        \"\"\"\n",
    "        return len(self.w2tri_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: w2v эмбеддинг для idx-го слова в датасете, список соответствующих ему триграмм (тензоры)\n",
    "        \"\"\"\n",
    "        word = self.vocab[idx]\n",
    "        word_vec = self.w2v_embeddings[word]\n",
    "        word_tri_index = self.w2tri_index[word]\n",
    "        \n",
    "        return (word_vec, word_tri_index)\n",
    "    \n",
    "ds = TrainTrigramDataset(w2v_vocab, wv_embeddings, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_dataset(ds, w2v_vocab, wv_embeddings, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание DataLoader'а и Collator'а\n",
    "\n",
    "Нас интересуют в первую очередь четыре параметра при создании DataLoader:\n",
    "1. Датасет. Реализует интерфейс массива - можно узнать длину и получить элемент с индексом, меньшим длины.\n",
    "2. batch_size. Задает размера батча (количества сэмплов, идущих одновременно в модель).\n",
    "3. shuffle. При shuffle == True каждую эпоху при итерировании по даталоадеру мы будем получать сэмплы в произвольном порядке.\n",
    "4. collate_fn. Этот параметр позволяет задать кастомную логику \"склеивания\" сэмплов из датасета в батч.\n",
    "\n",
    "В качестве модели мы будем использовать слой **torch.nn.EmbeddingBag**. Он принимает на вход список индексов и список сдвигов, начинающийся с нуля.\n",
    "\n",
    "Нужно наш список списков индексов триграмм превратить в соответствующий формат, преобразовать векторы слов и два списка (индексов и сдвигов) в pytorch тензоры (torch.tensor).\n",
    "\n",
    "Реализуйте следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        batch: список из элементов датасета, e.g. [ds[i] for i in [2, 3, 1, 15]]\n",
    "        \n",
    "        returns: w2v эмбеддинги, индексы триграмм, сдвиги для триграмм\n",
    "    \"\"\"\n",
    "    \n",
    "    w2v_embeddings = []\n",
    "    index_list = []\n",
    "    offsets = []\n",
    "\n",
    "    for vec, indexes in batch:\n",
    "        w2v_embeddings.append(vec)\n",
    "        index_list += indexes\n",
    "        offsets.append(len(indexes))\n",
    "    \n",
    "    off = []\n",
    "    for i, offset in enumerate(offsets):\n",
    "        if not off:\n",
    "            off.append(0)\n",
    "        else:\n",
    "            off.append(off[-1] + offsets[i-1])\n",
    "    \n",
    "    w2v_embeddings = torch.tensor(w2v_embeddings)\n",
    "    index_list = torch.tensor(index_list)\n",
    "    offset = torch.tensor(off)\n",
    "    \n",
    "    return w2v_embeddings, index_list, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_dataloader(ds, collate_fn, embedding_dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели\n",
    "\n",
    "При создании модели мы обычно наследуемся от **torch.nn.Module** и создаем нужные нам слои как атрибуты объекта нашего класса.\n",
    "\n",
    "В данном случае предлагается для формирования эмбеддингов использовать **torch.nn.EmbeddingBag**.\n",
    "\n",
    "Реализуйте предложенный шаблон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TrigramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\"\n",
    "            num_embeddings: количество триграмм, для которых обучаются эмбеддинги\n",
    "            embedding_dim: размерность эмбеддингов триграмм\n",
    "        \"\"\"\n",
    "        super(TrigramModel, self).__init__()\n",
    "        self.embedding_bag = nn.EmbeddingBag(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "        \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        \"\"\"\n",
    "            returns: размерность эмбеддингов\n",
    "        \"\"\"\n",
    "        return self.embedding_bag.embedding_dim\n",
    "    \n",
    "    @property\n",
    "    def num_embeddings(self):\n",
    "        \"\"\"\n",
    "            returns: количество эмбеддингов\n",
    "        \"\"\"\n",
    "        return self.embedding_bag.num_embeddings\n",
    "    \n",
    "    def forward(self, trigrams, offsets):\n",
    "        \"\"\"\n",
    "            trigrams: список индексов триграмм (тензор)\n",
    "            offsets: список сдвигов (тензор)\n",
    "            \n",
    "            returns: эмбеддинги слов, составленные из триграмм\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding_bag(trigrams, offsets)\n",
    "        return embeddings\n",
    "    \n",
    "model = TrigramModel(tri_tokenizer.vocab_size, embedding_dim=wv_embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_trigram_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание пайплайна обучения\n",
    "\n",
    "Далее необходимо совместить все наработки в единый пайплайн обучения, добавив также критерий для оптимизации и оптимизатор. \n",
    "\n",
    "Предлагается:\n",
    "\n",
    "1. В качестве оптимизатора использовать Adam (можно попробовать подобрать learning rate / weight decay)\n",
    "2. В качестве критерия оптимизации взять nn.MSELoss (можно также закодить лосс самому)\n",
    "3. Для даталоадера выбрать небольшой батч сайз (32, 64, 128, 256)\n",
    "4. Десяти эпох должно быть достаточно для хорошего качества\n",
    "\n",
    "Реализуйте предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        \"\"\"\n",
    "            model: триграммная модель\n",
    "            criterion: функционал ошибки, принимает на вход w2v эмбеддинги и триграммные эмбеддинги\n",
    "            optimizer: оптимизатор для модели\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            Делаем один проход по даталоадеру, с бэкпропом\n",
    "            \n",
    "            dataloader: даталоадер с тренировочными данными\n",
    "            \n",
    "            returns: лосс\n",
    "        \"\"\"\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for words, trigrams, offsets in dataloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            output = self.model(trigrams, offsets)\n",
    "            loss = self.criterion(words, output)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            \n",
    "        return epoch_loss / len(dataloader)\n",
    "\n",
    "    \n",
    "    def train(self, dataloader, n_epochs, verbose=False):\n",
    "        \"\"\"\n",
    "            dataloader: тренировочный даталоадер\n",
    "            n_epochs: количество эпох\n",
    "            verbose: выводить лосс каждую эпоху или нет\n",
    "            \n",
    "            returns: список лоссов\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        losses = []\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self._train_step(dataloader)\n",
    "            losses.append(loss)\n",
    "            if verbose:\n",
    "                print(f'epoch: {epoch + 1:>2}, loss: {loss:.4f}, time: {time.time() - start:.4f}')\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10\n",
    "LR = 0.001\n",
    "\n",
    "model = TrigramModel(tri_tokenizer.vocab_size, embedding_dim=wv_embeddings.vector_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "trainer = Trainer(model, criterion, optimizer)\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1, loss: 0.1992, time: 1.1218\n",
      "epoch:  2, loss: 0.1879, time: 2.0643\n",
      "epoch:  3, loss: 0.1778, time: 3.0427\n",
      "epoch:  4, loss: 0.1685, time: 3.9500\n",
      "epoch:  5, loss: 0.1600, time: 4.8364\n",
      "epoch:  6, loss: 0.1522, time: 5.8670\n",
      "epoch:  7, loss: 0.1449, time: 7.2494\n",
      "epoch:  8, loss: 0.1381, time: 8.2447\n",
      "epoch:  9, loss: 0.1319, time: 9.1843\n",
      "epoch: 10, loss: 0.1263, time: 10.1182\n"
     ]
    }
   ],
   "source": [
    "losses = trainer.train(dataloader, N_EPOCHS, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение векторов неизвестных слов. Инференс модели\n",
    "\n",
    "Теперь, когда мы обучили модель, нам необходимо применить её для всех неизвестных слов, т.е. получить для них эмбеддинги.\n",
    "\n",
    "Т.к. для этих слов у нас нет word2vec эмбеддингов, то dataset и collator для обучения не подходят для инференса. Необходимо реализовать датасет и коллатор для инференса по следующим шаблонам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainTrigramDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, vocab, w2v_embeddings, tri_tokenizer):\n",
    "        \"\"\"\n",
    "            Формируем выборку для обучения триграммной модели.\n",
    "            ЗАРАНЕЕ считаем маппинг в список индексов для всех известных в word2vec слов.\n",
    "            \n",
    "            vocab: список слов\n",
    "            w2v_embeddings: no comments\n",
    "            tri_tokenizer: токенизатор триграмм\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.w2v_embeddings = w2v_embeddings\n",
    "        self.tri_tokenizer = tri_tokenizer\n",
    "        \n",
    "        \n",
    "        self.w2tri_index = {}\n",
    "        for word in self.vocab:\n",
    "            self.w2tri_index[word] = self.tri_tokenizer(word)\n",
    "        \n",
    "                \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            returns: возвращает количество слов, вошедших в маппинг (размер словаря)\n",
    "        \"\"\"\n",
    "        return len(self.w2tri_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: w2v эмбеддинг для idx-го слова в датасете, список соответствующих ему триграмм (тензоры)\n",
    "        \"\"\"\n",
    "        word = self.vocab[idx]\n",
    "        word_vec = self.w2v_embeddings[word]\n",
    "        word_tri_index = self.w2tri_index[word]\n",
    "        \n",
    "        return (word_vec, word_tri_index)\n",
    "    \n",
    "ds = TrainTrigramDataset(w2v_vocab, wv_embeddings, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceTrigramDataset:\n",
    "    \n",
    "    def __init__(self, vocab, tri_tokenizer):\n",
    "        \"\"\"\n",
    "            Датасет с неизвестными словами\n",
    "            \n",
    "            vocab: список слов\n",
    "            tri_tokenizer: триграммный токенизатор\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.tri_tokenizer = tri_tokenizer\n",
    "        \n",
    "        self.w2tri_index = {}\n",
    "        for word in self.vocab:\n",
    "            self.w2tri_index[word] = self.tri_tokenizer(word)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.w2tri_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.vocab[idx]\n",
    "        word_tri_index = self.w2tri_index[word]\n",
    "        \n",
    "        return word_tri_index\n",
    "    \n",
    "    \n",
    "def inference_collate_fn(trigrams):\n",
    "    \"\"\"\n",
    "        trigrams: список списков индексов триграмм\n",
    "        \n",
    "        returns: список индексов, список сдвигов триграмм\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words w/o w2v represetaion: 9667\n",
      "Sample words: ['applicationdidbecomeactive', 'actionevent', 'serviceconnection', 'showsfields', 'modelinstance', 'prefences', 'mydirectory', 'sphinxse', 'radiobuttonlistfor', 'streamcoders']\n"
     ]
    }
   ],
   "source": [
    "vocab_unk = list(words_preprocessed - set(w2v_vocab))\n",
    "print(f\"Total number of words w/o w2v represetaion: {len(vocab_unk)}\")\n",
    "print(f\"Sample words: {vocab_unk[:10]}\")\n",
    "\n",
    "inf_ds = InferenceTrigramDataset(vocab_unk, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ols'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_ds.vocab[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1746, 4140, 3532]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_ds[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть всё необходимое, чтобы осуществить инференс. Не забудь перед инференсом перевести модель в режим эвала (**model.eval**), а также использовать контекстный менеджер **torch.no_grad**.\n",
    "\n",
    "После инференса сформируйте словарь из известных в word2vec слов и их эмбеддингов, затем дополните его эмбеддингами для неизвестных слов, полученными после инференса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя **Scorer** и **Embedder**, получите новые значения метрик для валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из критериев получения полных баллов является значение метрики **hits@500** $\\geqslant 0.89$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная часть. Обучение векторных представлений для целевой задачи. (до 5 баллов)\n",
    "\n",
    "Предполагается, что в этой части используются TextPreprocessor, Embedder, Scorer из предыдущих частей.\n",
    "\n",
    "Для обучения на целевую задачу нам понадобится обучающая выборка. Считайте её с диска, предобработайте текст вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for questions in tqdm.tqdm(read_corpus('data/train.tsv')):\n",
    "    train.append([text_preprocessor(text) for text in questions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо создать **токенизатор для текста** - составить словарь и сделать маппинг из слов в индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"\n",
    "            vocab: множество слов, встретившихся в обучающей выборке\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: количество слов в словаре\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: текст\n",
    "            \n",
    "            returns: список индексов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составление словаря и токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам также понадобится **новый датасет для обучения**. Для применения метода NT-Exent нам не нужно \"майнить негативы\", поэтому датасет надо сформировать как массив из пар-дубликатов.\n",
    "\n",
    "Так как данные в обучающей выборке содержат множества дубликатов (т.е. все дубликаты сгруппированы в списки), есть несколько способов сформировать итоговый датасет:\n",
    "1. Оставить из каждого множества дубликатов какие-нибудь случайные два (или просто первые два вопроса)\n",
    "2. Для первого вопроса в множестве взять все остальные как дубликаты (N вопросов-дубликатов - N-1 пара). Тогда мы увидим каждый вопрос хотя бы один раз при обучении\n",
    "3. Составить всевозможные уникальные пары-дубликаты из этих множеств (т.е. первый вопрос и все остальные вопросы, второй вопрос и все остальные, кроме первого).\n",
    "\n",
    "Каждый следующий способ, начиная с первого, раздувает выборку по размеру, но возможно дает прирост к качеству решения задачи.\n",
    "\n",
    "Реализуйте выбранный вами подход, используя предолженный шаблон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDuplicatesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, question_pairs, tokenizer):\n",
    "        \"\"\"\n",
    "            question_pairs: список из пар вопросов-дубликатов\n",
    "            tokenizer: объект класса TextTokenizer\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            returns: количество пар-дубликатов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: (вопрос, дубликат), idx-ю пару в датасете\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также нужно подготовить **даталоадер** (а именно - коллатор для даталоадера) по аналогии со второй частью задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_and_offsets(questions):\n",
    "    \"\"\"\n",
    "        questions: список из токенизированных вопросов\n",
    "        \n",
    "        returns: (ids, offsets), где ids - вытянутый список индексов слов в вопросах из батча, offsets - сдвиги\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        batch: список из пар токенизированных вопросов-дубликатов [(question, duplicate), ...]\n",
    "        \n",
    "        returns: (question_ids, question_offsets), (duplicate_ids, duplicate_offsets)\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поделите выборку на трейн и валидацию, используя train_test_split, затем **создайте датасеты и даталоадеры** для обучения и валидации. Сколько пар-дубликатов получилось в датасете для обучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью предложенного шаблона **задайте модель** для преобразования вопросов в векторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class DssmLikeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\"\n",
    "            num_embeddings: количество слов, для которых обучаем эмбеддинги\n",
    "            embedding_dim: размерность эмбеддинга\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def forward(self, ids, offsets):\n",
    "        \"\"\"\n",
    "            ids: вытянутая посл-ть индексов слов вопросов, попавших в батч\n",
    "            offsets: сдвиги для вопросов, попавших в батч\n",
    "            \n",
    "            returns: векторы вопросов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Критерий оптимизации** для NTExentLoss выглядит как:\n",
    "\n",
    "$$\\mathcal{L}(Q, D) = -0.5 \\log diag(softmax(QD^T / \\alpha)) - 0.5 \\log diag(softmax(DQ^T / \\alpha)),$$\n",
    "\n",
    "где:\n",
    "* $Q \\in \\mathbb{R}^{b \\times d}$ - эмбеддинги вопросов, \n",
    "* $D \\in \\mathbb{R}^{b \\times d}$ - эмбеддинги соответствующих вопросам дубликатов,\n",
    "* $b$ - количество пар (вопрос, дубликат), $d$ - размерность эмбеддингов, $\\alpha$ - гиперпараметр лосса. \n",
    "* Softmax берется по рядам\n",
    "* Матрицы $Q, D$ содержат нормированные эмбеддинги, т.е. считается именно косинус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTExentLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=1., eps=1e-8):\n",
    "        \"\"\"\n",
    "            alpha: коэффициент, на который мы делим скоры перед софтмаксом\n",
    "            eps: ||v|| = min(eps, ||v||)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def _normalize(self, embeddings):\n",
    "        \"\"\"\n",
    "            embeddings: матрица размера [batch_size, embedding_dim]\n",
    "            \n",
    "            returns: матрица такого же размера, но с нормироваными векторами\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def forward(self, embeddings, positives):\n",
    "        \"\"\"\n",
    "            embeddings: матрица размера [batch_size, embedding_dim]\n",
    "            positives: матрица такого же размера, с позитивами для векторов из матрицы embeddings\n",
    "            \n",
    "            returns: NT-Exent loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создайте пайплайн** для обучения и валидации, используя предложенный шаблон.\n",
    "\n",
    "Залогируйте с помощью **torch.utils.tensorboard.SummaryWriter** две величины:\n",
    "1. Лосс для каждого батча\n",
    "2. Лосс на валидации для каждой эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "    \n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            criterion, \n",
    "            logdir=None, \n",
    "            device=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса DssmModel\n",
    "            optimizer: оптимизатор\n",
    "            criterion: критерий оптимизации\n",
    "            logdir: директория, в которую SummaryWriter должен писать логи\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def _calculate_loss(self, batch):\n",
    "        \"\"\"\n",
    "            batch: батч из индексов и сдвигов для вопросов и их дубликатов\n",
    "            \n",
    "            returns: посчитанный для батча лосс\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: даталоадер для обучения\n",
    "            \n",
    "            returns: лосс на датасете для обучения\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def _eval_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: даталоадер для валидации\n",
    "            \n",
    "            returns: лосс на валидации\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def train(self, dataloaders, n_epochs, verbose=False):\n",
    "        \"\"\"\n",
    "            dataloaders: словарь вида {'train': train_dataloader, 'eval': eval_dataloader}\n",
    "            n_epochs: количество эпох обучения\n",
    "            verbose: нужно ли выводить каждую эпоху информацию про лоссы\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = self._train_step(dataloaders['train'])\n",
    "            \n",
    "            eval_loss = self._eval_step(dataloaders['eval'])\n",
    "            if self._writer is not None:\n",
    "                self._writer.add_scalar('eval/loss', eval_loss, global_step=self._n_epoch)\n",
    "                \n",
    "            if verbose:\n",
    "                print(\n",
    "                    'epoch: {:>2}, train loss: {:.4f}, eval loss: {:.4f}, time: {:.4f}' \\\n",
    "                        .format(epoch + 1, train_loss, eval_loss, time.time() - start)\n",
    "                )\n",
    "                    \n",
    "            self._n_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предлагается использовать для оптимизации Адам и обучать модель 10-60 эпох.\n",
    "\n",
    "Для этой части задания GPU даёт существенное ускорение при обучении, поэтому стоит по возможности делать обучение с большим batch size'ом и на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось достать из модели обученные под задачу векторы слов, составить маппинг слов в векторы, создать **Embedder** и **Scorer** и провалидировать качество на нашей исходной валидации, которой мы пользовались в первых двух частях.\n",
    "\n",
    "Чтобы достать из модели веса, можно использовать `model._embeddings.weight.cpu().detach().numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из критериев получения полных баллов является значение метрики **hits@500** $\\geqslant 0.98$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "363.366px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
