{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 3 \n",
    "\n",
    "# Классификация с использованием BERT  и Transfer learning\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: Смирнов Александр Львович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "\n",
    "В этом задании вы будете определять категории товара по данным из чеков, предоставленным в соревновании [Data Fusion Context](https://boosters.pro/championship/data_fusion/data).\n",
    "\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    " - [Pytorch](https://pytorch.org/).\n",
    " - [Transformers](https://github.com/huggingface/transformers).\n",
    " - [Tokenizers](https://github.com/huggingface/tokenizers).\n",
    "\n",
    "Данные лежат в архиве data.zip, в котором лежит файл `data.csv`, содержащий тексты и соответствующие им категории товаров. Все объекты поделены между train, test, val и unsupervised. Для unsupervised объектов категории товаров недоступны. \n",
    "\n",
    "Скачать данные можно здесь: [ссылка на google диск](https://drive.google.com/file/d/1AHs7qJYg2tc8zblGlT0Dpe50e6RW-gAW/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Подготовка данных (2 балла)\n",
    "\n",
    "Классические методы NLP (например, как мы делали в первом и втором домашнем задании) преобразуют тексты в списки индексов следующим образом:\n",
    "1. \"Очистка текста\" от плохих символов, приводим (или не приводим) текст к нижнему регистру.\n",
    "2. Текст делится по пробелам на слова.\n",
    "3. По полученной коллекции текстов строится словарь вида \"слово -> индекс\", редкие слова выбрасываются, стопслова иногда тоже\n",
    "4. Побитый на слова текст превращается в список индексов с помощью этого словаря.\n",
    "\n",
    "Для трансформеров схема выглядит немного по-другому — используются более продвинутые методы токенизации типа `wordpiece, bpe, sentencepiece`. Основное концептуальное отличие — текст делится не только на слова по пробелам, но и сами слова делятся на \"подслова\" (читай subwords). Это верно для BPE и wordpiece, а sentencepiece вообще не учитывает пробелы. Более подробно ознакомиться с этими методами токенизации можно в наших лекциях.\n",
    "\n",
    "В данном задании предлагается использовать wordpiece токенизатор, который использовали в оригинальной статье про BERT. Построить его можно с помощью библиотеки `tokenizers`:\n",
    "1. Считайте данные с помощью `pandas`\n",
    "2. Используя метод `tokenizers.BertWordPieceTokenizer.train` и список сырых текстов постройте токенизатор. Используйте нижний регистр (lowercase), чистый текст (clean_text), без акцентов (strip_accents), размера словаря 30000 (vocab_size).\n",
    "3. Сохраните построенный токенизатор (метод `tokenizer.save_model`) и создайте объект класса `transformers.BertTokenizerFast`, который работает быстрее стандартной реализации, но не позволяет её обучать.\n",
    "\n",
    "**Важно:** нужно при обучении c помощью параметра `special_tokens` завести индексы для токенов `[PAD], [UNK], [CLS], [SEP], [MASK]`, которые понадобятся нам дальше для обучения и использования модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/task3_data.csv')\n",
    "tokenizer_dump_save_path = './.cache/tokenizer/tokenizer_cached'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p .cache/tokenizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "\n",
    "class WordpieceTokenizer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_path, \n",
    "            strip_accents=True, \n",
    "            clean_text=True, \n",
    "            lowercase=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "            vocab_path: путь к словарю\n",
    "            strip_accents: очистка текста от акцентов\n",
    "            clean_text: просто чистка текста от непонятных символов\n",
    "            lowercase: приведение текста к нижнему регистру\n",
    "            \n",
    "            Подгружает токенизатор с помощью BertTokenizerFast.\n",
    "        \"\"\"\n",
    "        self._tokenizer = BertTokenizerFast(\n",
    "            vocab_file=vocab_path,\n",
    "            strip_accents=strip_accents,\n",
    "            do_lower_case=lowercase\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(\n",
    "            cls,\n",
    "            corpus,\n",
    "            corpus_save_path,\n",
    "            tokenizer_save_path,\n",
    "            tokenizer_name,\n",
    "            vocab_size=30000,\n",
    "            min_frequency=2,\n",
    "            strip_accents=True,\n",
    "            clean_text=True,\n",
    "            lowercase=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "            corpus: список текстов\n",
    "            corpus_save_path: временный путь для сохранения текстов в текстовом файле\n",
    "            tokenizer_save_path: путь для сохранения файлов токенизатора\n",
    "            tokenizer_name: название токенизатора, влияет на названия файлов токенизатора\n",
    "            vocab_size: размер словаря\n",
    "            min_frequency: минимальная частота элемента в словаре\n",
    "            strip_accents: очистка текста от акцентов\n",
    "            clean_text: просто чистка текста от непонятных символов\n",
    "            lowercase: приведение текста к нижнему регистру\n",
    "            \n",
    "            С помощью списка сырых текстов формирует токенизатор\n",
    "        \"\"\"\n",
    "        \n",
    "        # writing corpus to file\n",
    "        with open(corpus_save_path, 'w') as f:\n",
    "            for item in corpus:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        \n",
    "        tokenizer = BertWordPieceTokenizer(\n",
    "            clean_text=clean_text,\n",
    "            strip_accents=strip_accents,\n",
    "            lowercase=lowercase,\n",
    "        )\n",
    "        \n",
    "        tokenizer.train(\n",
    "            corpus_save_path,\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=min_frequency,\n",
    "            show_progress=True,\n",
    "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "        )\n",
    "        \n",
    "        vocab_path = tokenizer.save_model(\n",
    "            directory=tokenizer_save_path, \n",
    "            prefix=tokenizer_name\n",
    "        )[0]\n",
    "        \n",
    "        return cls(\n",
    "            vocab_path,\n",
    "            strip_accents,\n",
    "            clean_text,\n",
    "            lowercase\n",
    "        )\n",
    "\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: str. Сырой текст\n",
    "            \n",
    "            returns: list of ints. Список индексов\n",
    "            \n",
    "            C помощью метода .encode преобразует текст в индексы.\n",
    "        \"\"\"\n",
    "        return self._tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    @property\n",
    "    def cls_token_id(self):\n",
    "        \"\"\"\n",
    "            returns: индекс CLS токена\n",
    "        \"\"\"\n",
    "        return self._tokenizer.cls_token_id\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self._tokenizer.pad_token_id\n",
    "\n",
    "    @property\n",
    "    def mask_token_id(self):\n",
    "        return self._tokenizer.mask_token_id\n",
    "\n",
    "    @property\n",
    "    def sep_token_id(self):\n",
    "        return self._tokenizer.sep_token_id\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: размер словаря\n",
    "        \"\"\"\n",
    "        return self._tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте токенизатор:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = WordpieceTokenizer.from_corpus(\n",
    "    corpus=data[\"text\"].values,\n",
    "    corpus_save_path='./.cache/corpus',\n",
    "    tokenizer_save_path='./.cache/tokenizer/',\n",
    "    tokenizer_name='bert_word_piece_tokenizer'\n",
    ")\n",
    "\n",
    "with open(tokenizer_dump_save_path, 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tokenizer_dump_save_path, 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам доступно довольно большое количество неразмеченных данных, которое можно использовать для предобучения модели. Мы рассмотрим две задачи предобучения:\n",
    "1. **Masked Language Modeling** — задача из BERT. Выбираем 15% слов, из них 80% заменяем на токен [MASK], 10% меняем на другие случайные слова, 10% оставляем как есть. Эти 15% слов предсказываем моделью. Вспомним пример из оригинальной статьи:\n",
    "    * Исходный текст: `my dog is hairy`\n",
    "    * Выбираем случайным образом 15% токенов для задачи. Допустим, выбрали четвертый токен - `hairy`\n",
    "    * В 80% случаев заменяем токен на `[MASK]`: `my dog is [MASK]`\n",
    "    * В 10% случаев на другой случайный токен: `my dog is apple`\n",
    "    * В 10% случаев оставляем неизменным: `my dog is hairy`\n",
    "    \n",
    "    \n",
    "2. **Sentence Order Prediction** — задача из ALBERT. Делим текст на два сегмента, с вероятностью 50% меняет сегменты местами. Предсказываем, в правильном ли порядке находятся сегменты.\n",
    "    * Текст: `the man went to the store. he bought a gallon of milk`\n",
    "    * Токенизируем и делим его на два сегмента: `the man went to the store` и `he bought a gallon of milk`\n",
    "    * C вероятностью 50% меняем их местами: `[CLS] he bought a gallon of milk [SEP] the man went to the store`\n",
    "    * С вероятностью 50% оставляем на месте: `[CLS] the man went to the store [SEP] he bought a gallon of milk`\n",
    "    * \"Левому\" сегменту соответствует нулевой индекс сегмента, \"правому\" - индекс 1\n",
    "\n",
    "Большая часть логики предобучения реализуется при подготовке данных.\n",
    "\n",
    "Реализуйте **PretrainDataset**, который токенизирует поданные сырые тексты и умеет возвращать для текста с конкретным индексом случайный сегмент длины, не большей чем `maxlen`. Логика для задачи **SOP** должна быть реализована в `__getitem__`: выбранный сегмент надо поделить на два равных сегмента, подбросить монетку, и с 50% вероятностью поменять сегменты местами. Нужно также добавить `[CLS]` и `[SEP]` токены.\n",
    "\n",
    "**hint:** чтобы существенно ускорить обучение (не потеряв при этом в качестве), после токенизации отсортируйте датасет по длине текстов.\n",
    "**hint:** токенизация датасета для предобучения занимает существенное время (5 минут), поэтому во время отладки стоит сделать её один раз и сохранить результат на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dump_save_path = './.cache/dataset/dataset_cached'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p .cache/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            corpus, \n",
    "            tokenizer, \n",
    "            minlen,\n",
    "            maxlen,\n",
    "            permute_prob=0.5, \n",
    "            verbose=False, \n",
    "            presort=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "            corpus: list of strings. Список сырых текстов\n",
    "            tokenizer: токенизатор\n",
    "            minlen: минимально допустимая длина текста\n",
    "            permute_prob: вероятность, с которой два сегмента меняются местами (происходит swap)\n",
    "            maxlen: максимальная длина текста\n",
    "            verbose: вывод прогресса токенизации текстов с помощью tqdm\n",
    "            presort: отсортировать датасет по длинам токенизированных текстов (т.е. ds[0] выдает самый короткий текст)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._corpus = corpus\n",
    "        self._tokenizer = tokenizer\n",
    "        self._minlen = minlen\n",
    "        self._maxlen = maxlen\n",
    "        \n",
    "        self._permute_prob = permute_prob\n",
    "        self._verbose = verbose\n",
    "        self._presort = presort\n",
    "        \n",
    "        self._items = []\n",
    "        for text in tqdm(self._corpus, disable=not self._verbose):\n",
    "            text_tokenized = self._tokenizer(text)\n",
    "            if len(text_tokenized) >= self._minlen:\n",
    "                self._items.append(text_tokenized)\n",
    "        \n",
    "        if self._presort:\n",
    "            self._items.sort(key=len)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._items)\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"\n",
    "            returns: tokenizer. Нужно для тестов\n",
    "        \"\"\"\n",
    "        return self._tokenizer\n",
    "    \n",
    "    def set_maxlen(self, maxlen):\n",
    "        \"\"\"\n",
    "            maxlen: максимальная длина текста\n",
    "            \n",
    "            поставить новое максимальное значение длины\n",
    "        \"\"\"\n",
    "        self._maxlen = maxlen\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: \n",
    "                input_ids - тензор с индексами, \n",
    "                token_type_ids - тензор с сегментными айдишниками (0 у левого сегмента, 1 у правого),\n",
    "                permuted - был ли swap сегментов\n",
    "        \"\"\"\n",
    "        \n",
    "        #######################################\n",
    "        # random segment w/ length = maxlen\n",
    "        #######################################\n",
    "        input_ids = self._items[idx]\n",
    "        if len(input_ids) > self._maxlen:\n",
    "            i = np.random.randint(0, len(input_ids) - self._maxlen + 1)\n",
    "            input_ids = input_ids[i : i + self._maxlen]\n",
    "        #######################################\n",
    "        \n",
    "        #######################################\n",
    "        # SOP\n",
    "        #######################################\n",
    "        permuted = False\n",
    "        if random() < self._permute_prob:\n",
    "            permuted = True\n",
    "        \n",
    "        l = len(input_ids)\n",
    "        if permuted:\n",
    "            B, A = input_ids[:l//2], input_ids[l//2:]\n",
    "        else:\n",
    "            A, B = input_ids[:l//2], input_ids[l//2:]\n",
    "        input_ids = [self.tokenizer.cls_token_id] + A + [self.tokenizer.sep_token_id] + B\n",
    "        \n",
    "        # in original implementation there are\n",
    "        # 3 tokens for segment embeddings but not 2\n",
    "        # (to follow original paper we should place\n",
    "        # 1 and 2 here, and padding 0, but we doing\n",
    "        # 0 and 1 and padding 0)\n",
    "        token_type_ids = [0] * (len(A) + 2) + [1] * len(B)\n",
    "        \n",
    "        assert len(token_type_ids) == len(input_ids)\n",
    "        #######################################\n",
    "        \n",
    "        #return input_ids\n",
    "        return torch.tensor(input_ids), torch.tensor(token_type_ids), permuted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длину текстов нужно ограничить снизу. Нет смысла рассматривать слишком короткие тексты (например, единичную длину), для которых задачи предобучения вообще не работают. Длинные тексты более эффективны для задач типа MLM, так как у модели больше контекста для предсказания и больше таргетов на один объект.\n",
    "\n",
    "Разумный способ определить минимальную длину текстов для MLM  — подобрать такую минимальную длину, чтобы вероятность замаскировать хотя бы одно слово в тексте была больше заданного порога.\n",
    "\n",
    "Т.е. если мы каждое слово маскируем с вероятностью 15%, какой длины должен быть текст, чтобы с вероятностью $\\geqslant$ 50% было замаскировано хотя бы одно слово?\n",
    "\n",
    "Используйте ответ на данный вопрос как минимальную допустимую длину текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(round(np.log(0.5) / np.log(0.85), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$1 - (1 - 0.15)^x \\geq 0.5$$\n",
    "$$1 - 0.85^x \\geq 0.5$$\n",
    "$$0.85^x \\leq 0.5$$\n",
    "$$x \\times \\log{0.85} \\leq \\log{0.5}$$\n",
    "$$x \\geq \\frac{\\log{0.5}}{ \\log{0.85}}$$\n",
    "$$x \\gtrsim 4.27$$\n",
    "$$x = 5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте датасет (с произвольным разумным значением maxlen):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds = PretrainDataset(\n",
    "    corpus=data[\"text\"].values,\n",
    "    tokenizer=tokenizer,\n",
    "    minlen=5,\n",
    "    maxlen=200,\n",
    "    verbose=True,\n",
    "    presort=True\n",
    ")\n",
    "\n",
    "with open(dataset_dump_save_path, 'wb') as file:\n",
    "    pickle.dump(ds, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_dump_save_path, 'rb') as file:\n",
    "    ds = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 150мг №1 вертекс [SEP] флуконазол капс\n",
      "tensor([0, 0, 0, 0, 0, 1, 1])\n",
      "True\n",
      "------------------------------\n",
      "[CLS] огурцы мам [SEP]люк вес\n",
      "tensor([0, 0, 0, 0, 1, 1, 1])\n",
      "False\n",
      "------------------------------\n",
      "[CLS] мартина 100г соленые [SEP] семечки от\n",
      "tensor([0, 0, 0, 0, 0, 1, 1])\n",
      "True\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    input_ids, token_type_ids, permuted = ds[i]\n",
    "    print(tokenizer._tokenizer.decode(input_ids))\n",
    "    print(token_type_ids)\n",
    "    print(permuted)\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длину текстов нужно как-то ограничить сверху. Иначе, если встретится какой-то очень-очень длинный текст, он не поместится в видеопамять. Самый простой способ определить ограничение по длине  — после токенизации построить гистограмму длин (например, используя **sns.distplot**) и методом пристального взгляда определить разумное ограничение длины. Другой вариант  — взять большое значение квантили.\n",
    "\n",
    "**Вопрос:** какая максимальная длина текста подходит для этого датасета?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = list(map(lambda x: len(x[0]), ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,4))\n",
    "sns.histplot(lengths, bins=len(set(lengths)), kde=True, kde_kws={'bw_adjust': 3}, ax=ax[0])\n",
    "sns.boxplot(x=lengths, ax=ax[1])\n",
    "\n",
    "fig.suptitle(\"Optimal length of tokenized texts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 9.0\n",
      "Q3: 14.0\n",
      "IQR: 5.0\n",
      "X2: 21.5\n"
     ]
    }
   ],
   "source": [
    "Q1 = np.quantile(lengths, 0.25)\n",
    "Q3 = np.quantile(lengths, 0.75)\n",
    "IQR = Q3 - Q1\n",
    "X2 = Q3 + 1.5 * IQR\n",
    "print(f\"Q1: {Q1}\")\n",
    "print(f\"Q3: {Q3}\")\n",
    "print(f\"IQR: {IQR}\")\n",
    "print(f\"X2: {X2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Присвойте максимально допустимое значение длины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_maxlen(int(X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если для задачи **SOP** мы готовим данные при индексации датасета `PretrainDataset`, то маскирование для задачи **MLM** удобней делать в Collator'е в тензорном виде.\n",
    "\n",
    "Как с вероятностью `15%` заменить в тензоре `input_ids` значения на `0`: \n",
    "\n",
    "1. `mask = torch.rand(input_ids.shape) < 0.15`\n",
    "2. `input_ids = torch.where(mask, 0, input_ids)`\n",
    "\n",
    "Как сгенерировать случайные элементы словаря на каждый элемент батча: `torch.randint_like(input_ids, low=num_special_tokens, high=self._tokenizer.vocab_size)`.\n",
    "\n",
    "В `Collator` нужно также:\n",
    "1. сделать паддинг.\n",
    "2. из (примерно) 15% выбранных токенов 10% поменять на случайные и 10% оставить в исходном виде, остальные замаскировать\n",
    "3. сформировать таргеты. Нам нужно понимать, какие именно 15% токенов мы выбрали для предсказания + какие были исходные метки для них.\n",
    "\n",
    "Важно: `[CLS]` и другие специальные токены токены маскировать не надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class Collator:\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer,\n",
    "            non_target_idx=-100,\n",
    "            mask_prob=0.15,\n",
    "            random_prob=0.1,\n",
    "            keep_unchanged_prob=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "            tokenizer: токенизатор\n",
    "            non_target_idx: значение для индексов, не использующихся как таргеты. \n",
    "                Используйте его, чтобы пометить \"не таргет\" токены\n",
    "            mask_prob: вероятность выбрать индекс как таргет\n",
    "            random_prob: вероятность для уже выбранного индекса поменять его на случайное значение вместо маскирования\n",
    "            keen_unchanged_prob: вероятность оставить индекс в исходном виде вместо маскирования\n",
    "        \"\"\"\n",
    "        self._tokenizer = tokenizer\n",
    "        self._non_target_idx = non_target_idx\n",
    "        self._mask_prob = mask_prob\n",
    "        self._random_prob = random_prob\n",
    "        self._keep_unchanged_prob = keep_unchanged_prob\n",
    "        \n",
    "        self._special_tokens_ids = range(len(self._tokenizer._tokenizer.special_tokens_map))\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "            batch: список вида [ds[i] for i in [12, 3, 2, 5]]\n",
    "            \n",
    "            returns: \n",
    "                input_ids: испорченные входные индексы токенов с замаскированными значениями\n",
    "                token_type_ids: сегментные эмбеддинги\n",
    "                labels: истинные значения входных индексов, как таргеты\n",
    "                permuted: был ли свап сегментов\n",
    "        \"\"\"\n",
    "        \n",
    "        #######################################\n",
    "        # input_ids\n",
    "        #######################################\n",
    "        \n",
    "        # padding items\n",
    "        initial_input_ids = pad_sequence([item[0] for item in batch], batch_first=True, padding_value=self._tokenizer.pad_token_id)\n",
    "        input_ids = deepcopy(initial_input_ids)\n",
    "\n",
    "        # choosing tokens to replace\n",
    "        replace_mask = torch.rand(input_ids.shape) < self._mask_prob\n",
    "        dont_replace_special_tokens_mask = ~sum(input_ids==i for i in self._special_tokens_ids).bool() # False if token is special\n",
    "        final_replace_mask = replace_mask & dont_replace_special_tokens_mask\n",
    "        \n",
    "        # choosing items that tokens\n",
    "        # will be replaced with\n",
    "        rand = torch.rand(input_ids.shape)\n",
    "        random_tokens_mask = rand < self._random_prob\n",
    "        leave_tokens_mask = (rand >= self._random_prob) & (rand < self._random_prob + self._keep_unchanged_prob)\n",
    "        replace_with_mask_mask = ~random_tokens_mask & ~leave_tokens_mask # all other tokens\n",
    "        \n",
    "        # randoming tokens for random replacement\n",
    "        random_tokens = torch.randint_like(\n",
    "            input_ids,\n",
    "            low=len(self._tokenizer._tokenizer.special_tokens_map),\n",
    "            high=self._tokenizer._tokenizer.vocab_size\n",
    "        )\n",
    "        \n",
    "        # replacing tokens according to masks\n",
    "        input_ids = torch.where(replace_with_mask_mask & final_replace_mask, tokenizer.mask_token_id, input_ids)\n",
    "        input_ids = torch.where(random_tokens_mask & final_replace_mask, random_tokens, input_ids)\n",
    "        #######################################\n",
    "        \n",
    "        # in original implementation there are\n",
    "        # 3 tokens for segment embeddings but not 2\n",
    "        token_type_ids = pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)\n",
    "        \n",
    "        permuted = torch.tensor([item[2] for item in batch])\n",
    "        \n",
    "        labels = torch.where(final_replace_mask, initial_input_ids, self._non_target_idx)\n",
    "        \n",
    "        assert input_ids.shape == token_type_ids.shape == labels.shape\n",
    "        \n",
    "        return input_ids, token_type_ids, labels, permuted\n",
    "        #return initial_input_ids, input_ids, token_type_ids, labels, permuted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте `collator` и `dataloader`. Для предобучения предлагается использовать большой `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collator = Collator(tokenizer, non_target_idx=-100, mask_prob=1, random_prob=0.1)\n",
    "collator = Collator(tokenizer, non_target_idx=-100)\n",
    "\n",
    "dl = DataLoader(\n",
    "    ds, \n",
    "    collate_fn=collator, \n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_collator(ds, collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Создание модели (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве модели мы будем использовать энкодер трансформера точь-в-точь в таком же виде, как его использовали в оригинальной статье про BERT. \n",
    "\n",
    "Прежде чем начать писать составляющие энкодера, обсудим инициализацию весов. Для трансформера нам понадобится большое количество линейных слов (`nn.Linear`), у которых для инициализации по дефолту используется равномерное распределение и отсутствует зануление bias'ов: $$\\text{Uniform}\\left(-\\frac{1}{\\sqrt{N_{\\text{in_features}}}}, \\frac{1}{\\sqrt{N_{\\text{in_features}}}}\\right).$$\n",
    "\n",
    "В оригинальной статье про BERT для весов используется **TruncatedNormal** со стандартным отклонением 0.02, bias'ы инициализируются нулями и модель обучается значительно лучше (это можно в ходе домашнего задания проверить).\n",
    "\n",
    "Поэтому, после создания линейных слоев и матрицы эмбеддингов, необходимо в явном виде вызывать для них TruncatedNormal инициализацию:\n",
    "\n",
    "1. `layer = ...`\n",
    "2. `nn.init.trunc_normal_(layer.weight.data, std=0.02, a=-2 * 0.02, b=2 * 0.02)`.\n",
    "\n",
    "Для линейных слоев нужно также вызывать `layer.bias.data.zero_()`.\n",
    "\n",
    "\n",
    "**TruncatedNormal** распределение отличается от нормального тем, что если величины выходят за границы отрезка [a, b], для этих величин повторно происходит сэмплирование до тех пор, пока они не попадут в нужный отрезок. Для BERT stddev = 0.02:\n",
    "\n",
    "$$[a; b] = [- 2  \\cdot \\text{stddev}; 2 \\cdot \\text{stddev}].$$\n",
    "\n",
    "Напишите функцию для инициализации линейных слоев и матрицы эмбеддингов **TruncatedNormal** распределением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def init_layer(layer, initializer_range=0.02, zero_out_bias=True):\n",
    "    \"\"\"\n",
    "        layer: наследник nn.Module, т.е. слой в pytorch\n",
    "        initializer_range: stddev для truncated normal\n",
    "        zero_out_bias: True для линейных слоев, False для матрицы эмбеддингов\n",
    "    \"\"\"\n",
    "    nn.init.trunc_normal_(layer.weight.data, std=initializer_range, a=-2*initializer_range, b=2*initializer_range)\n",
    "    #nn.init.trunc_normal_(layer.weight.data, std=initializer_range, a=-2, b=2)\n",
    "    if zero_out_bias:\n",
    "        layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к созданию энкодера трансформера.\n",
    "\n",
    "<img src=\"images/transformer.png\" width=500 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем слой, создающий входные векторы токенов. Для этого нам нужны:\n",
    "1. Эмбеддинги токенов (`nn.Embedding`)\n",
    "2. Позиционные эмбеддинги (можно либо использовать `nn.Embedding`, либо явно создать матрицу эмбеддингов с помощью `nn.Parameter`)\n",
    "3. Сегментные эмбеддинги\n",
    "\n",
    "Эти три сущности складываются, затем идет layernorm и dropout.\n",
    "\n",
    "<img src=\"images/bert_input.png\" width=800 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size, \n",
    "            hidden_size, \n",
    "            max_seqlen,\n",
    "            dropout_prob=0., \n",
    "            type_vocab_size=2,\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        \"\"\"\n",
    "            vocab_size: размер словаря\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            max_seqlen: количество позиционных эмбеддингов\n",
    "            dropout_prob: вероятность дропаута в конце слоя\n",
    "            type_vocab_size: количество сегментных эмбеддингов\n",
    "            eps: eps для layernorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        init_layer(self._token_embeddings, zero_out_bias=False)\n",
    "        \n",
    "        self._sentence_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "        init_layer(self._sentence_embeddings, zero_out_bias=False)\n",
    "        \n",
    "        ###########################\n",
    "        # positional embeddings\n",
    "        ###########################\n",
    "        self._positional_embeddings = nn.Parameter(torch.zeros(max_seqlen, hidden_size))\n",
    "        self._positional_embeddings.requires_grad = False\n",
    "        \n",
    "        position = torch.arange(0, max_seqlen).unsqueeze(1)\n",
    "        div_term = (torch.arange(0, hidden_size, 2) * -(np.log(10000) / hidden_size)).exp()\n",
    "\n",
    "        self._positional_embeddings[:, 0::2] = torch.sin(position * div_term)\n",
    "        self._positional_embeddings[:, 1::2] = torch.cos(position * div_term)\n",
    "        ###########################\n",
    "        \n",
    "        self._layer_norm = nn.LayerNorm(normalized_shape=hidden_size, eps=eps)\n",
    "        self._dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "    def get_token_embeddings(self):\n",
    "        \"\"\"\n",
    "            returns: возвращает слой с матрицей эмбеддингов для токенов. Нужен для MLM головы\n",
    "        \"\"\"\n",
    "        return self._token_embeddings\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        \"\"\"\n",
    "            input_ids: тензор с индексами токенов\n",
    "            token_type_ids: сегментные индексы\n",
    "            \n",
    "            returns: эмбеддинги токенов\n",
    "        \"\"\"\n",
    "        token_embeddings = self._token_embeddings(input_ids)\n",
    "        sentence_embeddings = self._sentence_embeddings(token_type_ids)\n",
    "        positional_embeddings = self._positional_embeddings[:input_ids.size(1), :]\n",
    "        \n",
    "        embeddings = token_embeddings + sentence_embeddings + positional_embeddings\n",
    "        embeddings = self._layer_norm(embeddings)\n",
    "        \n",
    "        return self._dropout(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "bert_embeddings = BertEmbeddings(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    max_seqlen=ds._maxlen+2,\n",
    "    dropout_prob=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, token_type_ids, *_ = next(iter(dl))\n",
    "embeddings = bert_embeddings(input_ids, token_type_ids)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(bert_embeddings._positional_embeddings, cmap=\"YlGnBu\")\n",
    "plt.title(\"Visualization of positional embeddings\")\n",
    "plt.xlabel(\"Embedding coordinate\")\n",
    "plt.ylabel(\"№ of position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 7746560.\n"
     ]
    }
   ],
   "source": [
    "tests.test_bert_embeddings(BertEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выглядит одноголовый **SelfAttention**:\n",
    "    \n",
    "<img src=\"images/attention.png\" width=600 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выглядит многоголовый (multihead) **SelfAttention:**\n",
    "\n",
    "<img src=\"images/multihead.png\" width=200 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация **MultiHeadSelfAttention** — самая сложная часть энкодера. Дальше будет проще :)\n",
    "\n",
    "1. Принимаем на вход посл-ть векторов для каждого объекта в батче, т.е. тензор размера `batch_size x seqlen x dim`\n",
    "2. Получаем из исходных векторов векторы `query, key, value` с помощью линейного слоя. $W_q X, W_k X, W_v X$.\n",
    "    * **Важно:** не нужно делать три отдельных линейных слоя. Сделайте один линейный слой в три раза шире, затем после его применения разделите результат на три части с помощью метода `.chunk`. $W_{qkv} X$.\n",
    "3. Полученные query, key, value векторы делятся между \"головами\" аттеншна c помощью `.view`. Далее операции происходят для каждой головы отдельно.\n",
    "4. Нужно посчитать скалярные произведения всех запросов (queries) со всеми ключами (keys): $QK^T$.\n",
    "5. Заменить значения для паддинг токенов на очень маленькие (большие отрицательные), чтобы они не влияли на софтмакс:         \n",
    "    `attention_scores = attention_mask * attention_scores + (1 - attention_mask) * -100000`\n",
    "\n",
    "6. Применить Dropout аттеншн скоров, который  выкидывает из аттеншна токены целиком.\n",
    "7. Поделить \"аттеншны скоры\" на корень из размерности векторов и взять софтмакс по ключам. Т.е. $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}}, \\text{axis=1})$\n",
    "8. Посчитать контекстные векторы запросов $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}}, \\text{axis=1})V$.\n",
    "9. Сконкатенировать контекстные векторы всех голов и применить линейный слой той же размерности и dropout.\n",
    "10. Сложить со входом **MultiHeadSelfAttention** слоя, применить layernorm: $\\text{layernorm}(x + \\text{dropout}(f(x)))$.\n",
    "\n",
    "**Про аттеншн маску:**\n",
    "* В полном виде аттеншн маска имеет размерность `batch_size x seqlen x seqlen`\n",
    "* У нас же если токен не паддинг, то его видят остальные токены, поэтому по сути вся информация содержится в матрице размера `batch_size x seqlen` с предикатом является ли токен паддингом\n",
    "* Эту матрицу размера `batch_size x seqlen` можно привести к виду `batch_size x seqlen x seqlen` операцией вида `attention_mask[:, None] * torch.ones_like(attention_mask)[..., None]`\n",
    "\n",
    "**Вопросы:**\n",
    "1. Зачем нужно делить на корень из $d$ результаты скалярных произведений?\n",
    "2. Почему одно большое умножение на матрицу лучше, чем три маленьких?\n",
    "3. Что будет, если мы не будем заменять значения аттеншн скоров паддинг токенов на большие отрицательные значения?\n",
    "4. Какая вычислительная сложность (количество умножений) у операции **MultiheadSelfAttention**?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. Когда мы перемножаем Q и K, то мы переходим от размерностей `B x num_heads x seqlen x size_per_head` к `B x num_heads x seqlen x seqlen`, таким образом значения напрямую зависят от `size_per_head`. Чтобы значения `attention_scores` не зависели от `size_per_head`, мы делим на корень из `size_per_head`; своего рода нормализация.\n",
    "2. Не тратится время на инициализацию 3-х слоев. Операция выполняется параллельно, т.е. вместо последовательного выполнения 3-х параллельных умножений, мы сразу параллельно выполняем одно.\n",
    "3. Тогда после `softmax` у ембеддингов всех токенов будет ненулевой attention с паддинг токенами, чего мы не хотим.\n",
    "4. - `seq_len` $= n$ \n",
    "   - `hidden_size` $= d$ \n",
    "   - получение Q, K, V: $O(n \\times d \\times 3d) = O(n \\times d^2)$\n",
    "   - (Q @ K.T) @ V: $O(n \\times n \\times d) = O(n^2 \\times d)$\n",
    "   - итого: $O(n \\times d^2 + n^2 \\times d)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size,\n",
    "            num_attention_heads,\n",
    "            attention_probs_dropout_prob=0.,\n",
    "            dropout_prob=0.,\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            num_attention_heads: количество голов аттеншна. Обычно выбирается как hidden_size / num_attention_heads = 64,\n",
    "                т.е. размерность векторов у одной головы 64\n",
    "            attention_probs_dropout_prob: вероятность дропаута для аттеншн скоров\n",
    "            dropout_prob: вероятность дропаута в конце слоя (перед суммой со входами)\n",
    "            eps: eps для layernorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        \n",
    "        self._W_qkv = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        init_layer(self._W_qkv)\n",
    "        \n",
    "        self._attention_scores_dropout = nn.Dropout(p=attention_probs_dropout_prob)\n",
    "        \n",
    "        self._output_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        init_layer(self._output_linear)\n",
    "        \n",
    "        self._output_dropout = nn.Dropout(p=dropout_prob)\n",
    "        self._ouput_layer_norm = nn.LayerNorm(normalized_shape=hidden_size, eps=eps)\n",
    "        \n",
    "    @property\n",
    "    def size_per_head(self):\n",
    "        \"\"\"\n",
    "            returns: размерность векторов для одной головы\n",
    "        \"\"\"\n",
    "        return self._hidden_size // self._num_attention_heads\n",
    "    \n",
    "    def forward(self, embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "            embeddings: входные эмбеддинги\n",
    "            attention_mask: тензор из 0, 1 размерности batch_size x seqlen x seqlen\n",
    "            \n",
    "            returns: контекстные векторы\n",
    "        \"\"\"\n",
    "        batch_size = embeddings.size(0)\n",
    "        \n",
    "        # B x seqlen x hid_size\n",
    "        Q, K, V = self._W_qkv(embeddings).chunk(3, dim=-1)\n",
    "        \n",
    "        # B x num_heads x seqlen x size_per_head (size_per_head x num_heads = hid_size)\n",
    "        Q, K, V = [m.view(batch_size, -1, self._num_attention_heads, self.size_per_head).transpose(1, 2) for m in [Q, K, V]]\n",
    "        \n",
    "        # B x num_heads x seqlen x seqlen\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) \n",
    "        if attention_mask is not None:\n",
    "            # B x 1 x seqlen x seqlen\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "            attention_scores = attention_mask * attention_scores + (1 - attention_mask) * -100000\n",
    "        attention_scores = self._attention_scores_dropout(attention_scores)\n",
    "        \n",
    "        # B x num_heads x seqlen x seqlen\n",
    "        probabilities = F.softmax(attention_scores / self.size_per_head, dim=-1)\n",
    "        \n",
    "        # B x num_heads x seqlen x size_per_head\n",
    "        context_query_vectors = torch.matmul(probabilities, V)\n",
    "        \n",
    "        # B x seqlen x hid_size\n",
    "        context_query_vectors = context_query_vectors.transpose(1, 2).contiguous().view(batch_size, -1, self._hidden_size)\n",
    "        \n",
    "        # B x seqlen x hid_size\n",
    "        # x = layer_norm(x + dropout(f(x)))\n",
    "        out = self._ouput_layer_norm(embeddings + self._output_dropout(self._output_linear(context_query_vectors)))\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 256])\n",
      "torch.Size([32, 7, 7])\n",
      "torch.Size([32, 7, 256])\n"
     ]
    }
   ],
   "source": [
    "attention_mask = torch.ones(embeddings.size(0), embeddings.size(1))\n",
    "attention_mask = attention_mask[:, None] * torch.ones_like(attention_mask)[..., None]\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(attention_mask.shape)\n",
    "\n",
    "multi_head_self_attention = MultiHeadSelfAttention(\n",
    "    hidden_size=hidden_size,\n",
    "    num_attention_heads=hidden_size//64\n",
    ")\n",
    "\n",
    "hidden = multi_head_self_attention(embeddings, attention_mask)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters: 263680.\n"
     ]
    }
   ],
   "source": [
    "tests.test_attention(MultiHeadSelfAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать **полносвязный слой** гораздо проще - $\\text{layernorm}(\\text{dropout}(W_2 f(W_1 x + b_1) + b_2) + x)$:\n",
    "1. Линейный слой, расширяющий входные векторы до *intermediate_size*, который традиционно равен 4 * hidden_size, т.е. происходит расширение в четыре раза\n",
    "2. Функция активации (больше вы их нигде в модели не увидите)\n",
    "3. Линейный слой, сужающий векторы обратно до *hidden_size*\n",
    "4. Dropout, сложение со входом полносвязного слоя, layernorm\n",
    "\n",
    "**Вопросы:**\n",
    "1. Что дает \"расширение\" первым линейным слоем? Нельзя ли делать линейный слой поменьше?\n",
    "2. Какая вычислительная сложность (количество умножений) у операции?\n",
    "3. Используются ли где-то еще в трансформере функции активации (если не считать softmax функцией активации)?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. В статье про трансформер нашел:\n",
    "    - While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "    \n",
    "   То есть разным слоям энкодера нужны разные параметры, т.к. фичи становятся более высокоуровневыми.\n",
    "2. $O(n \\times d \\times 4d + n \\times 4d \\times d) = O(n \\times d^2)$\n",
    "3. Поиск `activation` по статье трансформера вернул единственный результат :) (в **BERT** также, они только используют `gelu`); для SOP пригодится tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            intermediate_size, \n",
    "            dropout_prob=0., \n",
    "            act_func='relu', \n",
    "            eps=1e-3\n",
    "    ):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            intermediate_size: размерность промежуточно слоя. Обычно 4 * hidden_size\n",
    "            dropout_prob: вероятность дропаута перед суммой со входными представлениями\n",
    "            act_func: функция активации. Должны быть доступны gelu, relu\n",
    "            eps: eps для layernorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._hidden_size = hidden_size\n",
    "        self._intermediate_size = intermediate_size\n",
    "        \n",
    "        self._linear_in = nn.Linear(hidden_size, intermediate_size)\n",
    "        init_layer(self._linear_in)\n",
    "        \n",
    "        if act_func == 'gelu':\n",
    "            self._act = nn.GELU()\n",
    "        else:\n",
    "            self._act = nn.ReLU()\n",
    "        \n",
    "        self._linear_out = nn.Linear(intermediate_size, hidden_size)\n",
    "        init_layer(self._linear_out)\n",
    "        \n",
    "        self._dropout = nn.Dropout(p=dropout_prob)\n",
    "        self._layer_norm = nn.LayerNorm(normalized_shape=hidden_size, eps=eps)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "            embeddings: входные эмбеддинги размерности batch_size x seqlen x hidden_size\n",
    "            \n",
    "            returns: преобразованные эмбеддинги той же размерности\n",
    "        \"\"\"\n",
    "        x = self._linear_in(embeddings)\n",
    "        x = self._act(x)\n",
    "        x = self._linear_out(x)\n",
    "        x = self._dropout(x) + embeddings\n",
    "        x = self._layer_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 256])\n"
     ]
    }
   ],
   "source": [
    "feed_forward = FeedForward(\n",
    "    hidden_size=hidden_size, \n",
    "    intermediate_size=hidden_size*4\n",
    ")\n",
    "out = feed_forward(hidden)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 526080.\n"
     ]
    }
   ],
   "source": [
    "tests.test_feedforward(FeedForward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим **MultiHeadSelfAttention** и **Feedforward** в один блок энкодера. Они применяются последовательно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            intermediate_size, \n",
    "            num_attention_heads, \n",
    "            dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            act_func='relu',\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._multihead_attention = MultiHeadSelfAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            dropout_prob=dropout_prob,\n",
    "            eps=eps\n",
    "        )\n",
    "        \n",
    "        self._feedforward = FeedForward(\n",
    "            hidden_size=hidden_size,\n",
    "            intermediate_size=intermediate_size,\n",
    "            act_func=act_func,\n",
    "            eps=eps,\n",
    "            dropout_prob=dropout_prob\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = self._multihead_attention(x, attention_mask)\n",
    "        x = self._feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 256])\n"
     ]
    }
   ],
   "source": [
    "bert_layer = BertLayer(\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=hidden_size*4,\n",
    "    num_attention_heads=hidden_size//64,\n",
    ")\n",
    "\n",
    "out = bert_layer(embeddings, attention_mask)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 789760.\n"
     ]
    }
   ],
   "source": [
    "tests.test_bert_layer(BertLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объедините **BertEmbeddings** и произвольное заданное число **BertLayer** слоёв в один слой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size,\n",
    "            max_seqlen,\n",
    "            hidden_size,\n",
    "            num_hidden_layers,\n",
    "            intermediate_size, \n",
    "            num_attention_heads, \n",
    "            input_dropout_prob=0.,\n",
    "            dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            act_func='relu',\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._embeddings = bert_embeddings = BertEmbeddings(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            max_seqlen=max_seqlen,\n",
    "            dropout_prob=input_dropout_prob,\n",
    "            type_vocab_size=2,\n",
    "            eps=eps\n",
    "        )\n",
    "        \n",
    "        self._layers = nn.ModuleList([\n",
    "            BertLayer(\n",
    "                hidden_size=hidden_size, \n",
    "                intermediate_size=intermediate_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                dropout_prob=dropout_prob, \n",
    "                attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                act_func=act_func,\n",
    "                eps=eps\n",
    "            ) for _ in range(num_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "    def get_token_embeddings(self):\n",
    "        \"\"\"\n",
    "            returns: эмбеддинги токенов (матрицу эмбеддингов)\n",
    "        \"\"\"\n",
    "        return self._embeddings.get_token_embeddings()\n",
    "    \n",
    "    @staticmethod\n",
    "    def expand_mask(attention_mask):\n",
    "        \"\"\"\n",
    "            attention_mask: маска паддинга размерности batch_size x seqlen\n",
    "            \n",
    "            returns: маска паддинга размерности batch_size x seqlen x seqlen\n",
    "        \"\"\"\n",
    "        return attention_mask[:, None] * torch.ones_like(attention_mask)[..., None]\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, token_type_ids=None):\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = Bert.expand_mask(attention_mask)\n",
    "        \n",
    "        x = self._embeddings(x, token_type_ids)\n",
    "        \n",
    "        for layer in self._layers:\n",
    "            x = layer(x, attention_mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, token_type_ids, labels, permuted = next(iter(dl))\n",
    "attention_mask = torch.ones(embeddings.size(0), embeddings.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7, 256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = Bert(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seqlen=ds._maxlen,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=hidden_size*4,\n",
    "    num_attention_heads=hidden_size//64,\n",
    "    input_dropout_prob=0.01,\n",
    "    dropout_prob=0.01, \n",
    "    attention_probs_dropout_prob=0.01,\n",
    "    act_func='gelu',\n",
    ")\n",
    "\n",
    "out = bert(x=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 10905600.\n"
     ]
    }
   ],
   "source": [
    "tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для предобучения (и для целевых задач) необходимо задать \"головы\" модели:\n",
    "\n",
    "Голова для **MLM** задачи выглядит как $W_2 \\text{layernorm} (f(W_1 x + b_1)) + b_2$:\n",
    "1. Линейный слой $d \\times d$\n",
    "2. Функция активации\n",
    "3. LayerNorm\n",
    "4. Линейный слой $d \\times |V|$, где $|V|$ --- размер словаря. **Важно:** в качестве матрицы, на которую происходит умножение при аффинном преобразовании, берется матрица эмбеддингов токенов.\n",
    "5. Функционал ошибки тоже будем считать сразу в голове, для него используется **nn.CrossEntropyLoss**: \n",
    "    * `self._criterion(preds.view(-1, self._vocab_size), labels.view(-1))`\n",
    "\n",
    "Чтобы использовать матрицу входных эмбеддингов вместо последнего линейного слоя в голове, можно использовать присваивание вида`self._decoder.weight = input_embeddings.weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlmHead(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            hidden_act, \n",
    "            eps=1e-3, \n",
    "            ignore_index=-100, \n",
    "            input_embeddings=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            vocab_size: размер словаря\n",
    "            hidden_act: функция активации\n",
    "            eps: eps для layernorm\n",
    "            ignore_index: индекс таргета, который необходимо игнорировать при подсчете лосса\n",
    "            input_embeddings: слой с эмбеддингами токенов, для использования матрицы эмбеддингов вместо линейного слоя\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._vocab_size = vocab_size\n",
    "        \n",
    "        self._linear_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        init_layer(self._linear_1)\n",
    "        \n",
    "        if hidden_act == 'gelu':\n",
    "            self._act = nn.GELU()\n",
    "        else:\n",
    "            self._act = nn.ReLU()\n",
    "        \n",
    "        self._layer_norm = nn.LayerNorm(normalized_shape=hidden_size, eps=eps)\n",
    "        \n",
    "        self._linear_2 = nn.Linear(hidden_size, vocab_size)\n",
    "        self._linear_2.weight = input_embeddings.weight\n",
    "        self._linear_2.bias.data.zero_()\n",
    "        \n",
    "        self._criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, hidden_states, labels):\n",
    "        \"\"\"\n",
    "            hidden_states: эмбеддинги токенов\n",
    "            labels: истинные метки, т.е. изначальные индексы токенов\n",
    "            \n",
    "            returns: посчитанный лосс\n",
    "        \"\"\"\n",
    "        x = self._linear_1(hidden_states)\n",
    "        x = self._act(x)\n",
    "        x = self._layer_norm(x)\n",
    "        x = self._linear_2(x)\n",
    "        \n",
    "        loss = self._criterion(x.view(-1, self._vocab_size), labels.view(-1))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.3977, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "mlm_head = MlmHead(\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_act='gelu',\n",
    "    input_embeddings=bert_embeddings._token_embeddings\n",
    ")\n",
    "\n",
    "loss = mlm_head(out, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 7776304.\n"
     ]
    }
   ],
   "source": [
    "tests.test_mlm_head(MlmHead, BertEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Голова для **SOP**-задачи выглядит аналогично и в оригинальной статье называется \"pooler-слоем\":\n",
    "1. Берем скрытое представление CLS токена\n",
    "2. Линейный слой $d \\times d$\n",
    "3. Функция активации, причем в качестве функции активации используется гиперболический тангенс **nn.Tanh**\n",
    "4. Dropout\n",
    "5. Линейный слой\n",
    "6. Функционал ошибки (бинарная кросс-энтропия с логитами, **nn.BCEWithLogitsLoss**)\n",
    "\n",
    "Эту голову (кроме последнего линейного слоя) мы будем использовать также и для целевой задачи (классификации чеков)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    CLS_POSITION = 0\n",
    "    CRITERION = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def __init__(self, hidden_size, num_classes=1, hidden_dropout_prob=0.):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            hidden_dropout_prob: вероятность дропаута\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._linear_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        init_layer(self._linear_1)\n",
    "        \n",
    "        self._act = nn.Tanh()\n",
    "        self._dropout = nn.Dropout(p=hidden_dropout_prob)\n",
    "        \n",
    "        self._linear_2 = nn.Linear(hidden_size, num_classes)\n",
    "        init_layer(self._linear_2)\n",
    "\n",
    "    def forward(self, hidden_states, permuted=None):\n",
    "        \"\"\"\n",
    "            hidden_states: эмбеддинги\n",
    "            permuted: таргеты (были ли свапы сегментов). Если их нет, то необходимо выдать предсказания\n",
    "        \"\"\"\n",
    "        \n",
    "        cls_states = hidden_states[:, self.CLS_POSITION]\n",
    "        x = self._linear_1(cls_states)\n",
    "        x = self._act(x)\n",
    "        x = self._dropout(x)\n",
    "        x = self._linear_2(x)\n",
    "        \n",
    "        if permuted is None:\n",
    "            return x\n",
    "        else:\n",
    "            loss = self.CRITERION(x.view(-1), permuted.float())\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6938, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "classifier_head = ClassifierHead(\n",
    "    hidden_size=hidden_size\n",
    ")\n",
    "\n",
    "loss = classifier_head(out, permuted)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 66049.\n"
     ]
    }
   ],
   "source": [
    "tests.test_classifier_head(ClassifierHead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим **Bert**, **MlmHead** и **ClassifierHead** в единую модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            vocab_size,\n",
    "            max_seqlen,\n",
    "            num_hidden_layers,\n",
    "            intermediate_size,\n",
    "            num_attention_heads,\n",
    "            act_func='relu',\n",
    "            input_dropout_prob=0.,\n",
    "            hidden_dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            eps=1e-3, \n",
    "            ignore_index=-100\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._backbone = Bert(\n",
    "            vocab_size=vocab_size,\n",
    "            max_seqlen=max_seqlen,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            intermediate_size=intermediate_size, \n",
    "            num_attention_heads=num_attention_heads, \n",
    "            input_dropout_prob=input_dropout_prob,\n",
    "            dropout_prob=hidden_dropout_prob, \n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            act_func=act_func,\n",
    "            eps=eps\n",
    "        )\n",
    "        self._mlm_head = MlmHead(\n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            act_func, \n",
    "            eps, \n",
    "            ignore_index, \n",
    "            input_embeddings=self._backbone.get_token_embeddings()\n",
    "        )\n",
    "        self._classifier_head = ClassifierHead(\n",
    "            hidden_size, \n",
    "            hidden_dropout_prob=hidden_dropout_prob, \n",
    "            num_classes=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attention_mask, labels, permuted, token_type_ids=None):\n",
    "        hidden_states = self._backbone(x, attention_mask, token_type_ids)\n",
    "        mlm_loss = self._mlm_head(hidden_states, labels)\n",
    "        sop_loss = self._classifier_head(hidden_states, permuted)\n",
    "        # в оригинальном BERT лоссы MLP и NSP используются с равными весами\n",
    "        return 0.5 * mlm_loss + 0.5 * sop_loss, {'MLM': mlm_loss, 'SOP': sop_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения гиперпараметров:\n",
    "* для успешного выполнения задания достаточно архитектуры bert-mini: `hidden_size=256`, `num_hidden_layers=4`, в качестве функции активации можно использовать `gelu`\n",
    "* стандартные практики: `intermediate_size = 4 * hidden_size`, `num_attention_heads = hidden_size // 64`\n",
    "* в оригинальной статье везде dropout равен 0.1, но для bert-mini модели можно попробовать значения поменьше. Вопрос - почему?\n",
    "    1. Возможно, потому, что маленькая модель меньше подвержена переобучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(5.4959, grad_fn=<AddBackward0>), {'MLM': tensor(10.2949, grad_fn=<NllLossBackward>), 'SOP': tensor(0.6970, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)})\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "vocab_size = tokenizer.vocab_size\n",
    "max_seqlen = ds._maxlen + 2 # initial size + CLS + SEP\n",
    "num_hidden_layers = 4\n",
    "\n",
    "model = BertModel(\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seqlen=max_seqlen,\n",
    "    act_func='gelu',\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    intermediate_size=hidden_size*4,\n",
    "    num_attention_heads=hidden_size//64,\n",
    "    input_dropout_prob=0.01,\n",
    "    hidden_dropout_prob=0.01, \n",
    "    attention_probs_dropout_prob=0.01,\n",
    "    eps=1e-3, \n",
    "    ignore_index=-100\n",
    ")\n",
    "\n",
    "input_ids, token_type_ids, labels, permuted = next(iter(dl))\n",
    "attention_mask = torch.where(input_ids != tokenizer.pad_token_id, 1., 0.)\n",
    "\n",
    "loss = model(input_ids, attention_mask, labels, permuted, token_type_ids)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "1. Какая часть модели содержит наибольшее количество параметров? Эмбеддинги, аттеншн, полносвязные слои, голова?\n",
    "2. Зачем объединять параметры в голове и параметры матрицы эмбеддингов?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. Больше всего параметров получилось в MLM голове за счёт того, что там мы пересчитываем градиенты для эмбеддингов.\n",
    "2. Мне кажется, это своего рода skip connection для того, чтобы значения эмбеддингов быстрее учились."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mlm_head</th>\n",
       "      <td>7776304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings</th>\n",
       "      <td>7686912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feedforward</th>\n",
       "      <td>2104320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention</th>\n",
       "      <td>1054720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier_head</th>\n",
       "      <td>66049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 parameters\n",
       "mlm_head            7776304\n",
       "embeddings          7686912\n",
       "feedforward         2104320\n",
       "attention           1054720\n",
       "classifier_head       66049"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\"embeddings\": (vocab_size + max_seqlen + 4) * hidden_size,\n",
    "     \"attention\": (4 * hidden_size ** 2 + 6 * hidden_size) * num_hidden_layers,\n",
    "     \"feedforward\": (8 * hidden_size ** 2 + 7 * hidden_size) * num_hidden_layers,\n",
    "     \"mlm_head\": hidden_size ** 2 + 2 * hidden_size + hidden_size * vocab_size + hidden_size + vocab_size,\n",
    "     \"classifier_head\": hidden_size ** 2 + hidden_size + hidden_size + 1,\n",
    "     #\"bert\": (vocab_size + max_seqlen + 4) * hidden_size + 4 * (12 * hidden_size ** 2 + 13 * hidden_size),\n",
    "    }, orient=\"index\", columns=[\"parameters\"]).sort_values(by=\"parameters\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Оптимизация (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации будем использовать **AdamW**, отличия которого от ванильного **Adam** можно почитать, например, [вот здесь](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)\n",
    "\n",
    "Параметры модели, передаваемые в оптимизатор, следует поделить на две группы с помощью `model.named_parameters()`:\n",
    "1. Все `bias` и `layernorm` слои, присутствующие в модели (их можно выцепить по названию). Для них $l_2$ регуляризацию стоит выключить, т.е. поставить `weight_decay=0`\n",
    "2. Оставшиеся слои, для которых регуляризация не нужна.\n",
    "\n",
    "\n",
    "**Вопрос:** почему $l_2$ регуляризацию не используют для bias'ов? Для layernorm?\n",
    "- bias: нет ничего плохого в том, чтобы сдвигать предсказания модели на большое число; это не приводит к переобучению.\n",
    "- layernorm: честно говоря, не очень понял, зачем так делают. Кажется, что эти веса тоже нужно нормировать, ведь если на вход слою прилетают небольшие значения (т.к. на всех остальных весах висит регуляризация), то layernorm хоть и отнормирует вход, но потом может умножить его на огромный вес, что может привести к переобучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, weight_decay=0.01, init_lr=1e-3):\n",
    "    \"\"\"\n",
    "        model: инициализированная модель\n",
    "        weight_decay: коэффициент l2 регуляризации\n",
    "        \n",
    "        returns: оптимизатор\n",
    "    \"\"\"\n",
    "    decayed_parameters, not_decayed_parameters = [], []\n",
    "    \n",
    "    for name, params in model.named_parameters():\n",
    "        if any(layer_off in name for layer_off in [\"bias\", \"layer_norm\"]):\n",
    "            not_decayed_parameters.append(params)\n",
    "        else:\n",
    "            decayed_parameters.append(params)\n",
    "            \n",
    "    grouped_parameters = [\n",
    "        {'params': decayed_parameters, 'weight_decay': weight_decay},\n",
    "        {'params': not_decayed_parameters, 'weight_decay': 0.}\n",
    "    ]\n",
    "\n",
    "    return torch.optim.AdamW(grouped_parameters, lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_optimizer(get_optimizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выглядит типичное расписание lr для трансформеров:\n",
    "\n",
    "<img src=\"images/lr.png\" width=300 height=300 />\n",
    "\n",
    "Почему мы сразу не стартуем с большого значения lr? Для больших архитектур трансформера модель разойдется, произойдет взрыв градиентов. Постепенно же увеличить lr до большого значения — можно. Процедуру линейного увеличения lr до какого-то пикового значения называют `linear warmup`.\n",
    "\n",
    "Реализуйте такое \"треугольное\" расписание для learning rate в предложенном шаблоне.\n",
    "\n",
    "**Вопрос:** а зачем нужно убывание learning rate?\n",
    "- В самом начале оптимизации нам необходимо быстрее двигаться в сторону минимума. Через несколько итераций нам необходимо снижать значения градиентов, чтобы не перескочить локальный минимум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            optimizer, \n",
    "            init_lr, \n",
    "            peak_lr, \n",
    "            final_lr, \n",
    "            num_warmup_steps, \n",
    "            num_training_steps\n",
    "    ):\n",
    "        \"\"\"\n",
    "            optimizer: оптимизатор\n",
    "            init_lr: начальное значение learning rate\n",
    "            peak_lr: пиковое значение learning rate\n",
    "            final_lr: финальное значение lr\n",
    "            num_warmup_steps: количество шагов разогрева (сколько шагов идем от начального до пикового значения)\n",
    "            num_training_steps: количество шагов обучения (количество батчей x количество эпох)\n",
    "            \n",
    "        \"\"\"\n",
    "        self._optimizer = optimizer\n",
    "        self._step = 0\n",
    "        \n",
    "        self._lrs = np.concatenate((\n",
    "            # skip init_lr as optimizer goes first\n",
    "            np.linspace(init_lr, peak_lr, num_warmup_steps + 1)[1:],\n",
    "            np.linspace(peak_lr, final_lr, num_training_steps - num_warmup_steps + 1)[1:],\n",
    "            # adding this because we have extra useless step at the end\n",
    "            np.array([final_lr])))\n",
    "        \n",
    "        #assert len(self._lrs) == num_training_steps + 1\n",
    "\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "            Меняет learning rate для оптимизатора\n",
    "            \n",
    "            Поменять learning rate для группы параметров в оптимизаторе можно присваиванием вида param_group['lr'] = lr\n",
    "        \"\"\"\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = self._lrs[self._step]\n",
    "            \n",
    "        self._step += 1\n",
    "        \n",
    "    def get_last_lr(self):\n",
    "        \"\"\"\n",
    "            returns: текущий learning rate оптимизатора. Нужно для логгирования\n",
    "        \"\"\"\n",
    "        return [param_group['lr'] for param_group in self._optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=0.01, init_lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = Scheduler(\n",
    "    optimizer, \n",
    "    init_lr=1, \n",
    "    peak_lr=5, \n",
    "    final_lr=2, \n",
    "    num_warmup_steps=3,\n",
    "    num_training_steps=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.333333333333333\n",
      "3.6666666666666665\n",
      "5.0\n",
      "4.25\n",
      "3.5\n",
      "2.75\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_scheduler(Scheduler, get_optimizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От запуска обучения нас отделяет только создание `Trainer`. От объектов класса `Trainer` требуется, чтобы:\n",
    "* логгировался лосс на каждом батче (`torch.utils.tensorboard.SummaryWriter`, `writer.add_scalar`)\n",
    "* клипались и логгировались нормы градиентов при каждом шаге спуска (`torch.nn.utils.clip_grad_norm_` возвращает нормы градиентов)\n",
    "* логгировались значения learning rate\n",
    "* была поддержана аккумуляция градиентов, нужная для эмуляции больших батчей\n",
    "\n",
    "При предобучении не нужно использовать какую-либо форму валидации, достаточно смотреть на батч лосс.\n",
    "\n",
    "Предлагается также для ускорения обучения использовать mixed precision из библиотеки `apex`:\n",
    "* перед обучением необходимо вызвать строчку вида `model, optimizer = amp.initialize(model, optimizer, opt_level='O1')`\n",
    "* при обучении `.backward()` надо делать в контекстном менеджере:     \n",
    "   `with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward()`\n",
    "        \n",
    "Что такое аккумуляция градиентов:\n",
    "* При использовании Adam в видеопамяти необходимо хранить градиенты и квадраты частных производных\n",
    "* При подсчете градиента по очередному батчу необязательно сразу делать шаг спуска, можно запомнить градиент, а затем посчитать градиент по другому батчу c теми же параметрами модели\n",
    "* Теперь эти два градиента можно сложить и получить градиент, который был посчитан как будто по одному большому батчу (составленному из этих двух). Сэмулировали большой батч. В данном случае количество шагов аккумуляции равно двум.\n",
    "* В данном случае количество шагов аккумуляции равно двум.\n",
    "\n",
    "Зачем нужны большие батчи? Обучение быстрее, оценки градиента точнее, позволяет увеличивать learning rate. Например, при предобучении авторы RoBERTA значительно увеличили размер батча по сравнению с ванильным BERT и получили прирост к качеству решения целевых задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from apex import amp\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            scheduler,\n",
    "            pad_token_id,\n",
    "            device,\n",
    "            num_accum_steps=1,\n",
    "            logdir=None,\n",
    "            max_grad_norm=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            scheduler: расписание learning rate. Нужно вызывать scheduler.step() ПОСЛЕ optimizer.step()\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            num_accum_steps: количество шагов аккумуляции\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        self._pad_token_id = pad_token_id\n",
    "        self._device = device\n",
    "        self._num_accum_steps = num_accum_steps\n",
    "        self._logdir = logdir\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self._model = model.to(self._device)\n",
    "        \n",
    "        #self._model, self._optimizer = amp.initialize(self._model, self._optimizer, opt_level='O1')\n",
    "        \n",
    "        if self._logdir is not None:\n",
    "            self._writer = SummaryWriter(log_dir=self._logdir, flush_secs=1)\n",
    "\n",
    "        self._n_epoch = 0\n",
    "        self._n_iter = 0\n",
    "\n",
    "    def train(self, dataloader, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "            self._train_step(dataloader)\n",
    "            self._n_epoch += 1\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        for input_ids, token_type_ids, labels, permuted in tqdm(dataloader, desc=f\"Epoch {self._n_epoch}\"):\n",
    "            \n",
    "            input_ids = input_ids.to(self._device)\n",
    "            token_type_ids = token_type_ids.to(self._device)\n",
    "            labels = labels.to(self._device)\n",
    "            permuted = permuted.to(self._device)\n",
    "            attention_mask = torch.where(input_ids != self._pad_token_id, 1., 0.).to(self._device)\n",
    "            \n",
    "            self._optimizer.zero_grad()        \n",
    "            loss = self._model(input_ids, attention_mask, labels, permuted, token_type_ids)\n",
    "            loss[0].backward()\n",
    "            if self._max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n",
    "            self._optimizer.step()\n",
    "            self._scheduler.step()\n",
    "            \n",
    "            #loss = self._model(input_ids, attention_mask, labels, permuted, token_type_ids)\n",
    "            #with amp.scale_loss(loss, self._optimizer) as scaled_loss:\n",
    "            #    scaled_loss.backward()\n",
    "            #if self._n_iter % self._num_accum_steps == 0:\n",
    "            #    torch.nn.utils.clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n",
    "            #    self._optimizer.step()\n",
    "            #    self._optimizer.zero_grad()        \n",
    "            #self._scheduler.step()\n",
    "            \n",
    "            if self._logdir is not None:\n",
    "                self._writer.add_scalar(\"loss/total\", loss[0].item(), global_step=self._n_iter)\n",
    "                self._writer.add_scalar(\"loss/MLM\", loss[1][\"MLM\"].item(), global_step=self._n_iter)\n",
    "                self._writer.add_scalar(\"loss/SOP\", loss[1][\"SOP\"].item(), global_step=self._n_iter)\n",
    "                self._writer.add_scalar(\"lr\", self._optimizer.param_groups[0][\"lr\"], global_step=self._n_iter)\n",
    "                \n",
    "            self._n_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите и сохраните предобученную модель с помощью `torch.save`. \n",
    "\n",
    "**Важно:** тензорборд логи успешного обучения необходимо сложить в архив и приложить вместе с решенным заданием.\n",
    "\n",
    "Про гиперпараметры:\n",
    "* `weight_decay` - $0.1, 0.01, 0.001$ и т.д.\n",
    "* расписание lr - bert-mini не очень чувствителен к линейному вормапу, поэтому существенное влияние оказывают только пиковое и финальное значение lr. Пиковое значение стоит поискать где-то в масштабе 1e-3 - 1e-4, финальный lr можно сделать очень маленьким.\n",
    "* конкретное значение для клиппинга нормы особо ни на что не влияет, как правило (и в оригинальной статье тоже) его всегда ставят единицой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "#batch_size = 1024\n",
    "batch_size = 256\n",
    "max_grad_norm = 1\n",
    "num_accum_steps = 1\n",
    "\n",
    "weight_decay = 0.01\n",
    "init_lr = 1e-4\n",
    "peak_lr = 1e-3\n",
    "final_lr = 1e-5\n",
    "\n",
    "#logdir = \"/content/logs/\"\n",
    "logdir = \"./logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    ds, \n",
    "    collate_fn=collator, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_training_steps = len(dl) * n_epochs\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "\n",
    "optimizer = get_optimizer(model, weight_decay=weight_decay, init_lr=init_lr)\n",
    "\n",
    "scheduler = Scheduler(\n",
    "    optimizer, \n",
    "    init_lr=init_lr, \n",
    "    peak_lr=peak_lr, \n",
    "    final_lr=final_lr, \n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    device=device,\n",
    "    num_accum_steps=num_accum_steps,\n",
    "    logdir=logdir,\n",
    "    max_grad_norm=max_grad_norm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total loss | MLM loss | SOP loss\n",
    "- | - | -\n",
    "<img src=\"./images/training_results/loss_total.svg\" width=\"100%\"/> | <img src=\"./images/training_results/loss_MLM.svg\" width=\"100%\"/> | <img src=\"./images/training_results/loss_SOP.svg\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "lr | _\n",
    "- | -\n",
    "<img src=\"./images/training_results/lr.svg\" width=\"100%\"/> | -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/training_results/lr.svg\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.train(dl, n_epochs=n_epochs)\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    './checkpoints/pretrained_weights.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После предобучения вам придется перезапустить ноутбук и снова перепрогнать блоки, нужные для дообучения. Использование apex'а ломает обучение других моделей (которые не передавались в `amp.initialize`) в одном запуске. Если не перезапустить, скор получится гораздо хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. Дообучение (5 баллов)\n",
    "\n",
    "Самая сложная часть уже позади, осталось чуть-чуть :)\n",
    "\n",
    "Так как для дообучения доступно гораздо меньше данных, оно занимает гораздо меньше времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.loc[data['split'] == 'train'].reset_index(drop=True).copy()\n",
    "val = data.loc[data['split'] == 'val'].reset_index(drop=True).copy()\n",
    "test = data.loc[data['split'] == 'test'].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет для дообучения выглядит стандартно: нужно токенизировать и запомнить тексты и соответствующие им метки, и затем в методе `__getitem__` их выдавать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            texts, \n",
    "            targets, \n",
    "            tokenizer,\n",
    "            maxlen, \n",
    "            presort=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "            texts: list of strings. Тексты чеков\n",
    "            targets: list of ints. Категории товаров\n",
    "            tokenizer: токенизатор\n",
    "            maxlen: максимальная длина текста\n",
    "            presort: отсортировать тексты по длине\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: input_ids - индексы токенов токенизированного текста, target - категория\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "        return input_ids, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте датасеты для обучения и валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FinetuneDataset(\n",
    "    train['text'].values, \n",
    "    train['label'].values, \n",
    "    maxlen=..., \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "val_ds = FinetuneDataset(\n",
    "    val['text'].values, \n",
    "    val['label'].values, \n",
    "    maxlen=..., \n",
    "    tokenizer=tokenizer, \n",
    "    presort=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллатор для дообучения делает только паддинг и конвертацию таргетов в тензоры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_idx):\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте даталоадеры для обучения и валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = ...\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        collate_fn=lambda batch: collate_fn(batch, tokenizer.pad_token_id)\n",
    "    ),\n",
    "    'eval': DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        collate_fn=lambda batch: collate_fn(batch, tokenizer.pad_token_id)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели теперь отсутствует MLM голова, а вместо SOP задачи голова классификации решает задачу определения категорий товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFinetuneModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            max_seqlen,\n",
    "            num_hidden_layers,\n",
    "            intermediate_size,\n",
    "            num_attention_heads,\n",
    "            num_classes,\n",
    "            act_func='relu',\n",
    "            input_dropout_prob=0.,\n",
    "            hidden_dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._backbone = Bert(\n",
    "            vocab_size=vocab_size,\n",
    "            max_seqlen=max_seqlen,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            intermediate_size=intermediate_size, \n",
    "            num_attention_heads=num_attention_heads, \n",
    "            input_dropout_prob=input_dropout_prob,\n",
    "            dropout_prob=hidden_dropout_prob, \n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            act_func=act_func,\n",
    "            eps=eps\n",
    "        )\n",
    "        self._classifier_head = ClassifierHead(hidden_size, num_classes, hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        hidden_states = self._backbone(x, attention_mask)\n",
    "        return self._classifier_head(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используйте ту же архитектуру, которую вы выбрали при предобучении. Количество классов - 96:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertFinetuneModel(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузить предобученные веса можно с помощью следующей функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(self, path):\n",
    "    found = []\n",
    "    with open(path, 'rb') as f:\n",
    "        weights = torch.load(f)\n",
    "    for name, param in weights.items():\n",
    "        if name in self.state_dict():\n",
    "            if param.shape == self.state_dict()[name].shape:\n",
    "                self.state_dict()[name].copy_(param)\n",
    "                found.append(name)\n",
    "\n",
    "    return found\n",
    "\n",
    "found = load_weights(model, 'pretrained_weights.pt')\n",
    "\n",
    "print('Amount of found weights: {}'.format(len(found)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте оптимизатор и расписание лр. Про гиперпараметры:\n",
    "* при дообучении используют маленький batch_size $\\in \\{32, 64\\}$\n",
    "* маленький learning rate:  $\\{1e-5, 2e-5, 4e-5\\}$ для больших моделей, для моделей вида bert-mini можно использовать и побольше: $\\{1e-4, 2e-4, 4e-4\\}$ \n",
    "* финальное значение все также маленькое\n",
    "* вормап можно делать где-то 0.06 от всех шагов обучения\n",
    "* количество эпох для дообучения - больше шести здесь не нужно\n",
    "* weight decay здесь потенциально можно использовать побольше, чем при предобучении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=...)\n",
    "scheduler = Scheduler(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось создать пайплайн обучения:\n",
    "* apex использовать не нужно, дообучение быстрое и не требует больших батчей\n",
    "* аккумуляция градиентов не нужна т.к. батчи  маленькие\n",
    "* лосс теперь считается вне модели, в Trainer нужно использовать torch.nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneTrainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            scheduler,\n",
    "            pad_token_id,\n",
    "            device,\n",
    "            logdir=None,\n",
    "            max_grad_norm=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            scheduler: расписание learning rate. Нужно вызывать scheduler.step() ПОСЛЕ optimizer.step()\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            num_accum_steps: количество шагов аккумуляции\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "\n",
    "    def train(self, dataloaders, n_epochs, scorer=None):\n",
    "        \"\"\"\n",
    "            dataloaders: dict of dataloaders, keys 'train', 'eval' should be present.\n",
    "            n_epochs: int. Num epochs to train for.\n",
    "            scorer: takes trainer, outputs metric name and value as a tuple.\n",
    "        \"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = self._train_step(dataloaders['train'])\n",
    "            val_loss = self._eval_step(dataloaders['eval'])\n",
    "\n",
    "            if self._writer is not None:\n",
    "                self._writer.add_scalar('eval', val_loss, global_step=self._n_epoch)\n",
    "                \n",
    "                if scorer is not None:\n",
    "                    name, value = scorer(self)\n",
    "                    self._writer.add_scalar(name, value, global_step=self._n_epoch)\n",
    "                    \n",
    "            self._n_epoch += 1\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: training dataloader.\n",
    "            \n",
    "            returns: train_loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "\n",
    "\n",
    "    def _eval_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: evaluation dataloader.\n",
    "            \n",
    "            returns: eval loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def predict(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: inference dataloader. Should not have targets.\n",
    "            \n",
    "            returns: np.array c предсказанными категориями\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = FinetuneTrainer(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для мониторинга целевой метрики используйте предоставленный scorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, maxlen, tokenizer):\n",
    "        \"\"\"\n",
    "            texts: list of str. Сырые тексты чеков\n",
    "            maxlen: максимальная длина текста\n",
    "            tokenizer: токенизатор\n",
    "        \"\"\"\n",
    "        self._texts = [\n",
    "            ([tokenizer.cls_token_id] + tokenizer(text) + [tokenizer.sep_token_id]) if tokenizer is not None else text \n",
    "            for text in texts\n",
    "        ]\n",
    "        self._maxlen = maxlen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: тензор из индексов токенов токенизированного текста\n",
    "        \"\"\"\n",
    "        text = self._texts[idx]\n",
    "        if self._maxlen is not None:\n",
    "            text = text[:self._maxlen]\n",
    "        return torch.tensor(text, dtype=torch.long)\n",
    "    \n",
    "def make_scorer(texts, targets, tokenizer, maxlen):\n",
    "    inference_ds = InferenceDataset(texts, maxlen=maxlen, tokenizer=tokenizer)\n",
    "    inference_dl = DataLoader(inference_ds, batch_size=32, shuffle=False, collate_fn=inference_collate_fn)\n",
    "    def get_score(trainer):\n",
    "        preds = trainer.predict(inference_dl)\n",
    "        return 'f1', f1_score(targets, preds, average='weighted')\n",
    "    return get_score\n",
    "\n",
    "\n",
    "inference_collate_fn = lambda x: pad_sequence(x, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "val_scorer = make_scorer(val['text'].values, val['label'].values, tokenizer, maxlen=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скор на валидационной выборке до обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scorer(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(dataloaders, n_epochs=..., scorer=val_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из критериев получения полного балла за задание является получение на тесте значения метрики $\\geqslant 0.8$. Скор на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scorer = make_scorer(test['text'].values, test['label'].values, tokenizer, maxlen=...)\n",
    "\n",
    "test_scorer(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не забудьте также приложить вместе со сделанным заданием тензорборд дообучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "1. Попробуйте также обучить модель без предобученных весов (просто закомментировав загрузку весов). Насколько сильно просело качество?\n",
    "2. Влияет ли длительность предобучения (количество эпох) как-то существенно на дообучение, или достаточно одной эпохи?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. ...\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть. Большие модели (максимум 5 баллов)\n",
    "\n",
    "Предлагается обучить модель побольше:\n",
    "* `hidden_size` $\\in \\{512, 768, 1024\\}$\n",
    "* `num_hidden_layers` $\\in \\{8, 12, 24\\}$\n",
    "\n",
    "Например, BERT-base архитектура выглядит как `hidden_size=768, num_hidden_layers=12`.\n",
    "\n",
    "Для большой модели придется также использовать другие гиперпараметры - нужен learning rate поменьше, weight decay побольше, дропаут больше. Возможно потребуется больше эпох предобучения.\n",
    "\n",
    "За выполнение этой части можно получить **до пяти бонусных баллов**, бонус зависит от полученных на тесте значений метрики (должно быть видно существенное улучшение)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
