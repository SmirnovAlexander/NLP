{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description\n",
    "\n",
    "## улучшение модели (4 балла)\n",
    "\n",
    "Вы реализовали бейзлайн, пришло время улучшить качество модели. Т.к. это последнее задание, мы не будем предлагать конкретные шаги, а только дадим несколько советов.\n",
    "\n",
    "1. Большой источник информации о работе командной строке — её документация, man. Один из способов улучшения модели - использование мана для генерации новых примеров. Структурированный ман можно найти по ссылке https://github.com/IBM/clai/blob/nlc2cmd/docs/manpage-data.md.\n",
    "2. Ещё один способ улучшить модель, разделить предсказание утилит и флагов. Т.к. задача предсказания утилит более важная, вы можете натренировать модель, которая предсказывает последовательность утилит, а затем к каждой утилите генерировать флаги.\n",
    "3. Можно аугментировать данные, чтобы увеличить выборку.\n",
    "4. Можно в качество входа подавать не только текстовый запрос, но и описание из мана. Т.к. всё описание достаточно большое, нужно сделать дополнительную модель, которая будет выбирать команды, для которых нужно вытащить описание.\n",
    "5. Найти дополнительные данные, улучшающие обучение\n",
    "6. Как всегда можно просто сделать больше слоёв, увеличить размер скрытого слоя и т.д.\n",
    "\n",
    "## comments\n",
    "\n",
    "After hours of trying to randomly make score better, I decided to focus on reproducibility of runs, so we will log everything just in case not to lose any information about past runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standart libraries\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import regex\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm, trange\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "\n",
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "# cmd preprocessing & metric calculation\n",
    "import sys\n",
    "sys.path.append(\"./utils/\")\n",
    "from bashlint.data_tools import bash_parser, pretty_print, cmd2template\n",
    "from metric.metric_utils import compute_metric\n",
    "\n",
    "# text preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertModel, EncoderDecoderConfig, EncoderDecoderModel\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 322\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "MAX_TEXT_LENGTH = 256\n",
    "MAX_CODE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# looking up for specific stop words\n",
    "tfidf = TfidfVectorizer(max_df=0.3, min_df=1)\n",
    "tfidf.fit(train_data[\"invocation\"].apply(lambda x: \" \".join([stemmer.stem(t) for t in x.split()])))\n",
    "specific_stop_words = tfidf.stop_words_\n",
    "specific_stop_words = {}\n",
    "print(specific_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = filter(lambda x: x not in stopwords, text.split())\n",
    "    tokens = map(stemmer.stem, tokens)\n",
    "    text = \" \".join(tokens)\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def get_invocations_cmds(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        contents = file.read()\n",
    "        invocations = re.findall(r\"[^.]- (.+):\\n\", contents)\n",
    "        cmds = re.findall(r\"[^.]`(.+)`\\n|\\0\", contents)\n",
    "    return invocations, cmds\n",
    "\n",
    "\n",
    "def get_invocations_cmds_from_dir(path):\n",
    "    paths = glob.glob(os.path.join(path, \"*.md\"))\n",
    "    all_invocations, all_cmds = [], []\n",
    "\n",
    "    for path in paths:\n",
    "        invocations, cmds = get_invocations_cmds(path)\n",
    "        all_invocations.extend(invocations)\n",
    "        all_cmds.extend(cmds)\n",
    "    \n",
    "    assert len(all_invocations) == len(all_cmds)\n",
    "    \n",
    "    return all_invocations, all_cmds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToBashDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, cmds, text_tokenizer, cmd_tokenizer,\n",
    "                 max_text_length=MAX_TEXT_LENGTH, max_code_length=MAX_CODE_LENGTH):\n",
    "        \n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.cmd_tokenizer = cmd_tokenizer\n",
    "        self.max_text_length = max_text_length\n",
    "        self.max_code_length = max_code_length\n",
    "        \n",
    "        self.items = []\n",
    "        \n",
    "        for text, cmd in zip(texts, cmds):\n",
    "            text_tokenized = text_tokenizer.tokenize(text, add_bos=True, add_eos=True)\n",
    "            cmd_tokenized = cmd_tokenizer.tokenize(cmd, add_bos=True, add_eos=True)\n",
    "            if len(text_tokenized) > max_text_length:\n",
    "                text_tokenized = text_tokenized[:max_text_length-1] + text_tokenized[-1:]\n",
    "            if len(cmd_tokenized) > max_code_length:\n",
    "                cmd_tokenized = cmd_tokenized[:max_code_length-1] + cmd_tokenized[-1:]\n",
    "            self.items.append((text_tokenized, cmd_tokenized))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    text_idxs = [torch.tensor(item[0]) for item in batch]\n",
    "    cmd_idxs = [torch.tensor(item[1]) for item in batch]\n",
    "    \n",
    "    text_idxs = pad_sequence(text_idxs, padding_value=PAD_ID, batch_first=True)\n",
    "    cmd_idxs = pad_sequence(cmd_idxs, padding_value=PAD_ID, batch_first=True)\n",
    "    \n",
    "    return text_idxs, cmd_idxs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(text_tokenized, model, max_len=20):\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(DEVICE)\n",
    "    text_tokenized = text_tokenized.unsqueeze(0).to(DEVICE)\n",
    "    cmd_prediction = torch.tensor([BOS_ID]).unsqueeze(0).to(DEVICE)\n",
    "    next_piece = BOS_ID\n",
    "    \n",
    "    while next_piece != EOS_ID and cmd_prediction.shape[-1] < max_len:\n",
    "        #print(cmd_prediction)\n",
    "        out = model(input_ids=text_tokenized, decoder_input_ids=cmd_prediction)\n",
    "        next_piece = torch.argmax(out.logits.squeeze(0)[-1]).item()\n",
    "        cmd_prediction = torch.cat((cmd_prediction, torch.tensor([[next_piece]]).to(DEVICE)), dim=1)\n",
    "        \n",
    "    return cmd_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_decode(i, data, ds, model, max_len=20):\n",
    "    \n",
    "    for k, v in dict(data.iloc[i]).items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    text_tokenized = torch.tensor(ds[i][0])\n",
    "    cmd_prediction = greedy_decode(text_tokenized, model, max_len)\n",
    "    cmd_prediction = [x.item() for x in cmd_prediction]\n",
    "    print(f\"cmd_predicted: {cmd_tokenizer.decode(cmd_prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_eval(data, ds, model, sample_size=5, max_len=20):\n",
    "    indexes = random.sample(range(len(ds)), sample_size)\n",
    "    for i in indexes:\n",
    "        eval_decode(i, data, ds, model, max_len)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.01):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.best_epoch = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss, step):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = step\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = step\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load from disk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = {\n",
    "    \"train\": pd.read_csv('data/train_data.csv'),\n",
    "    \"test\": pd.read_csv('data/test_data.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "invocations, cmds = get_invocations_cmds_from_dir(\"./tdlr_data/linux/\")\n",
    "invocations_common, cmds_common = get_invocations_cmds_from_dir(\"./tdlr_data/common/\")\n",
    "invocations.extend(invocations_common)\n",
    "cmds.extend(cmds_common)\n",
    "\n",
    "data[\"tldr\"] = pd.DataFrame.from_records({\"invocation\": invocations, \"cmd\": cmds})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload to wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run = wandb.init(project=\"text2bash\", job_type=\"upload\")\n",
    "\n",
    "for split, df in data.items():\n",
    "    \n",
    "    artifact = wandb.Artifact(split, type=\"raw_data\")\n",
    "    artifact.add(wandb.Table(dataframe=df), name=split)\n",
    "    run.log_artifact(artifact)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train / val split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "splits = [\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load raw from wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run = wandb.init(project=\"text2bash\", job_type=\"train_val_split\")\n",
    "\n",
    "data = {}\n",
    "for split in splits:\n",
    "    artifact = run.use_artifact(f\"{split}:latest\", type=\"raw_data\")\n",
    "    table = artifact.get(split)\n",
    "    data[split] = pd.DataFrame(table.data, columns=table.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data[\"train\"], data[\"val\"] = train_test_split(data[\"train\"], test_size=0.05, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload to wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for split, df in data.items():\n",
    "    artifact = wandb.Artifact(f\"{split}_split\", type=\"raw_data\")\n",
    "    artifact.add(wandb.Table(dataframe=df), name=f\"{split}_split\")\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train_split\", \"val_split\", \"test\", \"tldr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load raw from wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run = wandb.init(project=\"text2bash\", job_type=\"preprocess_invocation\")\n",
    "\n",
    "data = {}\n",
    "for split in splits:\n",
    "    artifact = run.use_artifact(f\"{split}:latest\", type=\"raw_data\")\n",
    "    table = artifact.get(split)\n",
    "    data[split] = pd.DataFrame(table.data, columns=table.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "for split, df in data.items():\n",
    "    data[split][\"invocation_preprocessed\"] = df[\"invocation\"].apply(clean_text)\n",
    "    data[split].drop(columns=[\"cmd\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload to wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for split, df in data.items():\n",
    "    item_name = f\"{split}_invocation_preprocessed\"\n",
    "    artifact = wandb.Artifact(item_name, type=\"preprocessed_data\")\n",
    "    artifact.add(wandb.Table(dataframe=df), name=item_name)\n",
    "    run.log_artifact(artifact, aliases=[f\"stemmer={type(stemmer).__name__.lower()}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load w/ preprocessed text from wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run = wandb.init(project=\"text2bash\", job_type=\"preprocess_cmd\")\n",
    "\n",
    "data = {}\n",
    "for split in splits:\n",
    "    artifact = run.use_artifact(f\"{split}:latest\", type=\"raw_data\")\n",
    "    table = artifact.get(split)\n",
    "    data[split] = pd.DataFrame(table.data, columns=table.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for split, df in data.items():\n",
    "    data[split][\"cmd_preprocessed\"] = df[\"cmd\"].apply(partial(cmd2template, loose_constraints=True))\n",
    "    data[split].drop(columns=[\"invocation\"], inplace=True)\n",
    "    #data[split] = data[split][data[split][\"cmd_cleaned\"].str.strip().astype(bool)] # removing empty rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload to wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for split, df in data.items():\n",
    "    item_name = f\"{split}_cmd_preprocessed\"\n",
    "    artifact = wandb.Artifact(item_name, type=\"preprocessed_data\")\n",
    "    artifact.add(wandb.Table(dataframe=df), name=item_name)\n",
    "    run.log_artifact(artifact, aliases=[\"method=bashlint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "splits = [\"train_split_invocation_preprocessed\"]\n",
    "stemmer = \"porterstemmer\"\n",
    "vocab_size = 3800\n",
    "\n",
    "model_prefix = './tokenizers/sp_invocation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load preprocessed from wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run = wandb.init(project=\"text2bash\", job_type=\"create_invocation_tokenizer\")\n",
    "\n",
    "data = {}\n",
    "for split in splits:\n",
    "    artifact = run.use_artifact(f\"{split}:stemmer={stemmer}\", type=\"preprocessed_data\")\n",
    "    table = artifact.get(split)\n",
    "    data[split] = pd.DataFrame(table.data, columns=table.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare tokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! mkdir -p ./tokenizers\n",
    "\n",
    "data[\"merged\"] = pd.concat(data.values())\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(data[\"merged\"][\"invocation_preprocessed\"]),\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload to wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "artifact = wandb.Artifact(\"invocation_tokenizer\", type=\"tokenizers\")\n",
    "artifact.add_file(model_prefix + \".model\")\n",
    "run.log_artifact(artifact, aliases=[f\"splits={str(splits)}|vocab_size={vocab_size}|stemmer={stemmer}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cmd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "splits = [\"train_split_cmd_preprocessed\"]\n",
    "method = \"bashlint\"\n",
    "vocab_size = 500\n",
    "\n",
    "model_prefix = './tokenizers/sp_cmd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load preprocessed from wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run = wandb.init(project=\"text2bash\", job_type=\"create_cmd_tokenizer\")\n",
    "\n",
    "data = {}\n",
    "for split in splits:\n",
    "    artifact = run.use_artifact(f\"{split}:method={method}\", type=\"preprocessed_data\")\n",
    "    table = artifact.get(split)\n",
    "    data[split] = pd.DataFrame(table.data, columns=table.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare tokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! mkdir -p ./tokenizers\n",
    "\n",
    "data[\"merged\"] = pd.concat(data.values())\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(data[\"merged\"][\"cmd_preprocessed\"]),\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload to wandb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "artifact = wandb.Artifact(\"cmd_tokenizer\", type=\"tokenizers\")\n",
    "artifact.add_file(model_prefix + \".model\")\n",
    "run.log_artifact(artifact, aliases=[f\"splits={str(splits)}|vocab_size={vocab_size}|method={method}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data/invocation/train/splits\": [\"train_split_invocation_preprocessed\"],\n",
    "    \"data/invocation/val/splits\": [\"val_split_invocation_preprocessed\"],\n",
    "    \"data/invocation/test/splits\": [\"test_invocation_preprocessed\"],\n",
    "    \"data/invocation/stemmer\": \"porterstemmer\",\n",
    "    \n",
    "    \"data/cmd/train/splits\": [\"train_split_cmd_preprocessed\"],\n",
    "    \"data/cmd/val/splits\": [\"val_split_cmd_preprocessed\"],\n",
    "    \"data/cmd/test/splits\": [\"test_cmd_preprocessed\"],\n",
    "    \"data/cmd/method\": \"bashlint\",\n",
    "    \n",
    "    \"tokenizer/cmd/vocab_size\": 500,\n",
    "    \"tokenizer/invocation/vocab_size\": 3800,\n",
    "    \n",
    "    \"model/hidden_size\": 256,\n",
    "    \"model/num_hidden_layers\": 2,\n",
    "    \"model/num_attention_heads\": 8,\n",
    "    \"model/intermediate_size\": 256 * 4,\n",
    "    \"model/hidden_dropout_prob\": 0.1,\n",
    "    \n",
    "    \"training/batch_size\": 64,\n",
    "    \"training/lr\": 1e-3,\n",
    "    \"training/stopper/patience\": 5,\n",
    "    \"training/stopper/min_delta\": 0.01,\n",
    "    \"training/max_grad_norm\": -1,\n",
    "    \"training/n_epochs\": 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfuriousteabag\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/furiousteabag/text2bash/runs/3qnhk1tt\" target=\"_blank\">eager-serenity-76</a></strong> to <a href=\"https://wandb.ai/furiousteabag/text2bash\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"text2bash\", job_type=\"train_model\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {\"train\": {}, \"val\": {}}\n",
    "y = {\"train\": {}, \"val\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in X.keys():\n",
    "    for split in config[f\"data/invocation/{key}/splits\"]:\n",
    "        artifact = run.use_artifact(f\"{split}:stemmer={config['data/invocation/stemmer']}\", type=\"preprocessed_data\")\n",
    "        table = artifact.get(split)\n",
    "        X[key][split] = pd.DataFrame(table.data, columns=table.columns)\n",
    "    \n",
    "    X[key][\"merged\"] = pd.concat(X[key].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in y.keys():\n",
    "    for split in config[f\"data/cmd/{key}/splits\"]:\n",
    "        artifact = run.use_artifact(f\"{split}:method={config['data/cmd/method']}\", type=\"preprocessed_data\")\n",
    "        table = artifact.get(split)\n",
    "        y[key][split] = pd.DataFrame(table.data, columns=table.columns)\n",
    "    \n",
    "    y[key][\"merged\"] = pd.concat(y[key].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"invocation_tokenizer\"\n",
    "artifact = run.use_artifact(f\"{tokenizer_name}:splits={str(config['data/invocation/train/splits'])}|vocab_size={config['tokenizer/invocation/vocab_size']}|stemmer={config['data/invocation/stemmer']}\",\n",
    "                            type=\"tokenizers\")\n",
    "tokenizer_path = artifact.download()\n",
    "invocation_tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path + \"/sp_invocation.model\")\n",
    "assert invocation_tokenizer.vocab_size() == config[\"tokenizer/invocation/vocab_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"cmd_tokenizer\"\n",
    "artifact = run.use_artifact(f\"{tokenizer_name}:splits={str(config['data/cmd/train/splits'])}|vocab_size={config['tokenizer/cmd/vocab_size']}|method={config['data/cmd/method']}\",\n",
    "                            type=\"tokenizers\")\n",
    "tokenizer_path = artifact.download()\n",
    "cmd_tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path + \"/sp_cmd.model\")\n",
    "assert cmd_tokenizer.vocab_size() == config[\"tokenizer/cmd/vocab_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextToBashDataset(\n",
    "    texts=X[\"train\"][\"merged\"][\"invocation_preprocessed\"],\n",
    "    cmds=y[\"train\"][\"merged\"][\"cmd_preprocessed\"],\n",
    "    text_tokenizer=invocation_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)\n",
    "\n",
    "valid_ds = TextToBashDataset(\n",
    "    texts=X[\"val\"][\"merged\"][\"invocation_preprocessed\"],\n",
    "    cmds=y[\"val\"][\"merged\"][\"cmd_preprocessed\"],\n",
    "    text_tokenizer=invocation_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train': DataLoader(train_ds, batch_size=config[\"training/batch_size\"], shuffle=True, collate_fn=collate_fn),\n",
    "    'val': DataLoader(valid_ds, batch_size=config[\"training/batch_size\"], collate_fn=collate_fn),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delet file current directori\n",
      "find Path -mindepth Quantity -delete\n"
     ]
    }
   ],
   "source": [
    "invocations, cmds = next(iter(loaders[\"train\"]))\n",
    "\n",
    "i = 15\n",
    "print(train_ds.text_tokenizer.decode(list(map(int, invocations[i]))))\n",
    "print(train_ds.cmd_tokenizer.decode(list(map(int, cmds[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display long list file current folder access today start day\n",
      "find Path -daystart -atime Timespan -ls\n"
     ]
    }
   ],
   "source": [
    "invocations, cmds = next(iter(loaders[\"val\"]))\n",
    "\n",
    "i = 14\n",
    "print(train_ds.text_tokenizer.decode(list(map(int, invocations[i]))))\n",
    "print(train_ds.cmd_tokenizer.decode(list(map(int, cmds[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "text_model_config = BertConfig(\n",
    "    vocab_size = invocation_tokenizer.vocab_size(),\n",
    "    hidden_size = config[\"model/hidden_size\"],\n",
    "    num_hidden_layers = config[\"model/num_hidden_layers\"],\n",
    "    num_attention_heads = config[\"model/num_attention_heads\"],\n",
    "    intermediate_size = config[\"model/intermediate_size\"],\n",
    "    hidden_dropout_prob = config[\"model/hidden_dropout_prob\"],\n",
    "    pad_token_id = PAD_ID,\n",
    ")\n",
    "\n",
    "cmd_model_config = BertConfig(\n",
    "    vocab_size = cmd_tokenizer.vocab_size(),\n",
    "    hidden_size = config[\"model/hidden_size\"],\n",
    "    num_hidden_layers = config[\"model/num_hidden_layers\"],\n",
    "    num_attention_heads = config[\"model/num_attention_heads\"],\n",
    "    intermediate_size = config[\"model/intermediate_size\"],\n",
    "    hidden_dropout_prob = config[\"model/hidden_dropout_prob\"],\n",
    "    pad_token_id = PAD_ID,\n",
    "    is_decoder = True,\n",
    "    add_cross_attention = True\n",
    ")\n",
    "\n",
    "print(cmd_model_config.is_decoder)\n",
    "print(cmd_model_config.add_cross_attention)\n",
    "\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(text_model_config, cmd_model_config)\n",
    "model = EncoderDecoderModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 70])\n",
      "torch.Size([64, 29])\n",
      "torch.Size([64, 29])\n"
     ]
    }
   ],
   "source": [
    "text_idxs, cmd_idxs = next(iter(loaders[\"val\"]))\n",
    "decoder_input = cmd_idxs[..., :-1]\n",
    "target = cmd_idxs[..., 1:]\n",
    "\n",
    "print(text_idxs.shape)\n",
    "print(decoder_input.shape)\n",
    "print(target.shape)\n",
    "\n",
    "text_idxs_mask = torch.where(text_idxs != PAD_ID, 1, 0)\n",
    "decoder_input_mask = torch.where(decoder_input != PAD_ID, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  6,  8,  ...,  0,  0,  0],\n",
       "        [ 1,  6,  8,  ...,  0,  0,  0],\n",
       "        [ 1,  6,  8,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 1, 48, 89,  ...,  0,  0,  0],\n",
       "        [ 1, 23, 16,  ...,  0,  0,  0],\n",
       "        [ 1,  6,  8,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_idxs[..., :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 29, 500])\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "    attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "\n",
    "logits = out.logits\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            criterion,\n",
    "            optimizer, \n",
    "            pad_token_id,\n",
    "            device,\n",
    "            stopper,\n",
    "            run=None,\n",
    "            max_grad_norm=-1\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        self._criterion = criterion\n",
    "        self._optimizer = optimizer\n",
    "        self._pad_token_id = pad_token_id\n",
    "        self._device = device\n",
    "        self._run = run\n",
    "        self._stopper = stopper\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self._model = model.to(self._device)\n",
    "\n",
    "        self._n_epoch = 0\n",
    "        self._n_iter = 0\n",
    "\n",
    "    def train(self, dataloaders, n_epochs):\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            train_loss = self._train_step(dataloaders[\"train\"])\n",
    "            val_loss = self._val_step(dataloaders[\"val\"])\n",
    "            self._n_epoch += 1\n",
    "            tqdm.write(f\"Epoch: {self._n_epoch} | train_loss: {train_loss:.3f} | val_loss: {val_loss:.3f}\")\n",
    "            \n",
    "            if self._run is not None:\n",
    "                self._run.log(data={\"loss/train\": train_loss, \"loss/val\": val_loss}, step=self._n_epoch)\n",
    "                \n",
    "            self._stopper(val_loss, self._n_epoch)\n",
    "            torch.save(self._model.state_dict(), f\"./checkpoints/{self._n_epoch}_{val_loss}.pt\")\n",
    "            \n",
    "            if self._stopper.early_stop:\n",
    "                break\n",
    "        \n",
    "        if self._run is not None:\n",
    "            # they are not the best but the first one\n",
    "            # to achieve loss score with given stopper\n",
    "            # threshold\n",
    "            self._run.summary[\"best_epoch\"] = self._stopper.best_epoch\n",
    "            self._run.summary[\"best_val_loss\"] = self._stopper.best_loss\n",
    "            \n",
    "            # LOG MODEL CHECKPOINT\n",
    "            \n",
    "            \n",
    "        self._model.load_state_dict(torch.load(f\"./checkpoints/{self._stopper.best_epoch}_{self._stopper.best_loss}.pt\"))\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        self._model.train()\n",
    "        epoch_loss = 0\n",
    "        for text_idxs, cmd_idxs in dataloader:\n",
    "            \n",
    "            text_idxs = text_idxs.to(self._device)\n",
    "            decoder_input = cmd_idxs[..., :-1].to(self._device)\n",
    "            target = cmd_idxs[..., 1:].to(self._device)\n",
    "            \n",
    "            text_idxs_mask = torch.where(text_idxs != self._pad_token_id, 1, 0).to(self._device)\n",
    "            decoder_input_mask = torch.where(decoder_input != self._pad_token_id, 1, 0).to(self._device)\n",
    "            \n",
    "            self._optimizer.zero_grad()        \n",
    "            \n",
    "            out = self._model(\n",
    "                input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "                attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "\n",
    "            logits = out.logits\n",
    "\n",
    "            loss = self._criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if self._max_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n",
    "                \n",
    "            self._optimizer.step()\n",
    "        return epoch_loss / len(dataloader)\n",
    "    \n",
    "    def _val_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        self._model.eval()\n",
    "        epoch_loss = 0\n",
    "        for text_idxs, cmd_idxs in dataloader:\n",
    "            \n",
    "            text_idxs = text_idxs.to(self._device)\n",
    "            decoder_input = cmd_idxs[..., :-1].to(self._device)\n",
    "            target = cmd_idxs[..., 1:].to(self._device)\n",
    "            \n",
    "            text_idxs_mask = torch.where(text_idxs != self._pad_token_id, 1, 0).to(self._device)\n",
    "            decoder_input_mask = torch.where(decoder_input != self._pad_token_id, 1, 0).to(self._device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = self._model(\n",
    "                    input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "                    attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "                logits = out.logits\n",
    "                loss = self._criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(logits.shape)\n",
    "print(target.shape)\n",
    "criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"training/lr\"])\n",
    "stopper = EarlyStopping(patience=config[\"training/stopper/patience\"], min_delta=config[\"training/stopper/min_delta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    pad_token_id=PAD_ID,\n",
    "    device=DEVICE,\n",
    "    run=run,\n",
    "    stopper=stopper,\n",
    "    max_grad_norm=config[\"training/max_grad_norm\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ./checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:18<08:55, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.116 | val_loss: 0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:36<08:32, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.916 | val_loss: 0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:55<08:16, 18.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.814 | val_loss: 0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:13<07:59, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.733 | val_loss: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [01:33<07:50, 18.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.679 | val_loss: 0.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [01:52<07:35, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 0.622 | val_loss: 0.753\n",
      "INFO: Early stopping counter 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [02:11<07:15, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss: 0.580 | val_loss: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [02:29<06:54, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | train_loss: 0.543 | val_loss: 0.738\n",
      "INFO: Early stopping counter 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [02:48<06:33, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | train_loss: 0.508 | val_loss: 0.746\n",
      "INFO: Early stopping counter 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [03:07<06:18, 18.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | train_loss: 0.474 | val_loss: 0.727\n",
      "INFO: Early stopping counter 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [03:26<05:59, 18.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | train_loss: 0.448 | val_loss: 0.732\n",
      "INFO: Early stopping counter 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [03:45<06:29, 20.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | train_loss: 0.419 | val_loss: 0.731\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train(loaders, config[\"training/n_epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переучивать не будем:\n",
    "\n",
    "![](./images/training.png)\n",
    "\n",
    "Видим, что лучший лосс был на 8-й эпохе.\n",
    "Подгрузим эти веса (при переобучении модель автоматически в конце подгружает лучшие веса):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.93\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./checkpoints/0.9302761256694794.pt\"))\n",
    "trainer._model = model\n",
    "print(f\"val_loss: {round(trainer._val_step(loaders['valid']), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация команд (2 балла)\n",
    "\n",
    "**Задание**. Реализуйте алгоритм beam-search в классе BeamSearchGenerator ниже. Ваша реализация должна поддерживать задание температуры софтмакса. Выходы модели, полученные на предыдущих итерациях, необходимо кэшировать для повышения скорости алгоритма. Вместо подсчёта произведения любых вероятностей необходимо считать сумму их логарифмов.\n",
    "\n",
    "Алгоритм должен возвращать список пар из получившихся выходных последовательностей и логарифмов их вероятностей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchGenerator:\n",
    "    def __init__(\n",
    "            self, pad_id, eos_id, bos_id,\n",
    "            max_length=20, beam_width=5, temperature=1.5,\n",
    "            device=torch.device('cuda'),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        pad_id : int\n",
    "        eos_id : int\n",
    "        bos_id : int\n",
    "        max_length : int\n",
    "            Maximum length of output sequence\n",
    "        beam_width : int\n",
    "            Width of the beam\n",
    "        temperature : float\n",
    "            Softmax temperature\n",
    "        device : torch.device\n",
    "            Your model device\n",
    "        \"\"\"\n",
    "        self.pad_id = pad_id\n",
    "        self.eos_id = eos_id\n",
    "        self.bos_id = bos_id\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.beam_width = beam_width\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def get_result(self, model, input_text_tokens):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : TextToBashModel\n",
    "        input_text_tokens : torch.tensor\n",
    "            One object input tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        chains = torch.full([self.beam_width, 1], self.bos_id).to(self.device)\n",
    "        chain_probabilities = torch.zeros(self.beam_width).to(self.device)\n",
    "        if_chain_ready = torch.full([self.beam_width], False).to(self.device)\n",
    "        \n",
    "        # saving encoder outputs\n",
    "        input_text_tokens = input_text_tokens.repeat([self.beam_width, 1]).to(device)\n",
    "        encoder_outputs = model._prepare_encoder_decoder_kwargs_for_generation(\n",
    "            input_ids=input_text_tokens,\n",
    "            model_kwargs={})[\"encoder_outputs\"]\n",
    "        \n",
    "        idx = 0\n",
    "        while idx <= self.max_length and not all(if_chain_ready):\n",
    "            \n",
    "            logits = model(encoder_outputs=encoder_outputs, decoder_input_ids=chains).logits[:, -1, :]\n",
    "            \n",
    "            probs = torch.log_softmax(logits / self.temperature, dim=-1)\n",
    "            probs, tokens = torch.topk(probs, k=self.beam_width)\n",
    "            \n",
    "            if idx == 0:\n",
    "                # pick all top tokens\n",
    "                # 0 because current chains are same\n",
    "                beam_tokens = tokens[0].view(self.beam_width, 1)\n",
    "                chains = torch.cat([chains, beam_tokens], dim=-1)\n",
    "                chain_probabilities += probs[0]\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            chain_probabilities_beam = chain_probabilities.reshape(-1, 1).repeat([1, self.beam_width])\n",
    "            ready_chains_idxs = if_chain_ready.nonzero()\n",
    "            \n",
    "            # if chain is already ready,\n",
    "            # keep one instance of it and add\n",
    "            # padding\n",
    "            probs[ready_chains_idxs, :] = -float(\"inf\")\n",
    "            probs[ready_chains_idxs, 0] = 0\n",
    "            tokens[ready_chains_idxs, :] = self.pad_id\n",
    "\n",
    "            chain_probabilities_beam = chain_probabilities_beam + probs\n",
    "            \n",
    "            # choosing best across all options\n",
    "            best_sequences = torch.argsort(chain_probabilities_beam.flatten())[-self.beam_width:]\n",
    "\n",
    "            chains = torch.cat([chains[best_sequences // self.beam_width, :], tokens.flatten()[best_sequences].view(-1, 1)], dim=-1)\n",
    "            chain_probabilities = chain_probabilities_beam.flatten()[best_sequences]\n",
    "\n",
    "            if_chain_ready = ((chains[:, -1] == self.eos_id) | (chains[:, -1] == self.pad_id))\n",
    "                \n",
    "            idx += 1\n",
    "        \n",
    "        return list(zip(chains, chain_probabilities))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте на нескольких примерах работу вашего алгоритма. Если всё реализовано правильно, то как минимум на трёх примерах из 5 всё должно работать правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_engine = BeamSearchGenerator(\n",
    "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "    max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "    temperature=1, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text: searches through the root filesystem (\"/\") for the file named chapter1, and prints the location\n",
      "text cleaned: search root filesystem  file name chapter print locat\n",
      "true: find / -name Chapter1 -type f -print\n",
      "true cleaned: find Path -name Regex -type f -print\n",
      "find Path -name Regex -type f -printf \"%f\\n\" tensor(-4.4631, device='cuda:0')\n",
      "find Path -name Regex -or -name Regex tensor(-3.2608, device='cuda:0')\n",
      "find Path -name Regex -type f -print tensor(-2.9428, device='cuda:0')\n",
      "find Path -name Regex -type f tensor(-1.5682, device='cuda:0')\n",
      "find Path -name Regex tensor(-0.7397, device='cuda:0')\n",
      "1.0\n",
      "\n",
      "text: searches through the root filesystem (\"/\") for the file named chapter1.\n",
      "text cleaned: search root filesystem  file name chapter\n",
      "true: find / -name Chapter1 -type f\n",
      "true cleaned: find Path -name Regex -type f\n",
      "find Path -name Regex -or -name Regex -type f tensor(-4.5106, device='cuda:0')\n",
      "find Path -name Regex -or -name Regex tensor(-2.8590, device='cuda:0')\n",
      "find Path -name Regex -type f -print tensor(-2.0952, device='cuda:0')\n",
      "find Path -name Regex -type f tensor(-1.5648, device='cuda:0')\n",
      "find Path -name Regex tensor(-0.8799, device='cuda:0')\n",
      "1.0\n",
      "\n",
      "text: searches through the root filesystem (\"/\") for the file named chapter1.\n",
      "text cleaned: search root filesystem  file name chapter\n",
      "true: find / -name Chapter1 -type f -print\n",
      "true cleaned: find Path -name Regex -type f -print\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/furiousteabag/.local/share/virtualenvs/base/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find Path -name Regex -or -name Regex -type f tensor(-4.5106, device='cuda:0')\n",
      "find Path -name Regex -or -name Regex tensor(-2.8590, device='cuda:0')\n",
      "find Path -name Regex -type f -print tensor(-2.0952, device='cuda:0')\n",
      "find Path -name Regex -type f tensor(-1.5648, device='cuda:0')\n",
      "find Path -name Regex tensor(-0.8799, device='cuda:0')\n",
      "1.0\n",
      "\n",
      "text: searching for all files with the extension mp3\n",
      "text cleaned: search file extens mp\n",
      "true: find / -name *.mp3\n",
      "true cleaned: find Path -name Regex\n",
      "find Path -type f -iname Regex -print tensor(-4.9602, device='cuda:0')\n",
      "find Path -type f -iname Regex tensor(-2.0453, device='cuda:0')\n",
      "find Path -iname Regex tensor(-1.9404, device='cuda:0')\n",
      "find Path -name Regex tensor(-1.7023, device='cuda:0')\n",
      "find Path -type f -name Regex tensor(-1.6675, device='cuda:0')\n",
      "1.0\n",
      "\n",
      "text: set myvariable to the value of variable_name\n",
      "text cleaned: set myvari valu variablenam\n",
      "true: myVariable=$(env  | grep VARIABLE_NAME | grep -oe '[^=]*$');\n",
      "true cleaned: env | grep Regex | grep -o -e Regex\n",
      "echo Regex | cut -f Number -d Regex | rev tensor(-5.7752, device='cuda:0')\n",
      "echo Regex | cut -f Number tensor(-4.5846, device='cuda:0')\n",
      "echo Regex | cut -d Regex -f Number tensor(-4.2235, device='cuda:0')\n",
      "echo Regex | rename Regex tensor(-4.0193, device='cuda:0')\n",
      "echo Regex | tee File tensor(-2.1091, device='cuda:0')\n",
      "-1.0\n",
      "average score: 0.600\n"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print()\n",
    "        print('text:', valid_data.invocation.iloc[i])\n",
    "        print('text cleaned:', valid_data.text_cleaned.iloc[i])\n",
    "        print('true:', valid_data.cmd.iloc[i])\n",
    "        print('true cleaned:', valid_data.cmd_cleaned.iloc[i])\n",
    "\n",
    "        src = torch.tensor(valid_ds[i][0])\n",
    "        pred = beam_search_engine.get_result(model, src)\n",
    "        \n",
    "        #print('greedy decode:', cmd_tokenizer.decode(list(map(int, greedy_decode(src, model)))))\n",
    "        scores = []\n",
    "        for x, proba in pred[:5]:\n",
    "            pred_cmd = cmd_tokenizer.decode(list(map(int, x)))\n",
    "            score = compute_metric(pred_cmd, 1, valid_data.cmd.iloc[i])\n",
    "            scores.append(score)\n",
    "            print(pred_cmd, proba)\n",
    "        print(max(scores))\n",
    "        all_scores.append(max(scores))\n",
    "        \n",
    "print(f\"average score: {np.mean(all_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i = 1\n",
    "print(valid_data.iloc[i])\n",
    "input_text_tokens = torch.tensor(valid_ds[i][0])\n",
    "out = beam_search_engine.get_result(model, input_text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Дополните функцию для подсчёта качества. Посчитайте качество вашей модели на валидационном и тестовых датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_scores(model, df, beam_engine):\n",
    "    all_scores = []\n",
    "\n",
    "    for i, (text, target_cmd) in tqdm(enumerate(zip(df.text_cleaned.values, df.cmd.values)), total=len(df)):\n",
    "        \n",
    "        input_tokens = text_tokenizer.tokenize(text, add_bos=True, add_eos=True)\n",
    "        if len(input_tokens) > MAX_TEXT_LENGTH:\n",
    "            input_tokens = input_tokens[:MAX_TEXT_LENGTH-1] + input_tokens[-1:]\n",
    "        input_tokens = torch.tensor(input_tokens)\n",
    "        \n",
    "        predictions = beam_engine.get_result(model, input_tokens)\n",
    "        \n",
    "        # get only 5 top results\n",
    "        predictions = predictions[:5]\n",
    "        object_scores = []\n",
    "        for output_tokens, proba in predictions:\n",
    "            output_cmd = cmd_tokenizer.decode(list(map(int, output_tokens)))\n",
    "            score = compute_metric(output_cmd, 1, target_cmd)\n",
    "            object_scores.append(score)\n",
    "        \n",
    "        all_scores.append(max(object_scores))\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы всё реализовали правильно, подобрали параметры BeamSearch то ваш средний скор на валидации должен быть >= 0.25, а скор на `handcrafted` части теста >= 0.13. На `mined` части датасета скор может быть низкий, т.к. некоторых команд из датасета нет в обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_temperature(model, valid_data):\n",
    "    scores = []\n",
    "    temperatures = np.arange(0.5, 2.1, 0.1)\n",
    "    for temperature in temperatures:\n",
    "        beam_search_engine = BeamSearchGenerator(\n",
    "            pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "            max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "            temperature=temperature, device=DEVICE\n",
    "        )\n",
    "        scores.append(np.mean(compute_all_scores(model, valid_data, beam_search_engine)))\n",
    "    return temperatures[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temperature = find_best_temperature(model, valid_data)\n",
    "print(temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/searching_temperature.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_engine = BeamSearchGenerator(\n",
    "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "    max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "    temperature=1.5, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score on validation: 0.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_scores = compute_all_scores(model, valid_data, beam_search_engine)\n",
    "print(f\"average score on validation: {np.mean(val_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [00:11<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score on handcrafted: 0.149\n"
     ]
    }
   ],
   "source": [
    "test_scores = compute_all_scores(model, test_data[test_data[\"origin\"] == \"handcrafted\"], beam_search_engine)\n",
    "print(f\"average score on handcrafted: {np.mean(test_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 592/592 [00:48<00:00, 12.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score on mined part: -0.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mined_scores = compute_all_scores(model, test_data[test_data[\"origin\"] == \"mined\"], beam_search_engine)\n",
    "print(f\"average score on mined part: {np.mean(mined_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От вас ожидается скор на `mined` >= 0 при скоре на `handrafted` >= 0.16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим данных\n",
    "\n",
    "Долго пользуюсь утилиткой [tldr](https://github.com/tldr-pages/tldr), вывод выглядит примерно так:\n",
    "![](./images/tldr.png)\n",
    "\n",
    "Скачаем и распарсим из [репозитория](https://github.com/tldr-pages/tldr/tree/main/pages) доки к `common` и `linux`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_train_data['cmd_cleaned'] = additional_train_data['cmd'].apply(partial(cmd2template, loose_constraints=True))\n",
    "\n",
    "print(f\"number of empty CMDs: {len(additional_train_data[additional_train_data['cmd_cleaned'] == ''])} / {len(additional_train_data)}\")\n",
    "display(additional_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему-то после очистки `cmd` у нас почти все строчки пустые:\n",
    "![](./images/empty_cmds.png)\n",
    "\n",
    "Около 2-х часов пытался менять захардкоженные команды в исходнике функции очищения, но ничего не вышло.\n",
    "Попробуем вообще не чистить `cmd`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "additional_train_data = pd.DataFrame.from_records({\"invocation\": invocations, \"cmd\": cmds, \"cmd_cleaned\": cmds})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data['cmd_cleaned'] = train_data['cmd'].apply(partial(cmd2template, loose_constraints=True))\n",
    "#train_data['cmd_cleaned'] = train_data['cmd']\n",
    "\n",
    "train_data = train_data.append(additional_train_data)\n",
    "train_data['text_cleaned'] = train_data['invocation'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = train_data.sample(frac=1, random_state=322).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text_tokenizer = io.BytesIO()\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(train_data[\"text_cleaned\"]),\n",
    "    #sentence_iterator=iter(train_data[\"invocation\"]),\n",
    "    model_writer=text_tokenizer,\n",
    "    vocab_size=6000,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")\n",
    "text_tokenizer = spm.SentencePieceProcessor(model_proto=text_tokenizer.getvalue())\n",
    "\n",
    "cmd_tokenizer = io.BytesIO()\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(train_data[\"cmd_cleaned\"]),\n",
    "    model_writer=cmd_tokenizer,\n",
    "    #model_prefix='./tokenizers/sp_cmd',\n",
    "    vocab_size=6000,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")\n",
    "cmd_tokenizer = spm.SentencePieceProcessor(model_proto=cmd_tokenizer.getvalue())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = train_data.iloc[:-500]\n",
    "valid_data = train_data.iloc[-500:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_ds = TextToBashDataset(\n",
    "    texts=train_data[\"text_cleaned\"],\n",
    "    cmds=train_data[\"cmd_cleaned\"],\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)\n",
    "\n",
    "valid_ds = TextToBashDataset(\n",
    "    texts=valid_data[\"text_cleaned\"],\n",
    "    cmds=valid_data[\"cmd_cleaned\"],\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loaders = {\n",
    "    'train': DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn),\n",
    "    'valid': DataLoader(valid_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn),\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text_model_config = BertConfig(\n",
    "    vocab_size = text_tokenizer.vocab_size(),\n",
    "    hidden_size = 256,\n",
    "    num_hidden_layers = 4,\n",
    "    num_attention_heads = 8,\n",
    "    intermediate_size = 256 * 4,\n",
    "    hidden_dropout_prob = 0.1,\n",
    "    pad_token_id = PAD_ID,\n",
    ")\n",
    "\n",
    "cmd_model_config = BertConfig(\n",
    "    vocab_size = cmd_tokenizer.vocab_size(),\n",
    "    hidden_size = 256,\n",
    "    num_hidden_layers = 4,\n",
    "    num_attention_heads = 8,\n",
    "    intermediate_size = 256 * 4,\n",
    "    hidden_dropout_prob = 0.1,\n",
    "    pad_token_id = PAD_ID,\n",
    "    is_decoder = True,\n",
    "    add_cross_attention = True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(text_model_config, cmd_model_config)\n",
    "model = EncoderDecoderModel(config=config)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "stopper = EarlyStopping(patience=5, min_delta=0.01)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    pad_token_id=PAD_ID,\n",
    "    device=device,\n",
    "    logdir=\"./runs\",\n",
    "    stopper=stopper,\n",
    "    max_grad_norm=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! mkdir -p ./checkpoints"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.train(loaders, 50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "test_scores = compute_all_scores(model, test_data[test_data[\"origin\"] == \"handcrafted\"], beam_search_engine)\n",
    "print(f\"average score on handcrafted: {np.mean(test_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mined_scores = compute_all_scores(model, test_data[test_data[\"origin\"] == \"mined\"], beam_search_engine)\n",
    "print(f\"average score on mined part: {np.mean(mined_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_scores = compute_all_scores(model, valid_data, beam_search_engine)\n",
    "print(f\"average score on validation: {np.mean(val_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_scores = []\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print()\n",
    "        print('text:', valid_data.invocation.iloc[i])\n",
    "        print('text cleaned:', valid_data.text_cleaned.iloc[i])\n",
    "        print('true:', valid_data.cmd.iloc[i])\n",
    "        print('true cleaned:', valid_data.cmd_cleaned.iloc[i])\n",
    "\n",
    "        src = torch.tensor(valid_ds[i][0])\n",
    "        pred = beam_search_engine.get_result(model, src)\n",
    "        \n",
    "        #print('greedy decode:', cmd_tokenizer.decode(list(map(int, greedy_decode(src, model)))))\n",
    "        scores = []\n",
    "        for x, proba in pred[:5]:\n",
    "            pred_cmd = cmd_tokenizer.decode(list(map(int, x)))\n",
    "            score = compute_metric(pred_cmd, 1, valid_data.cmd.iloc[i])\n",
    "            scores.append(score)\n",
    "            print(pred_cmd, proba)\n",
    "        print(max(scores))\n",
    "        all_scores.append(max(scores))\n",
    "        \n",
    "print(f\"average score: {np.mean(all_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перебирал разные параметры, было на порядок хуже, чем раньше.\n",
    "\n",
    "Нужен рефактор препроцессинга, чтобы он не выдавал пустые строки на незнакомые команды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "lines = []\n",
    "with open(\"./data/manpage-data.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        lines.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36669"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': {'$oid': '5ef4f7386e9e960500981ce6'},\n",
       " 'source': 'zzxordir.1.gz',\n",
       " 'name': 'zzxordir',\n",
       " 'synopsis': 'small tools using zziplib',\n",
       " 'paragraphs': [{'idx': 0,\n",
       "   'text': '       zzcat, zzdir, zzxorcat, zzxordir zzxorcopy - small tools using zziplib',\n",
       "   'section': 'NAME',\n",
       "   'is_option': False},\n",
       "  {'idx': 1,\n",
       "   'text': '       <b>zzcat</b>  <b>FILE</b>  <b>[...]</b>   <b>zzdir</b>  <b>DIR</b>  <b>[...]</b>   <b>zzxorcat</b> <b>[-HEX]</b> <b>FILE</b> <b>[...]</b>  <b>zzxordir</b> <b>[-HEX]</b> <b>DIR</b> <b>[...]</b>  <b>zzxorcopy</b>\\n       <b>[-HEX]</b> <b>FILE</b> <b>OUTFILE</b>',\n",
       "   'section': 'SYNOPSIS',\n",
       "   'is_option': False},\n",
       "  {'idx': 2,\n",
       "   'text': '       <u>zzcat</u> prints the given files to stdout, so you may want to redirect the output. The FILE can be a  normal\\n       file or an inflated part of a zip archive (see OPTIONS).',\n",
       "   'section': 'DESCRIPTION',\n",
       "   'is_option': False},\n",
       "  {'idx': 3,\n",
       "   'text': '       <u>zzdir</u> prints the content table to stdout of the given DIR or zipfile.',\n",
       "   'section': 'DESCRIPTION',\n",
       "   'is_option': False},\n",
       "  {'idx': 4,\n",
       "   'text': '       <u>zzxorcat</u>  prints  the  given  files  to  stdout,  so  you may want to redirect the output. The FILE is an\\n       inflated part of a zip archive obfusctated with a xor value optionally given by the HEX option.',\n",
       "   'section': 'DESCRIPTION',\n",
       "   'is_option': False},\n",
       "  {'idx': 5,\n",
       "   'text': '       <u>zzxordir</u> prints the content table to stdout of the given DIR or zipfile. The DIR can br an inflated  part\\n       of a zip archive obfusctated with a xor value optionally given by the HEX option.',\n",
       "   'section': 'DESCRIPTION',\n",
       "   'is_option': False},\n",
       "  {'idx': 6,\n",
       "   'text': '       <u>zzxorcopy</u>  copies  data  from FILE to OUTFILE adding simple obfuscation by xor-ing each byte with the HEX\\n       value given.  Remember that copying data twice with the same xor-value will result in the  original  file\\n       data.',\n",
       "   'section': 'DESCRIPTION',\n",
       "   'is_option': False},\n",
       "  {'idx': 7,\n",
       "   'text': '       <u>zziplib-bin</u> tools accept the following options:',\n",
       "   'section': 'OPTIONS',\n",
       "   'is_option': False},\n",
       "  {'idx': 8,\n",
       "   'text': \"       <b>FILE</b>    Input  filename  for reading or part (if supported by tool). Part is written as zipname/filename,\\n               e.g.: for accessing README inside of data.zip you'll specify data/README.\",\n",
       "   'section': 'OPTIONS',\n",
       "   'is_option': False},\n",
       "  {'idx': 9,\n",
       "   'text': '       <b>DIR</b>     Disk directory, zipfile or inflated part of a zipfile.',\n",
       "   'section': 'OPTIONS',\n",
       "   'is_option': False},\n",
       "  {'idx': 10,\n",
       "   'text': '<b>-HEX</b>    Hex number for xor-ing the zipfile contents. If not supplied all tools that use it take  0x55  as\\n        default value for xor-ing.  <b>OUTFILE</b> Output file name for zzxorcopy.',\n",
       "   'section': 'OPTIONS',\n",
       "   'is_option': True,\n",
       "   'short': ['-HEX'],\n",
       "   'long': [],\n",
       "   'expectsarg': False,\n",
       "   'argument': None,\n",
       "   'nestedcommand': False},\n",
       "  {'idx': 11,\n",
       "   'text': '       <b>zziplib</b> was written by Guido Draheim &lt;<a href=\"mailto:guidod@gmx.de\">guidod@gmx.de</a>&gt;.',\n",
       "   'section': 'AUTHORS',\n",
       "   'is_option': False},\n",
       "  {'idx': 12,\n",
       "   'text': '       This manual page was written by Ricardo Mones &lt;<a href=\"mailto:mones@aic.uniovi.es\">mones@aic.uniovi.es</a>&gt;, for the Debian GNU/Linux system (but\\n       may be used by others).',\n",
       "   'section': 'AUTHORS',\n",
       "   'is_option': False},\n",
       "  {'idx': 13,\n",
       "   'text': '                                                  June 25, 2004                                   <a href=\"http://manpages.ubuntu.com/manpages/precise/en/man1/ZZIPLIB-BIN.1.html\">ZZIPLIB-BIN</a>(1)',\n",
       "   'section': 'AUTHORS',\n",
       "   'is_option': False}],\n",
       " 'aliases': [['zzxordir', 10], ['zzcat', 1], ['zzdir', 1], ['zzxorcat', 1]],\n",
       " 'partialmatch': False,\n",
       " 'multicommand': False,\n",
       " 'updated': False,\n",
       " 'nestedcommand': False}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
