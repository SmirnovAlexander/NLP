{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task \n",
    "\n",
    "## What I did\n",
    "\n",
    "- added data from `man` pages:\n",
    "    - 1st: pretraining only on `man`\n",
    "    - 2nd: training only on provided data\n",
    "- added scheduler;\n",
    "- changed optimizer;\n",
    "- added l2 regularization;\n",
    "- made model bigger;\n",
    "\n",
    "## What I achieved\n",
    "\n",
    "- `0.453` on val part;\n",
    "- `0.227` on handcrafted part;\n",
    "- `-0.234` on mined part;\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "- this notebook contains last run logs\n",
    "- it will be easier to navigate w/ [table of contents jupyter extension](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html)\n",
    "- everything I did I put in [wandb logs](https://wandb.ai/furiousteabag/text2bash_final)\n",
    "- **IMPORTANT**: I created new workspace where I cleaned everything out, so here is brief description of what it contains:\n",
    "    - **hyperparams** panel: here I showed only necessary hyperparams agains last three most important ones: `val_score`, `handcrafted_score`, `mined_score`, so please take a look at it;\n",
    "    - **base** panel: losses an lr's of 2nd part of training (training only on provided data);\n",
    "    - **man** panel: losses an lr's of 1st part of training (pretraining only on `man`);\n",
    "    - [artifacts](https://wandb.ai/furiousteabag/text2bash_final/artifacts/models/base_model/bd131a116fe133b574c5): I logged best model from each run so it is easy to reproduce any provided results;\n",
    "    - if you click on any run (e.g. [v3](https://wandb.ai/furiousteabag/text2bash_final/runs/3qq32bq6)) you will see all parameters and gradients during training (so it is easy to catch exploding gradients)\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "- model size matters much\n",
    "- `man` data matters a bit\n",
    "- `handcrafted` score does not strongly correlates w/ `mined` (`v1` run vs `v3` run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting bashlex grammar using file: /home/furiousteabag/Projects/NLP/04_generation/./utils/bashlint/grammar/grammar100.txt\n",
      "Bashlint grammar set up (148 utilities)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# standart libraries\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import regex\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "\n",
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "# cmd preprocessing & metric calculation\n",
    "import sys\n",
    "sys.path.append(\"./utils/\")\n",
    "from bashlint.data_tools import bash_parser, pretty_print, cmd2template\n",
    "from metric.metric_utils import compute_metric\n",
    "\n",
    "# text preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertModel, EncoderDecoderConfig, EncoderDecoderModel\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 322\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "MAX_TEXT_LENGTH = 256\n",
    "MAX_CODE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# looking up for specific stop words\n",
    "tfidf = TfidfVectorizer(max_df=0.3, min_df=1)\n",
    "tfidf.fit(train_data[\"invocation\"].apply(lambda x: \" \".join([stemmer.stem(t) for t in x.split()])))\n",
    "specific_stop_words = tfidf.stop_words_\n",
    "specific_stop_words = {}\n",
    "print(specific_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = filter(lambda x: x not in stopwords, text.split())\n",
    "    tokens = map(stemmer.stem, tokens)\n",
    "    text = \" \".join(tokens)\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def get_invocations_cmds(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        contents = file.read()\n",
    "        invocations = re.findall(r\"[^.]- (.+):\\n\", contents)\n",
    "        cmds = re.findall(r\"[^.]`(.+)`\\n|\\0\", contents)\n",
    "    return invocations, cmds\n",
    "\n",
    "\n",
    "def get_invocations_cmds_from_dir(path):\n",
    "    paths = glob.glob(os.path.join(path, \"*.md\"))\n",
    "    all_invocations, all_cmds = [], []\n",
    "\n",
    "    for path in paths:\n",
    "        invocations, cmds = get_invocations_cmds(path)\n",
    "        all_invocations.extend(invocations)\n",
    "        all_cmds.extend(cmds)\n",
    "    \n",
    "    assert len(all_invocations) == len(all_cmds)\n",
    "    \n",
    "    return all_invocations, all_cmds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToBashDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, cmds, text_tokenizer, cmd_tokenizer,\n",
    "                 max_text_length=MAX_TEXT_LENGTH, max_code_length=MAX_CODE_LENGTH):\n",
    "        \n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.cmd_tokenizer = cmd_tokenizer\n",
    "        self.max_text_length = max_text_length\n",
    "        self.max_code_length = max_code_length\n",
    "        \n",
    "        self.items = []\n",
    "        \n",
    "        for text, cmd in zip(texts, cmds):\n",
    "            text_tokenized = text_tokenizer.tokenize(text, add_bos=True, add_eos=True)\n",
    "            cmd_tokenized = cmd_tokenizer.tokenize(cmd, add_bos=True, add_eos=True)\n",
    "            if len(text_tokenized) > max_text_length:\n",
    "                text_tokenized = text_tokenized[:max_text_length-1] + text_tokenized[-1:]\n",
    "            if len(cmd_tokenized) > max_code_length:\n",
    "                cmd_tokenized = cmd_tokenized[:max_code_length-1] + cmd_tokenized[-1:]\n",
    "            self.items.append((text_tokenized, cmd_tokenized))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    text_idxs = [torch.tensor(item[0]) for item in batch]\n",
    "    cmd_idxs = [torch.tensor(item[1]) for item in batch]\n",
    "    \n",
    "    text_idxs = pad_sequence(text_idxs, padding_value=PAD_ID, batch_first=True)\n",
    "    cmd_idxs = pad_sequence(cmd_idxs, padding_value=PAD_ID, batch_first=True)\n",
    "    \n",
    "    return text_idxs, cmd_idxs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(text_tokenized, model, max_len=20):\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(DEVICE)\n",
    "    text_tokenized = text_tokenized.unsqueeze(0).to(DEVICE)\n",
    "    cmd_prediction = torch.tensor([BOS_ID]).unsqueeze(0).to(DEVICE)\n",
    "    next_piece = BOS_ID\n",
    "    \n",
    "    while next_piece != EOS_ID and cmd_prediction.shape[-1] < max_len:\n",
    "        #print(cmd_prediction)\n",
    "        out = model(input_ids=text_tokenized, decoder_input_ids=cmd_prediction)\n",
    "        next_piece = torch.argmax(out.logits.squeeze(0)[-1]).item()\n",
    "        cmd_prediction = torch.cat((cmd_prediction, torch.tensor([[next_piece]]).to(DEVICE)), dim=1)\n",
    "        \n",
    "    return cmd_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_decode(i, data, ds, model, max_len=20):\n",
    "    \n",
    "    for k, v in dict(data.iloc[i]).items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    text_tokenized = torch.tensor(ds[i][0])\n",
    "    cmd_prediction = greedy_decode(text_tokenized, model, max_len)\n",
    "    cmd_prediction = [x.item() for x in cmd_prediction]\n",
    "    print(f\"cmd_predicted: {cmd_tokenizer.decode(cmd_prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_eval(data, ds, model, sample_size=5, max_len=20):\n",
    "    indexes = random.sample(range(len(ds)), sample_size)\n",
    "    for i in indexes:\n",
    "        eval_decode(i, data, ds, model, max_len)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.01):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.best_epoch = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss, step):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = step\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = step\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            criterion,\n",
    "            optimizer, \n",
    "            pad_token_id,\n",
    "            device,\n",
    "            stopper,\n",
    "            watch=True,\n",
    "            prefix=\"\",\n",
    "            run=None,\n",
    "            scheduler=None,\n",
    "            max_grad_norm=-1\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        self._criterion = criterion\n",
    "        self._optimizer = optimizer\n",
    "        self._pad_token_id = pad_token_id\n",
    "        self._device = device\n",
    "        self._run = run\n",
    "        self._stopper = stopper\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        self._scheduler = scheduler\n",
    "        self._prefix = prefix\n",
    "        \n",
    "        self._model = model.to(self._device)\n",
    "        \n",
    "        if self._run is not None:\n",
    "            if watch:\n",
    "                wandb.watch(self._model, criterion=self._criterion, log=\"all\", log_freq=1000, log_graph=True)\n",
    "\n",
    "        self._n_epoch = 0\n",
    "        self._n_iter = 0\n",
    "\n",
    "    def train(self, dataloaders, n_epochs):\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            train_loss = self._train_step(dataloaders[\"train\"])\n",
    "            val_loss = self._val_step(dataloaders[\"val\"])\n",
    "            self._n_epoch += 1\n",
    "            tqdm.write(f\"Epoch: {self._n_epoch} | train_loss: {train_loss:.3f} | val_loss: {val_loss:.3f}\")\n",
    "            \n",
    "            if self._run is not None:\n",
    "                #self._run.log(data={\"loss/train\": train_loss, \"loss/val\": val_loss}, step=self._n_epoch)\n",
    "                wandb.log(data={f\"{self._prefix}loss/train\": train_loss, f\"{self._prefix}loss/val\": val_loss, f\"{self._prefix}epoch\": self._n_epoch})\n",
    "                wandb.log(data={f\"{self._prefix}lr\": self._optimizer.param_groups[0][\"lr\"], f\"{self._prefix}epoch\": self._n_epoch})\n",
    "                \n",
    "            self._stopper(val_loss, self._n_epoch)\n",
    "            torch.save(self._model.state_dict(), f\"./checkpoints/{self._prefix}{self._n_epoch}_{val_loss}.pt\")\n",
    "            \n",
    "            if self._stopper.early_stop:\n",
    "                break\n",
    "        \n",
    "        if self._run is not None:\n",
    "            # they are not the best but the first one\n",
    "            # to achieve loss score with given stopper\n",
    "            # threshold\n",
    "            #self._run.summary[\"best_epoch\"] = self._stopper.best_epoch\n",
    "            #self._run.summary[\"best_val_loss\"] = self._stopper.best_loss\n",
    "            wandb.summary[f\"{self._prefix}best_epoch\"] = self._stopper.best_epoch\n",
    "            wandb.summary[f\"{self._prefix}best_val_loss\"] = self._stopper.best_loss\n",
    "            \n",
    "        best_model_path = f\"./checkpoints/{self._prefix}{self._stopper.best_epoch}_{self._stopper.best_loss}.pt\"\n",
    "        \n",
    "        artifact = wandb.Artifact(f\"{self._prefix[:-1]}_model\", type=\"models\", metadata=config)\n",
    "        artifact.add_file(best_model_path)\n",
    "        #run.log_artifact(artifact)\n",
    "        wandb.log_artifact(artifact)\n",
    "            \n",
    "        self._model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        self._model.train()\n",
    "        epoch_loss = 0\n",
    "        for text_idxs, cmd_idxs in dataloader:\n",
    "            \n",
    "            text_idxs = text_idxs.to(self._device)\n",
    "            decoder_input = cmd_idxs[..., :-1].to(self._device)\n",
    "            target = cmd_idxs[..., 1:].to(self._device)\n",
    "            \n",
    "            text_idxs_mask = torch.where(text_idxs != self._pad_token_id, 1, 0).to(self._device)\n",
    "            decoder_input_mask = torch.where(decoder_input != self._pad_token_id, 1, 0).to(self._device)\n",
    "            \n",
    "            self._optimizer.zero_grad()        \n",
    "            \n",
    "            out = self._model(\n",
    "                input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "                attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "\n",
    "            logits = out.logits\n",
    "\n",
    "            loss = self._criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if self._max_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n",
    "                \n",
    "            self._optimizer.step()\n",
    "            \n",
    "            if self._scheduler:\n",
    "                self._scheduler.step()\n",
    "            \n",
    "        return epoch_loss / len(dataloader)\n",
    "    \n",
    "    def _val_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        self._model.eval()\n",
    "        epoch_loss = 0\n",
    "        for text_idxs, cmd_idxs in dataloader:\n",
    "            \n",
    "            text_idxs = text_idxs.to(self._device)\n",
    "            decoder_input = cmd_idxs[..., :-1].to(self._device)\n",
    "            target = cmd_idxs[..., 1:].to(self._device)\n",
    "            \n",
    "            text_idxs_mask = torch.where(text_idxs != self._pad_token_id, 1, 0).to(self._device)\n",
    "            decoder_input_mask = torch.where(decoder_input != self._pad_token_id, 1, 0).to(self._device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = self._model(\n",
    "                    input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "                    attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "                logits = out.logits\n",
    "                loss = self._criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchGenerator:\n",
    "    def __init__(\n",
    "            self, pad_id, eos_id, bos_id,\n",
    "            max_length=20, beam_width=5, temperature=1.5,\n",
    "            device=torch.device('cuda'),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        pad_id : int\n",
    "        eos_id : int\n",
    "        bos_id : int\n",
    "        max_length : int\n",
    "            Maximum length of output sequence\n",
    "        beam_width : int\n",
    "            Width of the beam\n",
    "        temperature : float\n",
    "            Softmax temperature\n",
    "        device : torch.device\n",
    "            Your model device\n",
    "        \"\"\"\n",
    "        self.pad_id = pad_id\n",
    "        self.eos_id = eos_id\n",
    "        self.bos_id = bos_id\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.beam_width = beam_width\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def get_result(self, model, input_text_tokens):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : TextToBashModel\n",
    "        input_text_tokens : torch.tensor\n",
    "            One object input tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        chains = torch.full([self.beam_width, 1], self.bos_id).to(self.device)\n",
    "        chain_probabilities = torch.zeros(self.beam_width).to(self.device)\n",
    "        if_chain_ready = torch.full([self.beam_width], False).to(self.device)\n",
    "        \n",
    "        # saving encoder outputs\n",
    "        input_text_tokens = input_text_tokens.repeat([self.beam_width, 1]).to(self.device)\n",
    "        encoder_outputs = model._prepare_encoder_decoder_kwargs_for_generation(\n",
    "            input_ids=input_text_tokens,\n",
    "            model_kwargs={})[\"encoder_outputs\"]\n",
    "        \n",
    "        idx = 0\n",
    "        while idx <= self.max_length and not all(if_chain_ready):\n",
    "            \n",
    "            logits = model(encoder_outputs=encoder_outputs, decoder_input_ids=chains).logits[:, -1, :]\n",
    "            \n",
    "            probs = torch.log_softmax(logits / self.temperature, dim=-1)\n",
    "            probs, tokens = torch.topk(probs, k=self.beam_width)\n",
    "            \n",
    "            if idx == 0:\n",
    "                # pick all top tokens\n",
    "                # 0 because current chains are same\n",
    "                beam_tokens = tokens[0].view(self.beam_width, 1)\n",
    "                chains = torch.cat([chains, beam_tokens], dim=-1)\n",
    "                chain_probabilities += probs[0]\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            chain_probabilities_beam = chain_probabilities.reshape(-1, 1).repeat([1, self.beam_width])\n",
    "            ready_chains_idxs = if_chain_ready.nonzero()\n",
    "            \n",
    "            # if chain is already ready,\n",
    "            # keep one instance of it and add\n",
    "            # padding\n",
    "            probs[ready_chains_idxs, :] = -float(\"inf\")\n",
    "            probs[ready_chains_idxs, 0] = 0\n",
    "            tokens[ready_chains_idxs, :] = self.pad_id\n",
    "\n",
    "            chain_probabilities_beam = chain_probabilities_beam + probs\n",
    "            \n",
    "            # choosing best across all options\n",
    "            best_sequences = torch.argsort(chain_probabilities_beam.flatten())[-self.beam_width:]\n",
    "\n",
    "            chains = torch.cat([chains[best_sequences // self.beam_width, :], tokens.flatten()[best_sequences].view(-1, 1)], dim=-1)\n",
    "            chain_probabilities = chain_probabilities_beam.flatten()[best_sequences]\n",
    "\n",
    "            if_chain_ready = ((chains[:, -1] == self.eos_id) | (chains[:, -1] == self.pad_id))\n",
    "                \n",
    "            idx += 1\n",
    "        \n",
    "        return list(zip(chains, chain_probabilities))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(model, beam_search_engine, data, n=5):\n",
    "    all_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            print('invocation:', data[\"invocation\"].iloc[i])\n",
    "            print('invocation preprocessed:', data[\"invocation_preprocessed\"].iloc[i])\n",
    "            print('cmd:', data[\"cmd\"].iloc[i])\n",
    "            if \"cmd_preprocessed\" in data.columns:\n",
    "                print('cmd preprocessed:', data[\"cmd_preprocessed\"].iloc[i])\n",
    "\n",
    "            text_tokenized = invocation_tokenizer.tokenize(data[\"invocation_preprocessed\"].iloc[i], add_bos=True, add_eos=True)\n",
    "            src = torch.tensor(text_tokenized)\n",
    "            pred = beam_search_engine.get_result(model, src)\n",
    "\n",
    "            #print('greedy decode:', cmd_tokenizer.decode(list(map(int, greedy_decode(src, model)))))\n",
    "            scores = []\n",
    "            for x, proba in pred[:5]:\n",
    "                pred_cmd = cmd_tokenizer.decode(list(map(int, x)))\n",
    "                score = compute_metric(pred_cmd, 1, data[\"cmd\"].iloc[i])\n",
    "                scores.append(score)\n",
    "                print(pred_cmd, round(proba.item(), 2))\n",
    "            print(max(scores))\n",
    "            print()\n",
    "            all_scores.append(max(scores))\n",
    "\n",
    "    print(f\"average score: {np.mean(all_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, weight_decay=0.01, init_lr=1e-3, betas=(0.9, 0.999)):\n",
    "    \"\"\"\n",
    "        model: инициализированная модель\n",
    "        weight_decay: коэффициент l2 регуляризации\n",
    "        \n",
    "        returns: оптимизатор\n",
    "    \"\"\"\n",
    "    decayed_parameters, not_decayed_parameters = [], []\n",
    "    \n",
    "    for name, params in model.named_parameters():\n",
    "        if any(layer_off in name for layer_off in [\"bias\", \"layer_norm\"]):\n",
    "            not_decayed_parameters.append(params)\n",
    "        else:\n",
    "            decayed_parameters.append(params)\n",
    "            \n",
    "    grouped_parameters = [\n",
    "        {'params': decayed_parameters, 'weight_decay': weight_decay},\n",
    "        {'params': not_decayed_parameters, 'weight_decay': 0.}\n",
    "    ]\n",
    "\n",
    "    return torch.optim.AdamW(grouped_parameters, lr=init_lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_scores(model, df, beam_engine):\n",
    "    all_scores = []\n",
    "\n",
    "    for i, (text, target_cmd) in tqdm(enumerate(zip(df.invocation_preprocessed.values, df.cmd.values)), total=len(df)):\n",
    "        \n",
    "        input_tokens = invocation_tokenizer.tokenize(text, add_bos=True, add_eos=True)\n",
    "        if len(input_tokens) > MAX_TEXT_LENGTH:\n",
    "            input_tokens = input_tokens[:MAX_TEXT_LENGTH-1] + input_tokens[-1:]\n",
    "        input_tokens = torch.tensor(input_tokens)\n",
    "        \n",
    "        predictions = beam_engine.get_result(model, input_tokens)\n",
    "        \n",
    "        # get only 5 top results\n",
    "        predictions = predictions[:5]\n",
    "        object_scores = []\n",
    "        for output_tokens, proba in predictions:\n",
    "            output_cmd = cmd_tokenizer.decode(list(map(int, output_tokens)))\n",
    "            score = compute_metric(output_cmd, 1, target_cmd)\n",
    "            object_scores.append(score)\n",
    "        \n",
    "        all_scores.append(max(object_scores))\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_temperature(model, valid_data):\n",
    "    scores = []\n",
    "    temperatures = np.arange(0.5, 2.1, 0.1)\n",
    "    for temperature in temperatures:\n",
    "        beam_search_engine = BeamSearchGenerator(\n",
    "            pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "            max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "            temperature=temperature, device=DEVICE\n",
    "        )\n",
    "        scores.append(np.mean(compute_all_scores(model, valid_data, beam_search_engine)))\n",
    "    return temperatures[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"train\": pd.read_csv('data/train_data.csv'),\n",
    "    \"test\": pd.read_csv('data/test_data.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "invocations, cmds = get_invocations_cmds_from_dir(\"./tdlr_data/linux/\")\n",
    "invocations_common, cmds_common = get_invocations_cmds_from_dir(\"./tdlr_data/common/\")\n",
    "invocations.extend(invocations_common)\n",
    "cmds.extend(cmds_common)\n",
    "\n",
    "data[\"tldr\"] = pd.DataFrame.from_records({\"invocation\": invocations, \"cmd\": cmds})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "man = []\n",
    "with open(\"./data/manpage-data.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        command = json.loads(line)\n",
    "        man.append((command[\"synopsis\"], command[\"name\"]))\n",
    "        \n",
    "data[\"man\"] = pd.DataFrame(man, columns=[\"invocation\", \"cmd\"])\n",
    "data[\"man\"] = data[\"man\"].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "#stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "for split, df in data.items():\n",
    "    data[split][\"invocation_preprocessed\"] = df[\"invocation\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\"]:\n",
    "    data[split][\"cmd_preprocessed\"] = data[split][\"cmd\"].apply(partial(cmd2template, loose_constraints=True))\n",
    "    #data[split] = data[split][data[split][\"cmd_cleaned\"].str.strip().astype(bool)] # removing empty rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train / val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train_train\"], data[\"train_val\"] = train_test_split(data[\"train\"], test_size=0.05, random_state=seed)\n",
    "data[\"man_train\"], data[\"man_val\"] = train_test_split(data[\"man\"], test_size=0.05, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVOCATION_VOCAB_SIZE = 10000\n",
    "\n",
    "invocation_tokenizer = io.BytesIO()\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(list(data[\"man_train\"][\"invocation_preprocessed\"]) + list(data[\"train_train\"][\"invocation_preprocessed\"])),\n",
    "    model_writer=invocation_tokenizer,\n",
    "    vocab_size=INVOCATION_VOCAB_SIZE,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")\n",
    "invocation_tokenizer = spm.SentencePieceProcessor(model_proto=invocation_tokenizer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMD_VOCAB_SIZE = 12000\n",
    "\n",
    "cmd_tokenizer = io.BytesIO()\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(list(data[\"man_train\"][\"cmd\"]) + list(data[\"train_train\"][\"cmd_preprocessed\"])),\n",
    "    model_writer=cmd_tokenizer,\n",
    "    vocab_size=CMD_VOCAB_SIZE,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")\n",
    "cmd_tokenizer = spm.SentencePieceProcessor(model_proto=cmd_tokenizer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test', 'tldr', 'man', 'train_train', 'train_val', 'man_train', 'man_val'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "datasets[\"man_train\"] = TextToBashDataset(\n",
    "    texts=data[\"man_train\"][\"invocation_preprocessed\"],\n",
    "    cmds=data[\"man_train\"][\"cmd\"],\n",
    "    text_tokenizer=invocation_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)\n",
    "\n",
    "datasets[\"man_val\"] = TextToBashDataset(\n",
    "    texts=data[\"man_val\"][\"invocation_preprocessed\"],\n",
    "    cmds=data[\"man_val\"][\"cmd\"],\n",
    "    text_tokenizer=invocation_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)\n",
    "\n",
    "datasets[\"train_train\"] = TextToBashDataset(\n",
    "    texts=data[\"train_train\"][\"invocation_preprocessed\"],\n",
    "    cmds=data[\"train_train\"][\"cmd_preprocessed\"],\n",
    "    text_tokenizer=invocation_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)\n",
    "\n",
    "datasets[\"train_val\"] = TextToBashDataset(\n",
    "    texts=data[\"train_val\"][\"invocation_preprocessed\"],\n",
    "    cmds=data[\"train_val\"][\"cmd_preprocessed\"],\n",
    "    text_tokenizer=invocation_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAN_BATCH_SIZE = 32\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "\n",
    "loaders_base = {\n",
    "    'train': DataLoader(datasets[\"train_train\"], batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn),\n",
    "    'val': DataLoader(datasets[\"train_val\"], batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn),\n",
    "}\n",
    "loaders_man = {\n",
    "    'train': DataLoader(datasets[\"man_train\"], batch_size=MAN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn),\n",
    "    'val': DataLoader(datasets[\"man_val\"], batch_size=MAN_BATCH_SIZE, collate_fn=collate_fn),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"tokenizer/cmd/vocab_size\": CMD_VOCAB_SIZE,\n",
    "    \"tokenizer/invocation/vocab_size\": INVOCATION_VOCAB_SIZE,\n",
    "    \n",
    "    \"model/hidden_size\": 512,\n",
    "    \"model/num_hidden_layers\": 4,\n",
    "    \"model/num_attention_heads\": 8,\n",
    "    \"model/intermediate_size\": 512 * 4,\n",
    "    \"model/hidden_dropout_prob\": 0.2,\n",
    "    \n",
    "    \"training/man/batch_size\": MAN_BATCH_SIZE,\n",
    "    \"training/man/lr\": 1e-4,\n",
    "    \"training/man/weight_decay\": 0.01,\n",
    "    \"training/man/stopper/patience\": 3,\n",
    "    \"training/man/stopper/min_delta\": 0.01,\n",
    "    \"training/man/max_grad_norm\": -1,\n",
    "    \"training/man/n_epochs\": 15,\n",
    "    \n",
    "    \"training/base/batch_size\": TRAIN_BATCH_SIZE,\n",
    "    \"training/base/lr\": 1e-4,\n",
    "    \"training/base/weight_decay\": 0.01,\n",
    "    \"training/base/stopper/patience\": 4,\n",
    "    \"training/base/stopper/min_delta\": 0.01,\n",
    "    \"training/base/max_grad_norm\": -1,\n",
    "    \"training/base/n_epochs\": 25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfuriousteabag\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/furiousteabag/text2bash_final/runs/3qq32bq6\" target=\"_blank\">dashing-morning-13</a></strong> to <a href=\"https://wandb.ai/furiousteabag/text2bash_final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"text2bash_final\", job_type=\"train_model\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "encoder_config = BertConfig(\n",
    "    vocab_size = invocation_tokenizer.vocab_size(),\n",
    "    hidden_size = config[\"model/hidden_size\"],\n",
    "    num_hidden_layers = config[\"model/num_hidden_layers\"],\n",
    "    num_attention_heads = config[\"model/num_attention_heads\"],\n",
    "    intermediate_size = config[\"model/intermediate_size\"],\n",
    "    hidden_dropout_prob = config[\"model/hidden_dropout_prob\"],\n",
    "    pad_token_id = PAD_ID,\n",
    ")\n",
    "\n",
    "decoder_config = BertConfig(\n",
    "    vocab_size = cmd_tokenizer.vocab_size(),\n",
    "    hidden_size = config[\"model/hidden_size\"],\n",
    "    num_hidden_layers = config[\"model/num_hidden_layers\"],\n",
    "    num_attention_heads = config[\"model/num_attention_heads\"],\n",
    "    intermediate_size = config[\"model/intermediate_size\"],\n",
    "    hidden_dropout_prob = config[\"model/hidden_dropout_prob\"],\n",
    "    pad_token_id = PAD_ID,\n",
    "    is_decoder = True,\n",
    "    add_cross_attention = True\n",
    ")\n",
    "\n",
    "print(decoder_config.is_decoder)\n",
    "print(decoder_config.add_cross_attention)\n",
    "\n",
    "model_config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n",
    "model = EncoderDecoderModel(config=model_config)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text_idxs, cmd_idxs = next(iter(loaders[\"val\"]))\n",
    "decoder_input = cmd_idxs[..., :-1]\n",
    "target = cmd_idxs[..., 1:]\n",
    "\n",
    "print(text_idxs.shape)\n",
    "print(decoder_input.shape)\n",
    "print(target.shape)\n",
    "\n",
    "text_idxs_mask = torch.where(text_idxs != PAD_ID, 1, 0)\n",
    "decoder_input_mask = torch.where(decoder_input != PAD_ID, 1, 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cmd_idxs[..., :-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "out = model(\n",
    "    input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "    attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "\n",
    "logits = out.logits\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training on man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=config[\"training/man/lr\"])\n",
    "optimizer = get_optimizer(model, weight_decay=config[\"training/man/weight_decay\"], init_lr=config[\"training/man/lr\"])\n",
    "stopper = EarlyStopping(patience=config[\"training/man/stopper/patience\"], min_delta=config[\"training/man/stopper/min_delta\"])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=config[\"training/man/lr\"],\n",
    "    epochs=config[\"training/man/n_epochs\"], steps_per_epoch=len(loaders_man[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    pad_token_id=PAD_ID,\n",
    "    device=DEVICE,\n",
    "    run=run,\n",
    "    stopper=stopper,\n",
    "    scheduler=scheduler,\n",
    "    prefix=\"man/\",\n",
    "    max_grad_norm=config[\"training/man/max_grad_norm\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ./checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930453b0dfa548348385a87ed62bfbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 6.725 | val_loss: 5.879\n",
      "Epoch: 2 | train_loss: 5.304 | val_loss: 4.982\n",
      "Epoch: 3 | train_loss: 4.585 | val_loss: 4.428\n",
      "Epoch: 4 | train_loss: 4.108 | val_loss: 4.064\n",
      "Epoch: 5 | train_loss: 3.682 | val_loss: 3.782\n",
      "Epoch: 6 | train_loss: 3.240 | val_loss: 3.574\n",
      "Epoch: 7 | train_loss: 2.799 | val_loss: 3.403\n",
      "Epoch: 8 | train_loss: 2.394 | val_loss: 3.332\n",
      "Epoch: 9 | train_loss: 2.020 | val_loss: 3.327\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: 10 | train_loss: 1.688 | val_loss: 3.284\n",
      "Epoch: 11 | train_loss: 1.401 | val_loss: 3.282\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: 12 | train_loss: 1.156 | val_loss: 3.356\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: 13 | train_loss: 0.961 | val_loss: 3.378\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: 14 | train_loss: 0.806 | val_loss: 3.400\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: 15 | train_loss: 0.689 | val_loss: 3.485\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    }
   ],
   "source": [
    "trainer.train(loaders_man, config[\"training/man/n_epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_engine = BeamSearchGenerator(\n",
    "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "    max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "    temperature=1.5, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invocation: command-line interface for chrony daemon\n",
      "invocation preprocessed: commandlin interfac chroni daemon\n",
      "cmd: chronyc\n",
      "pkid-client -11.23\n",
      "pkid-cli -9.44\n",
      "pki-client -9.3\n",
      "rubyd -8.93\n",
      "pkid -7.57\n",
      "0.0\n",
      "\n",
      "invocation: GNAT toolbox\n",
      "invocation preprocessed: gnat toolbox\n",
      "cmd: mips-linux-gnu-gnatbind-8\n",
      "powerpc-linux-gnuspe-gnatmake-8 -7.58\n",
      "arm-linux-gnueabihf-gnatmake-6 -7.57\n",
      "arm-linux-gnueabihf-gnatmake-7 -7.56\n",
      "arm-linux-gnueabihf-gnatmake-8 -7.49\n",
      "arm-linux-gnueabihf-gnatmake-5 -7.49\n",
      "0.0\n",
      "\n",
      "invocation: SFTP connection handler of FileZilla\n",
      "invocation preprocessed: sftp connect handler filezilla\n",
      "cmd: fzsftp\n",
      "fatget_query -20.61\n",
      "fprint_get -19.99\n",
      "fatget -13.17\n",
      "fatcat -12.38\n",
      "fprint -11.71\n",
      "0.0\n",
      "\n",
      "invocation: show Chinese characters' phonations\n",
      "invocation preprocessed: show chines characters phonat\n",
      "cmd: hime-juyin-learn\n",
      "botch-y-reg-client -22.34\n",
      "botch-y-reg -19.33\n",
      "wordview2d -13.63\n",
      "wordview2 -10.67\n",
      "wordview -9.6\n",
      "0.0\n",
      "\n",
      "invocation: Sets privacy flags or quota for a Protection Database entry\n",
      "invocation preprocessed: set privaci flag quota protect databas entri\n",
      "cmd: pts_setfields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/furiousteabag/.local/share/virtualenvs/base/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git-annex-setcache -17.93\n",
      "git-annex-cache -16.07\n",
      "pts_setpasswd -10.58\n",
      "pts_setquota -10.19\n",
      "lizardfs-setquota -10.08\n",
      "0.0\n",
      "\n",
      "average score: 0.000\n"
     ]
    }
   ],
   "source": [
    "show_examples(model, beam_search_engine, data[\"man_val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training on base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=config[\"training/base/lr\"])\n",
    "optimizer = get_optimizer(model, weight_decay=config[\"training/base/weight_decay\"], init_lr=config[\"training/base/lr\"])\n",
    "stopper = EarlyStopping(patience=config[\"training/base/stopper/patience\"], min_delta=config[\"training/base/stopper/min_delta\"])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=config[\"training/base/lr\"],\n",
    "    epochs=config[\"training/base/n_epochs\"], steps_per_epoch=len(loaders_base[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    pad_token_id=PAD_ID,\n",
    "    device=DEVICE,\n",
    "    run=run,\n",
    "    stopper=stopper,\n",
    "    scheduler=scheduler,\n",
    "    prefix=\"base/\",\n",
    "    watch=False,\n",
    "    max_grad_norm=config[\"training/base/max_grad_norm\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4c0f2744b145928c4bcc16c80252ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 6.988 | val_loss: 4.204\n",
      "Epoch: 2 | train_loss: 3.527 | val_loss: 2.763\n",
      "Epoch: 3 | train_loss: 2.487 | val_loss: 2.007\n",
      "Epoch: 4 | train_loss: 1.876 | val_loss: 1.519\n",
      "Epoch: 5 | train_loss: 1.441 | val_loss: 1.218\n",
      "Epoch: 6 | train_loss: 1.167 | val_loss: 1.033\n",
      "Epoch: 7 | train_loss: 0.974 | val_loss: 0.926\n",
      "Epoch: 8 | train_loss: 0.831 | val_loss: 0.877\n",
      "Epoch: 9 | train_loss: 0.722 | val_loss: 0.816\n",
      "Epoch: 10 | train_loss: 0.630 | val_loss: 0.798\n",
      "Epoch: 11 | train_loss: 0.550 | val_loss: 0.789\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: 12 | train_loss: 0.483 | val_loss: 0.787\n",
      "Epoch: 13 | train_loss: 0.429 | val_loss: 0.775\n",
      "Epoch: 14 | train_loss: 0.377 | val_loss: 0.781\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: 15 | train_loss: 0.335 | val_loss: 0.794\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: 16 | train_loss: 0.297 | val_loss: 0.802\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: 17 | train_loss: 0.259 | val_loss: 0.817\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: 18 | train_loss: 0.230 | val_loss: 0.825\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    }
   ],
   "source": [
    "trainer.train(loaders_base, config[\"training/base/n_epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invocation: search the files from the current directory tree for \"chrome\"\n",
      "invocation preprocessed: search file current directori tree chrome\n",
      "cmd: find . -exec grep chrome {} \\;\n",
      "cmd preprocessed: find Path -exec grep Regex {} \\;\n",
      "find Path | xargs -I {} grep Regex {} | grep Regex -5.43\n",
      "find Path | xargs -3.49\n",
      "find Path -exec grep Regex {} + -3.01\n",
      "find Path -exec grep Regex {} \\; -2.95\n",
      "find Path | xargs -I {} grep Regex {} -2.41\n",
      "1.0\n",
      "\n",
      "invocation: find all *.dbf files/directories in entire file system\n",
      "invocation preprocessed: find dbf filesdirectori entir file system\n",
      "cmd: find / -name \"*.dbf\"\n",
      "cmd preprocessed: find Path -name Regex\n",
      "find Path -name Regex | xargs -I {} echo {} -9.99\n",
      "find Path Path -4.24\n",
      "find Path -4.15\n",
      "find Path -name Regex -print -2.71\n",
      "find Path -name Regex -0.39\n",
      "1.0\n",
      "\n",
      "invocation: find all the files which have size 0 bytes in temp folder\n",
      "invocation preprocessed: find file size  byte temp folder\n",
      "cmd: find /tmp -type f -empty\n",
      "cmd preprocessed: find Path -type f -empty\n",
      "find Path -size +Size -print -4.16\n",
      "find Path -type f -size Size -3.44\n",
      "find Path -size Size -print -3.05\n",
      "find Path -size +Size -2.85\n",
      "find Path -size Size -1.37\n",
      "0.25\n",
      "\n",
      "invocation: for each line in 'file', print \"result = \" followed by the line backwards.\n",
      "invocation preprocessed: line file print result   follow line backwards\n",
      "cmd: awk '{print \"result =\",$0}' <(rev file)\n",
      "cmd preprocessed: awk Program <( rev File )\n",
      "rev File | cut -f Number -7.21\n",
      "rev File | cut -d Regex -f Number -6.4\n",
      "tac File | awk Program -4.64\n",
      "rev File -4.16\n",
      "rev File | awk Program -3.55\n",
      "-1.0\n",
      "\n",
      "invocation: changes group ownership of /sys/class/gpio/export and /sys/class/gpio/unexport to 'gpio'.\n",
      "invocation preprocessed: chang group ownership sysclassgpioexport sysclassgpiounexport gpio\n",
      "cmd: sudo chgrp gpio /sys/class/gpio/export /sys/class/gpio/unexport\n",
      "cmd preprocessed: chgrp Regex File File\n",
      "find Path -type f -name Regex -print0 | xargs -0 -I {} chgrp Regex {} -12.11\n",
      "find Path -type f -print0 | xargs -0 -I {} chgrp Regex {} -10.57\n",
      "find Path -type f -print0 | xargs -0 -I {} chown Regex {} -10.49\n",
      "chown Regex File -3.79\n",
      "chgrp Regex File -2.53\n",
      "1.0\n",
      "\n",
      "average score: 0.450\n"
     ]
    }
   ],
   "source": [
    "show_examples(model, beam_search_engine, data[\"train_val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temperature = find_best_temperature(model, )\n",
    "print(temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_engine = BeamSearchGenerator(\n",
    "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "    max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "    temperature=1.5, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb5b94fafda464188bd2d66df465591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score on validation: 0.453\n"
     ]
    }
   ],
   "source": [
    "val_scores = compute_all_scores(model, data[\"train_val\"], beam_search_engine)\n",
    "val_score = round(np.mean(val_scores), 3)\n",
    "print(f\"average score on validation: {val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c4d5ddc6fe434aad95886a353cae4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score on handcrafted: 0.142\n"
     ]
    }
   ],
   "source": [
    "handcrafted_scores = compute_all_scores(model, data[\"test\"][data[\"test\"][\"origin\"] == \"handcrafted\"], beam_search_engine)\n",
    "handcrafted_score = round(np.mean(handcrafted_scores), 3)\n",
    "print(f\"average score on handcrafted: {handcrafted_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32c4a28b8154fcb95e1abbda1f1ae44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score on mined part: -0.234\n"
     ]
    }
   ],
   "source": [
    "mined_scores = compute_all_scores(model, data[\"test\"][data[\"test\"][\"origin\"] == \"mined\"], beam_search_engine)\n",
    "mined_score = round(np.mean(mined_scores), 3)\n",
    "print(f\"average score on mined part: {mined_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.summary[\"val_score\"] = val_score\n",
    "wandb.summary[\"handcrafted_score\"] = handcrafted_score\n",
    "wandb.summary[\"mined_score\"] = mined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 50514... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 318.76MB of 318.76MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>base/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>base/loss/train</td><td>█▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>base/loss/val</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>base/lr</td><td>▁▂▃▄▅▆▇█████▇▇▇▆▆▅</td></tr><tr><td>man/epoch</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██</td></tr><tr><td>man/loss/train</td><td>█▆▆▅▄▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>man/loss/val</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>man/lr</td><td>▁▂▄▆▇███▇▇▆▅▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>base/best_epoch</td><td>13</td></tr><tr><td>base/best_val_loss</td><td>0.77511</td></tr><tr><td>base/epoch</td><td>18</td></tr><tr><td>base/loss/train</td><td>0.2295</td></tr><tr><td>base/loss/val</td><td>0.82467</td></tr><tr><td>base/lr</td><td>6e-05</td></tr><tr><td>handcrafted_score</td><td>0.142</td></tr><tr><td>man/best_epoch</td><td>10</td></tr><tr><td>man/best_val_loss</td><td>3.28388</td></tr><tr><td>man/epoch</td><td>15</td></tr><tr><td>man/loss/train</td><td>0.68942</td></tr><tr><td>man/loss/val</td><td>3.48473</td></tr><tr><td>man/lr</td><td>3e-05</td></tr><tr><td>mined_score</td><td>-0.234</td></tr><tr><td>val_score</td><td>0.453</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dashing-morning-13</strong>: <a href=\"https://wandb.ai/furiousteabag/text2bash_final/runs/3qq32bq6\" target=\"_blank\">https://wandb.ai/furiousteabag/text2bash_final/runs/3qq32bq6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211222_211345-3qq32bq6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
