{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 4\n",
    "# Генерация bash команды по текстовому запросу\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "### ФИО: Смирнов Александр Львович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы построите систему, выдающую пользователю последовательность утилит командной строки linux (с нужными флагами) по его текстовому запросу. Вам дан набор пар текстовый запрос - команда на выходе. \n",
    "\n",
    "Решение этого задания будет построено на encoder-decoder архитектуре и модели transformer.\n",
    "\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    "* pytorch\n",
    "* transformers\n",
    "* sentencepiece (bpe токенизация)\n",
    "* clai utils (скачать с гитхаба отсюда https://github.com/IBM/clai/tree/nlc2cmd/utils) \n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "В качестве обучающей выборке используются данные, сгенерированные автоматически по запросам с сайта stack overflow. В качестве тестовых данных используются пары запросов, размеченные асессорами.\n",
    "\n",
    "Данные можно скачать по ссылке: https://drive.google.com/file/d/1n457AAgrMwd5VbT6mGZ_rws3g2wwdEfX/view?usp=sharing\n",
    "\n",
    "### Метрика качества\n",
    "\n",
    "Ваш алгоритм должен выдавать пять вариантов ответа для каждого запроса. \n",
    "Для упрощения задачи метрика качества будет учитывать утилиты и флаги ответа, но не учитывать подставленные значения. Пусть $\\{ u_1, \\ldots, u_T \\}$, $\\{ f_1, \\ldots, f_T \\}$ --- список утилит и множества их флагов ответа алгоритма, $\\{v_1, \\ldots, v_T \\}$, $\\{ \\phi_1, \\ldots, \\phi_T \\}$ --- список утилит и множества их флагов эталонного ответа. Если ответы отличаются по длине, они дополняются `None` утилитой. \n",
    "\n",
    "$$ S = \\frac{1}{T} \\sum_{i=1}^{T} \\left(\\mathbb{I}[u_i = v_i]\\left( 1 + \\frac{1}{2}s(f_i, \\phi_i)\\right) - 1\\right)$$\n",
    "\n",
    "$$ s(f, \\phi) = 1 + \\frac{2 |f \\cap \\phi| - |f \\cup \\phi|}{\\max(|f|, |\\phi|)} $$\n",
    "\n",
    "Метрика учитывает, что предсказать правильную утилиту важнее чем правильный флаг. При этом порядок флагов не важен (однако, чтобы корректно "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "PATH_TO_CLAI_UTILS = \"./utils/\"\n",
    "sys.path.append(PATH_TO_CLAI_UTILS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting bashlex grammar using file: /home/furiousteabag/Projects/NLP/04_generation/./utils/bashlint/grammar/grammar100.txt\n",
      "Bashlint grammar set up (148 utilities)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bashlint.data_tools import bash_parser, pretty_print, cmd2template\n",
    "from metric.metric_utils import compute_metric\n",
    "from functools import partial\n",
    "\n",
    "from collections import Counter\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем данные. В столбце `invocation` находится текстовый запрос, в столбце `cmd` находится релевантная команда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>copy loadable kernel module \"mymodule.ko\" to t...</td>\n",
       "      <td>sudo cp mymodule.ko /lib/modules/$(uname -r)/k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>display all lines containing \"ip_mroute\" in th...</td>\n",
       "      <td>cat /boot/config-`uname -r` | grep IP_MROUTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>display current running kernel's compile-time ...</td>\n",
       "      <td>cat /boot/config-`uname -r`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find all loadable modules for current kernel, ...</td>\n",
       "      <td>find /lib/modules/`uname -r` -regex .*perf.*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look for any instance of \"highmem\" in the curr...</td>\n",
       "      <td>grep “HIGHMEM” /boot/config-`uname -r`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9938</th>\n",
       "      <td>using exec in find command to dispaly the sear...</td>\n",
       "      <td>find . ... -exec cat {} \\; -exec echo \\;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9939</th>\n",
       "      <td>verbosely create intermediate directoriy tmp a...</td>\n",
       "      <td>mkdir -pv /tmp/boostinst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9940</th>\n",
       "      <td>view the manual page of find</td>\n",
       "      <td>man find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>wait 2 seconds and then print \"hello\"</td>\n",
       "      <td>echo \"hello `sleep 2 &amp;`\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>when using vi-insert keymap bind command \"\\c-v...</td>\n",
       "      <td>bind -m vi-insert '\"{\" \"\\C-v{}\\ei\"'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9943 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             invocation  \\\n",
       "0     copy loadable kernel module \"mymodule.ko\" to t...   \n",
       "1     display all lines containing \"ip_mroute\" in th...   \n",
       "2     display current running kernel's compile-time ...   \n",
       "3     find all loadable modules for current kernel, ...   \n",
       "4     look for any instance of \"highmem\" in the curr...   \n",
       "...                                                 ...   \n",
       "9938  using exec in find command to dispaly the sear...   \n",
       "9939  verbosely create intermediate directoriy tmp a...   \n",
       "9940                       view the manual page of find   \n",
       "9941              wait 2 seconds and then print \"hello\"   \n",
       "9942  when using vi-insert keymap bind command \"\\c-v...   \n",
       "\n",
       "                                                    cmd  \n",
       "0     sudo cp mymodule.ko /lib/modules/$(uname -r)/k...  \n",
       "1          cat /boot/config-`uname -r` | grep IP_MROUTE  \n",
       "2                           cat /boot/config-`uname -r`  \n",
       "3          find /lib/modules/`uname -r` -regex .*perf.*  \n",
       "4                grep “HIGHMEM” /boot/config-`uname -r`  \n",
       "...                                                 ...  \n",
       "9938           find . ... -exec cat {} \\; -exec echo \\;  \n",
       "9939                           mkdir -pv /tmp/boostinst  \n",
       "9940                                           man find  \n",
       "9941                           echo \"hello `sleep 2 &`\"  \n",
       "9942                bind -m vi-insert '\"{\" \"\\C-v{}\\ei\"'  \n",
       "\n",
       "[9943 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тестовых данных столбец `origin` отвечает за источник данных, значения `handrafted` соответствуют парам, составленными людьми, а `mined` парам, собранным автоматически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>create ssh connection to specified ip from spe...</td>\n",
       "      <td>ssh user123@176.0.13.154</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>search for commands containing string \"zeppeli...</td>\n",
       "      <td>history | grep zeppelin</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>search for location of specified file or appli...</td>\n",
       "      <td>whereis python3</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grant all rights to root folder</td>\n",
       "      <td>sudo chmod 777 -R /</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>search in running processes for specified name</td>\n",
       "      <td>ps -aux | grep zepp</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>outputs the 2nd and every later field, so \"b c...</td>\n",
       "      <td>echo a b c d | cut -d\" \" -f2-</td>\n",
       "      <td>mined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>outputs abcfgh</td>\n",
       "      <td>echo abcdefgh | cut -c1-3,6-8</td>\n",
       "      <td>mined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>displays \"b\", separating fields by any number ...</td>\n",
       "      <td>echo a  b|awk '{print $2}'</td>\n",
       "      <td>mined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>displays \"ba\", preserving the order of the fie...</td>\n",
       "      <td>echo a  b|awk '{print $2 $1}'</td>\n",
       "      <td>mined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>compact a sequence of spaces into a single space.</td>\n",
       "      <td>echo a  b|sed \"s/  */ /g\" |cut -f2 -d\" \"</td>\n",
       "      <td>mined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>721 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            invocation  \\\n",
       "0    create ssh connection to specified ip from spe...   \n",
       "1    search for commands containing string \"zeppeli...   \n",
       "2    search for location of specified file or appli...   \n",
       "3                      grant all rights to root folder   \n",
       "4       search in running processes for specified name   \n",
       "..                                                 ...   \n",
       "716  outputs the 2nd and every later field, so \"b c...   \n",
       "717                                     outputs abcfgh   \n",
       "718  displays \"b\", separating fields by any number ...   \n",
       "719  displays \"ba\", preserving the order of the fie...   \n",
       "720  compact a sequence of spaces into a single space.   \n",
       "\n",
       "                                          cmd       origin  \n",
       "0                    ssh user123@176.0.13.154  handcrafted  \n",
       "1                     history | grep zeppelin  handcrafted  \n",
       "2                             whereis python3  handcrafted  \n",
       "3                         sudo chmod 777 -R /  handcrafted  \n",
       "4                         ps -aux | grep zepp  handcrafted  \n",
       "..                                        ...          ...  \n",
       "716             echo a b c d | cut -d\" \" -f2-        mined  \n",
       "717             echo abcdefgh | cut -c1-3,6-8        mined  \n",
       "718                echo a  b|awk '{print $2}'        mined  \n",
       "719             echo a  b|awk '{print $2 $1}'        mined  \n",
       "720  echo a  b|sed \"s/  */ /g\" |cut -f2 -d\" \"        mined  \n",
       "\n",
       "[721 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Проведите предобработку текста. Рекомендуется:\n",
    "* перевести всё в нижний регистр\n",
    "* удалить стоп-слова (специфичные для выборки)\n",
    "* провести стемминг токенов\n",
    "* удалить все символы кроме латинских букв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# looking up for specific stop words\n",
    "tfidf = TfidfVectorizer(max_df=0.3, min_df=1)\n",
    "tfidf.fit(train_data[\"invocation\"].apply(lambda x: \" \".join([stemmer.stem(t) for t in x.split()])))\n",
    "specific_stop_words = tfidf.stop_words_\n",
    "specific_stop_words = {}\n",
    "print(specific_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = filter(lambda x: x not in stopwords, text.split())\n",
    "    tokens = map(stemmer.stem, tokens)\n",
    "    text = \" \".join(tokens)\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_cleaned'] = train_data['invocation'].apply(clean_text)\n",
    "test_data['text_cleaned'] = test_data['invocation'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки кода воспользуемся функцией `cmd2template`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cmd_cleaned'] = train_data['cmd'].apply(partial(cmd2template, loose_constraints=True))\n",
    "test_data['cmd_cleaned'] = test_data['cmd'].apply(partial(cmd2template, loose_constraints=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>cmd_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>copy loadable kernel module \"mymodule.ko\" to t...</td>\n",
       "      <td>sudo cp mymodule.ko /lib/modules/$(uname -r)/k...</td>\n",
       "      <td>copi loadabl kernel modul mymoduleko driver mo...</td>\n",
       "      <td>cp File $( uname -r )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>display all lines containing \"ip_mroute\" in th...</td>\n",
       "      <td>cat /boot/config-`uname -r` | grep IP_MROUTE</td>\n",
       "      <td>display line contain ipmroute current kernel c...</td>\n",
       "      <td>cat $( uname -r ) | grep Regex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>display current running kernel's compile-time ...</td>\n",
       "      <td>cat /boot/config-`uname -r`</td>\n",
       "      <td>display current run kernel compiletim config file</td>\n",
       "      <td>cat $( uname -r )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find all loadable modules for current kernel, ...</td>\n",
       "      <td>find /lib/modules/`uname -r` -regex .*perf.*</td>\n",
       "      <td>find loadabl modul current kernel whose name i...</td>\n",
       "      <td>find Path $( uname -r ) -regex Regex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look for any instance of \"highmem\" in the curr...</td>\n",
       "      <td>grep “HIGHMEM” /boot/config-`uname -r`</td>\n",
       "      <td>look instanc highmem current kernel compiletim...</td>\n",
       "      <td>grep Regex $( uname -r )</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          invocation  \\\n",
       "0  copy loadable kernel module \"mymodule.ko\" to t...   \n",
       "1  display all lines containing \"ip_mroute\" in th...   \n",
       "2  display current running kernel's compile-time ...   \n",
       "3  find all loadable modules for current kernel, ...   \n",
       "4  look for any instance of \"highmem\" in the curr...   \n",
       "\n",
       "                                                 cmd  \\\n",
       "0  sudo cp mymodule.ko /lib/modules/$(uname -r)/k...   \n",
       "1       cat /boot/config-`uname -r` | grep IP_MROUTE   \n",
       "2                        cat /boot/config-`uname -r`   \n",
       "3       find /lib/modules/`uname -r` -regex .*perf.*   \n",
       "4             grep “HIGHMEM” /boot/config-`uname -r`   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  copi loadabl kernel modul mymoduleko driver mo...   \n",
       "1  display line contain ipmroute current kernel c...   \n",
       "2  display current run kernel compiletim config file   \n",
       "3  find loadabl modul current kernel whose name i...   \n",
       "4  look instanc highmem current kernel compiletim...   \n",
       "\n",
       "                            cmd_cleaned  \n",
       "0                 cp File $( uname -r )  \n",
       "1        cat $( uname -r ) | grep Regex  \n",
       "2                     cat $( uname -r )  \n",
       "3  find Path $( uname -r ) -regex Regex  \n",
       "4              grep Regex $( uname -r )  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучение и валидацию. Т.к. данных очень мало, то для валидационной выборки выделим только 100 примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = train_data.iloc[-100:]\n",
    "train_data = train_data.iloc[:-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Стандартный формат входных данных для трансформеров — BPE токены. Воспользуйтесь библиотекой sentencepiece для обучения токенайзеров для текста и кода. Используйте небольшое число токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "\n",
    "MAX_TEXT_LENGTH = 256\n",
    "MAX_CODE_LENGTH = 40\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mkdir -p ./tokenizers\n",
    "\n",
    "text_tokenizer = io.BytesIO()\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(train_data[\"text_cleaned\"]),\n",
    "    #sentence_iterator=iter(train_data[\"invocation\"]),\n",
    "    model_writer=text_tokenizer,\n",
    "    vocab_size=4000,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")\n",
    "text_tokenizer = spm.SentencePieceProcessor(model_proto=text_tokenizer.getvalue())\n",
    "\n",
    "cmd_tokenizer = io.BytesIO()\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(train_data[\"cmd_cleaned\"]),\n",
    "    model_writer=cmd_tokenizer,\n",
    "    #model_prefix='./tokenizers/sp_cmd',\n",
    "    vocab_size=500,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")\n",
    "cmd_tokenizer = spm.SentencePieceProcessor(model_proto=cmd_tokenizer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text:\n",
      "copi loadabl kernel modul mymoduleko driver modul directori matchig current kernel\n",
      "\n",
      "\n",
      "Indexes:\n",
      "[1, 48, 1454, 559, 431, 1253, 216, 3554, 3103, 12, 3060, 1253, 5, 30, 1826, 6, 431, 2]\n",
      "\n",
      "\n",
      "Decoded version:\n",
      "copi loadabl kernel modul mymoduleko driver modul directori matchig current kernel\n",
      "\n",
      "\n",
      "Pieces:\n",
      "<s> ▁copi ▁load abl ▁kernel ▁modul ▁my module ko ▁ driver ▁modul ▁directori ▁match ig ▁current ▁kernel </s>\n"
     ]
    }
   ],
   "source": [
    "text = train_data[\"text_cleaned\"][0]\n",
    "print(\"Initial text:\")\n",
    "print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "tokenized = text_tokenizer.tokenize(text, add_bos=True, add_eos=True)\n",
    "print(\"Indexes:\")\n",
    "print(tokenized)\n",
    "print(\"\\n\")\n",
    "print(\"Decoded version:\")\n",
    "print(text_tokenizer.decode(tokenized))\n",
    "print(\"\\n\")\n",
    "print(\"Pieces:\")\n",
    "print(*text_tokenizer.IdToPiece(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cmd:\n",
      "cp File $( uname -r )\n",
      "\n",
      "\n",
      "Indexes:\n",
      "[1, 84, 12, 14, 45, 26, 210, 11, 4, 41, 31, 2]\n",
      "\n",
      "\n",
      "Decoded version:\n",
      "cp File $( uname -r )\n",
      "\n",
      "\n",
      "Pieces:\n",
      "<s> ▁cp ▁File ▁ $ ( ▁u name ▁- r ▁) </s>\n"
     ]
    }
   ],
   "source": [
    "cmd = train_data[\"cmd_cleaned\"][0]\n",
    "print(\"Initial cmd:\")\n",
    "print(cmd)\n",
    "print(\"\\n\")\n",
    "\n",
    "tokenized = cmd_tokenizer.tokenize(cmd, add_bos=True, add_eos=True)\n",
    "print(\"Indexes:\")\n",
    "print(tokenized)\n",
    "print(\"\\n\")\n",
    "print(\"Decoded version:\")\n",
    "print(cmd_tokenizer.decode(tokenized))\n",
    "print(\"\\n\")\n",
    "print(\"Pieces:\")\n",
    "print(*cmd_tokenizer.IdToPiece(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Задайте датасеты и лоадеры для ваших данных. Каждая последовательность должна начинаться с BOS токена и заканчиваться EOS токеном. Рекомендуется ограничить длину входных и выходных последовательностей!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToBashDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, cmds, text_tokenizer, cmd_tokenizer,\n",
    "                 max_text_length=MAX_TEXT_LENGTH, max_code_length=MAX_CODE_LENGTH):\n",
    "        \n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.cmd_tokenizer = cmd_tokenizer\n",
    "        self.max_text_length = max_text_length\n",
    "        self.max_code_length = max_code_length\n",
    "        \n",
    "        self.items = []\n",
    "        \n",
    "        for text, cmd in zip(texts, cmds):\n",
    "            text_tokenized = text_tokenizer.tokenize(text, add_bos=True, add_eos=True)\n",
    "            cmd_tokenized = cmd_tokenizer.tokenize(cmd, add_bos=True, add_eos=True)\n",
    "            if len(text_tokenized) > max_text_length:\n",
    "                text_tokenized = text_tokenized[:max_text_length-1] + text_tokenized[-1:]\n",
    "            if len(cmd_tokenized) > max_code_length:\n",
    "                cmd_tokenized = cmd_tokenized[:max_code_length-1] + cmd_tokenized[-1:]\n",
    "            self.items.append((text_tokenized, cmd_tokenized))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextToBashDataset(\n",
    "    texts=train_data[\"text_cleaned\"],\n",
    "    cmds=train_data[\"cmd_cleaned\"],\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)\n",
    "\n",
    "valid_ds = TextToBashDataset(\n",
    "    texts=valid_data[\"text_cleaned\"],\n",
    "    cmds=valid_data[\"cmd_cleaned\"],\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invocation      searches through the root filesystem (\"/\") for...\n",
      "cmd                          find / -name Chapter1 -type f -print\n",
      "text_cleaned    search root filesystem  file name chapter prin...\n",
      "cmd_cleaned                  find Path -name Regex -type f -print\n",
      "Name: 9843, dtype: object\n",
      "\n",
      "\n",
      "[1, 13, 76, 179, 4, 9, 890, 11, 162, 2]\n",
      "search root filesystem file name chapter print locat\n",
      "find Path -name Regex -type f -print\n"
     ]
    }
   ],
   "source": [
    "text_idx, cmd_idx = valid_ds[0]\n",
    "print(valid_data.iloc[0])\n",
    "print(\"\\n\")\n",
    "print(text_idx)\n",
    "print(text_tokenizer.decode(text_idx))\n",
    "print(cmd_tokenizer.decode(cmd_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    text_idxs = [torch.tensor(item[0]) for item in batch]\n",
    "    cmd_idxs = [torch.tensor(item[1]) for item in batch]\n",
    "    \n",
    "    text_idxs = pad_sequence(text_idxs, padding_value=PAD_ID, batch_first=True)\n",
    "    cmd_idxs = pad_sequence(cmd_idxs, padding_value=PAD_ID, batch_first=True)\n",
    "    \n",
    "    return text_idxs, cmd_idxs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train': DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn),\n",
    "    'valid': DataLoader(valid_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   1,   13,   76,  179,    4,    9,  890,   11,  162,    2,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   13,   76,  179,    4,    9,  890,    2,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   13,   76,  179,    4,    9,  890,    2,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   13,    4,   80,  183,    2,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   45, 1633,   42,  171,  158, 1125,    2,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   45,  789,   38,  350,  225, 3274, 2922,    6,    5,  109,   42,\n",
       "          3079,    2,    0,    0,    0],\n",
       "         [   1,   45,   29,  750,  223,  211,  823, 1200,   74,  481,  166,    2,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   45,   29,  310,    6,  145,  431, 2208,  481,  736,    2,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,   49,   35,  108,   40,   64,    6,   78,   77,   63,  235,\n",
       "            96,    2,    0,    0,    0],\n",
       "         [   1,   87,  421,   72,  294,  294, 2124,    4,    8,  361,    2,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    5,    6,    8,   62,  170,  165,    5,  494,  534,  254,\n",
       "             2,    0,    0,    0,    0],\n",
       "         [   1,   87,    5,    6,    8,   62,  170,   18,  494,  534,  254,    2,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    4,  187, 2767,   16,    4,   40,  614, 1731,    2,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    4,    6,    5,   96,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    4,    6,    5,   96,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,   17,  111,    4,    6,    8,   34,   32,   54,  152,   93,\n",
       "           126,   62,  452,  266,    2],\n",
       "         [   1,   87,  409,    4,    8,   36,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,  137, 1183,    4,    6,    8,   13,  165,    5,    2,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    5,    6,    8,    2,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    4,    6,    5,    2,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    4,    6,    8,   62,    5,   34,   32,    9,   65,  236,\n",
       "             2,    0,    0,    0,    0],\n",
       "         [   1,   87,    4,    6,    8,   68,  502,  211,    2,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    4,   71,   60,    4,   26,  152,   93, 1093,    2,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    4,   71,   60,    4,   26,  152,   93,  106,    2,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,    4,    8,  148,   20,   22,   90,    2,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,  183,    4,    8,   36,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,   17,    4,    6,    8,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,   17,  111,    4,    8, 1354,    2,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,  177,   70,   17,  111,    4,    6,    8,  152,   93,  106,\n",
       "             2,    0,    0,    0,    0],\n",
       "         [   1,   87,  177,   70,   17,  111,    4,    4,   26,  152,   93,  106,\n",
       "             2,    0,    0,    0,    0],\n",
       "         [   1,   87,   15,    4,    6,    8,   20,  120,   90,    2,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   87,  209,  177,   70,    4,  119,  267,   19,  100,   71,   60,\n",
       "             4,   26,    2,    0,    0]]),\n",
       " tensor([[1, 6, 8,  ..., 0, 0, 0],\n",
       "         [1, 6, 8,  ..., 0, 0, 0],\n",
       "         [1, 6, 8,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 6, 8,  ..., 0, 0, 0],\n",
       "         [1, 6, 8,  ..., 0, 0, 0],\n",
       "         [1, 6, 8,  ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loaders[\"valid\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение бейзлайна (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel, EncoderDecoderConfig, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание.** Реализуйте модель encoder-decoder ниже. В качестве моделей энкодера и декодера рекомендуется использовать BertModel из библиотеки transformers, заданную через BertConfig. В случае декодера необходимо выставить параметры is_decoder=True и add_cross_attention=True. В качестве модели, <<сцепляющей>> энкодер и декодер, в одну архитектуру рекомендуется использовать EncoderDecoderModel.\n",
    "\n",
    "**Обратите внимание!** EncoderDecoderModel поддерживает использование кэшированных результатов при последовательной генерации. Это пригодится при реализации beam-search ниже.\n",
    "\n",
    "Для того, чтобы удобнее задавать модели, рекомендуется реализовать задание модели через конфиг. Ниже представлены базовые параметры, при которых модель должна работать быстро и с приемлемым качеством."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "text_model_config = BertConfig(\n",
    "    vocab_size = text_tokenizer.vocab_size(),\n",
    "    hidden_size = 256,\n",
    "    num_hidden_layers = 2,\n",
    "    num_attention_heads = 8,\n",
    "    intermediate_size = 256 * 4,\n",
    "    hidden_dropout_prob = 0.1,\n",
    "    pad_token_id = PAD_ID,\n",
    ")\n",
    "\n",
    "cmd_model_config = BertConfig(\n",
    "    vocab_size = cmd_tokenizer.vocab_size(),\n",
    "    hidden_size = 256,\n",
    "    num_hidden_layers = 2,\n",
    "    num_attention_heads = 8,\n",
    "    intermediate_size = 256 * 4,\n",
    "    hidden_dropout_prob = 0.1,\n",
    "    pad_token_id = PAD_ID,\n",
    "    is_decoder = True,\n",
    "    add_cross_attention = True\n",
    ")\n",
    "\n",
    "print(cmd_model_config.is_decoder)\n",
    "print(cmd_model_config.add_cross_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(text_model_config, cmd_model_config)\n",
    "model = EncoderDecoderModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 17])\n",
      "torch.Size([32, 38])\n",
      "torch.Size([32, 38])\n"
     ]
    }
   ],
   "source": [
    "text_idxs, cmd_idxs = next(iter(loaders[\"valid\"]))\n",
    "decoder_input = cmd_idxs[..., :-1]\n",
    "target = cmd_idxs[..., 1:]\n",
    "\n",
    "print(text_idxs.shape)\n",
    "print(decoder_input.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idxs_mask = torch.where(text_idxs != PAD_ID, 1, 0)\n",
    "decoder_input_mask = torch.where(decoder_input != PAD_ID, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 6, 8,  ..., 0, 0, 0],\n",
       "        [1, 6, 8,  ..., 0, 0, 0],\n",
       "        [1, 6, 8,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 6, 8,  ..., 0, 0, 0],\n",
       "        [1, 6, 8,  ..., 0, 0, 0],\n",
       "        [1, 6, 8,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_idxs[..., :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 38, 500])\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "    attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "\n",
    "logits = out.logits\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(text_tokenized, model, max_len=20):\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(DEVICE)\n",
    "    text_tokenized = text_tokenized.unsqueeze(0).to(DEVICE)\n",
    "    cmd_prediction = torch.tensor([BOS_ID]).unsqueeze(0).to(DEVICE)\n",
    "    next_piece = BOS_ID\n",
    "    \n",
    "    while next_piece != EOS_ID and cmd_prediction.shape[-1] < max_len:\n",
    "        #print(cmd_prediction)\n",
    "        out = model(input_ids=text_tokenized, decoder_input_ids=cmd_prediction)\n",
    "        next_piece = torch.argmax(out.logits.squeeze(0)[-1]).item()\n",
    "        cmd_prediction = torch.cat((cmd_prediction, torch.tensor([[next_piece]]).to(DEVICE)), dim=1)\n",
    "        \n",
    "    return cmd_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_decode(i, data, ds, model, max_len=20):\n",
    "    \n",
    "    for k, v in dict(data.iloc[i]).items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    text_tokenized = torch.tensor(ds[i][0])\n",
    "    cmd_prediction = greedy_decode(text_tokenized, model, max_len)\n",
    "    cmd_prediction = [x.item() for x in cmd_prediction]\n",
    "    print(f\"cmd_predicted: {cmd_tokenizer.decode(cmd_prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_eval(data, ds, model, sample_size=5, max_len=20):\n",
    "    indexes = random.sample(range(len(ds)), sample_size)\n",
    "    for i in indexes:\n",
    "        eval_decode(i, data, ds, model, max_len)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eval_decode(3, train_data, train_ds, model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_eval(train_data, train_ds, model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_eval(valid_data, valid_ds, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Обучите вашу модель ниже.\n",
    "\n",
    "Рекомендуется:\n",
    "* в качестве лосса использовать стандартную кросс-энтропию, не забывайте игнорировать PAD токены\n",
    "* использовать Adam для оптимизации\n",
    "* не использовать scheduler для бейзлайна (модель легко переобучается с ним)\n",
    "* использовать early stopping по валидационному лоссу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.01):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "#from apex import amp\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            criterion,\n",
    "            optimizer, \n",
    "            pad_token_id,\n",
    "            device,\n",
    "            stopper,\n",
    "            logdir=None,\n",
    "            max_grad_norm=-1\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        self._criterion = criterion\n",
    "        self._optimizer = optimizer\n",
    "        self._pad_token_id = pad_token_id\n",
    "        self._device = device\n",
    "        self._logdir = logdir\n",
    "        self._stopper = stopper\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self._model = model.to(self._device)\n",
    "        \n",
    "        if self._logdir is not None:\n",
    "            self._writer = SummaryWriter(log_dir=f\"{logdir}/{datetime.now()}/\", flush_secs=1)\n",
    "\n",
    "        self._n_epoch = 0\n",
    "        self._n_iter = 0\n",
    "\n",
    "    def train(self, dataloaders, n_epochs):\n",
    "        for epoch in trange(n_epochs):\n",
    "            train_loss = self._train_step(dataloaders[\"train\"])\n",
    "            val_loss = self._val_step(dataloaders[\"valid\"])\n",
    "            self._n_epoch += 1\n",
    "            print(f\"Epoch: {self._n_epoch} | train_loss: {train_loss:.3f} | val_loss: {val_loss:.3f}\")\n",
    "            \n",
    "            if self._logdir is not None:\n",
    "                self._writer.add_scalar(\"loss/train\", train_loss, global_step=self._n_epoch)\n",
    "                self._writer.add_scalar(\"loss/val\", val_loss, global_step=self._n_epoch)\n",
    "                \n",
    "            self._stopper(val_loss)\n",
    "            torch.save(self._model.state_dict(), f\"./checkpoints/{val_loss}.pt\")\n",
    "            \n",
    "            trainer._stopper.best_loss\n",
    "            if self._stopper.early_stop:\n",
    "                break\n",
    "        \n",
    "        self._model.load_state_dict(torch.load(f\"./checkpoints/{self._stopper.best_loss}.pt\"))\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        self._model.train()\n",
    "        epoch_loss = 0\n",
    "        for text_idxs, cmd_idxs in dataloader:\n",
    "            \n",
    "            text_idxs = text_idxs.to(self._device)\n",
    "            decoder_input = cmd_idxs[..., :-1].to(self._device)\n",
    "            target = cmd_idxs[..., 1:].to(self._device)\n",
    "            \n",
    "            text_idxs_mask = torch.where(text_idxs != self._pad_token_id, 1, 0).to(self._device)\n",
    "            decoder_input_mask = torch.where(decoder_input != self._pad_token_id, 1, 0).to(self._device)\n",
    "            \n",
    "            self._optimizer.zero_grad()        \n",
    "            \n",
    "            out = self._model(\n",
    "                input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "                attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "\n",
    "            logits = out.logits\n",
    "\n",
    "            loss = self._criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if self._max_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n",
    "                \n",
    "            self._optimizer.step()\n",
    "        return epoch_loss / len(dataloader)\n",
    "    \n",
    "    def _val_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        self._model.eval()\n",
    "        epoch_loss = 0\n",
    "        for text_idxs, cmd_idxs in dataloader:\n",
    "            \n",
    "            text_idxs = text_idxs.to(self._device)\n",
    "            decoder_input = cmd_idxs[..., :-1].to(self._device)\n",
    "            target = cmd_idxs[..., 1:].to(self._device)\n",
    "            \n",
    "            text_idxs_mask = torch.where(text_idxs != self._pad_token_id, 1, 0).to(self._device)\n",
    "            decoder_input_mask = torch.where(decoder_input != self._pad_token_id, 1, 0).to(self._device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = self._model(\n",
    "                    input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "                    attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "                logits = out.logits\n",
    "                loss = self._criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(logits.shape)\n",
    "print(target.shape)\n",
    "criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "stopper = EarlyStopping(patience=5, min_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    pad_token_id=PAD_ID,\n",
    "    device=device,\n",
    "    logdir=\"./runs\",\n",
    "    stopper=stopper,\n",
    "    max_grad_norm=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ./checkpoints"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.train(loaders, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переучивать не будем:\n",
    "\n",
    "![](./training.png)\n",
    "\n",
    "Видим, что лучший лосс был на 8-й эпохе.\n",
    "Подгрузим эти веса (при переобучении модель автоматически в конце подгружает лучшие веса):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.93\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./checkpoints/0.9302761256694794.pt\"))\n",
    "trainer._model = model\n",
    "print(f\"val_loss: {round(trainer._val_step(loaders['valid']), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация команд (2 балла)\n",
    "\n",
    "**Задание**. Реализуйте алгоритм beam-search в классе BeamSearchGenerator ниже. Ваша реализация должна поддерживать задание температуры софтмакса. Выходы модели, полученные на предыдущих итерациях, необходимо кэшировать для повышения скорости алгоритма. Вместо подсчёта произведения любых вероятностей необходимо считать сумму их логарифмов.\n",
    "\n",
    "Алгоритм должен возвращать список пар из получившихся выходных последовательностей и логарифмов их вероятностей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchGenerator:\n",
    "    def __init__(\n",
    "            self, pad_id, eos_id, bos_id,\n",
    "            max_length=20, beam_width=5, temperature=1.5,\n",
    "            device=torch.device('cuda'),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        pad_id : int\n",
    "        eos_id : int\n",
    "        bos_id : int\n",
    "        max_length : int\n",
    "            Maximum length of output sequence\n",
    "        beam_width : int\n",
    "            Width of the beam\n",
    "        temperature : float\n",
    "            Softmax temperature\n",
    "        device : torch.device\n",
    "            Your model device\n",
    "        \"\"\"\n",
    "        self.pad_id = pad_id\n",
    "        self.eos_id = eos_id\n",
    "        self.bos_id = bos_id\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.beam_width = beam_width\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def get_result(self, model, input_text_tokens):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : TextToBashModel\n",
    "        input_text_tokens : torch.tensor\n",
    "            One object input tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        chains = torch.full([self.beam_width, 1], self.bos_id).to(self.device)\n",
    "        chain_probabilities = torch.zeros(self.beam_width).to(self.device)\n",
    "        if_chain_ready = torch.full([self.beam_width], False).to(self.device)\n",
    "        \n",
    "        # saving encoder outputs\n",
    "        input_text_tokens = input_text_tokens.repeat([self.beam_width, 1]).to(device)\n",
    "        encoder_outputs = model._prepare_encoder_decoder_kwargs_for_generation(\n",
    "            input_ids=input_text_tokens,\n",
    "            model_kwargs={})[\"encoder_outputs\"]\n",
    "        \n",
    "        idx = 0\n",
    "        while idx <= self.max_length and not all(if_chain_ready):\n",
    "            \n",
    "            logits = model(encoder_outputs=encoder_outputs, decoder_input_ids=chains).logits[:, -1, :]\n",
    "            \n",
    "            probs = torch.log_softmax(logits / self.temperature, dim=-1)\n",
    "            probs, tokens = torch.topk(probs, k=self.beam_width)\n",
    "            \n",
    "            if idx == 0:\n",
    "                # pick all top tokens\n",
    "                # 0 because current chains are same\n",
    "                beam_tokens = tokens[0].view(self.beam_width, 1)\n",
    "                chains = torch.cat([chains, beam_tokens], dim=-1)\n",
    "                chain_probabilities += probs[0]\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            chain_probabilities_beam = chain_probabilities.reshape(-1, 1).repeat([1, self.beam_width])\n",
    "            ready_chains_idxs = if_chain_ready.nonzero()\n",
    "            \n",
    "            # if chain is already ready,\n",
    "            # keep one instance of it and add\n",
    "            # padding\n",
    "            probs[ready_chains_idxs, :] = -float(\"inf\")\n",
    "            probs[ready_chains_idxs, 0] = 0\n",
    "            tokens[ready_chains_idxs, :] = self.pad_id\n",
    "\n",
    "            chain_probabilities_beam = chain_probabilities_beam + probs\n",
    "            \n",
    "            # choosing best across all options\n",
    "            best_sequences = torch.argsort(chain_probabilities_beam.flatten())[-self.beam_width:]\n",
    "\n",
    "            chains = torch.cat([chains[best_sequences // self.beam_width, :], tokens.flatten()[best_sequences].view(-1, 1)], dim=-1)\n",
    "            chain_probabilities = chain_probabilities_beam.flatten()[best_sequences]\n",
    "\n",
    "            if_chain_ready = ((chains[:, -1] == self.eos_id) | (chains[:, -1] == self.pad_id))\n",
    "                \n",
    "            idx += 1\n",
    "        \n",
    "        return list(zip(chains, chain_probabilities))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте на нескольких примерах работу вашего алгоритма. Если всё реализовано правильно, то как минимум на трёх примерах из 5 всё должно работать правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_engine = BeamSearchGenerator(\n",
    "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "    max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "    temperature=1, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text: searches through the root filesystem (\"/\") for the file named chapter1, and prints the location\n",
      "text cleaned: search root filesystem  file name chapter print locat\n",
      "true: find / -name Chapter1 -type f -print\n",
      "true cleaned: find Path -name Regex -type f -print\n",
      "find Path -name Regex -type f -printf \"%f\\n\" tensor(-4.4631, device='cuda:0')\n",
      "find Path -name Regex -or -name Regex tensor(-3.2608, device='cuda:0')\n",
      "find Path -name Regex -type f -print tensor(-2.9428, device='cuda:0')\n",
      "find Path -name Regex -type f tensor(-1.5682, device='cuda:0')\n",
      "find Path -name Regex tensor(-0.7397, device='cuda:0')\n",
      "1.0\n",
      "\n",
      "text: searches through the root filesystem (\"/\") for the file named chapter1.\n",
      "text cleaned: search root filesystem  file name chapter\n",
      "true: find / -name Chapter1 -type f\n",
      "true cleaned: find Path -name Regex -type f\n",
      "find Path -name Regex -or -name Regex -type f tensor(-4.5106, device='cuda:0')\n",
      "find Path -name Regex -or -name Regex tensor(-2.8590, device='cuda:0')\n",
      "find Path -name Regex -type f -print tensor(-2.0952, device='cuda:0')\n",
      "find Path -name Regex -type f tensor(-1.5648, device='cuda:0')\n",
      "find Path -name Regex tensor(-0.8799, device='cuda:0')\n",
      "1.0\n",
      "\n",
      "text: searches through the root filesystem (\"/\") for the file named chapter1.\n",
      "text cleaned: search root filesystem  file name chapter\n",
      "true: find / -name Chapter1 -type f -print\n",
      "true cleaned: find Path -name Regex -type f -print\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/furiousteabag/.local/share/virtualenvs/base/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find Path -name Regex -or -name Regex -type f tensor(-4.5106, device='cuda:0')\n",
      "find Path -name Regex -or -name Regex tensor(-2.8590, device='cuda:0')\n",
      "find Path -name Regex -type f -print tensor(-2.0952, device='cuda:0')\n",
      "find Path -name Regex -type f tensor(-1.5648, device='cuda:0')\n",
      "find Path -name Regex tensor(-0.8799, device='cuda:0')\n",
      "1.0\n",
      "\n",
      "text: searching for all files with the extension mp3\n",
      "text cleaned: search file extens mp\n",
      "true: find / -name *.mp3\n",
      "true cleaned: find Path -name Regex\n",
      "find Path -type f -iname Regex -print tensor(-4.9602, device='cuda:0')\n",
      "find Path -type f -iname Regex tensor(-2.0453, device='cuda:0')\n",
      "find Path -iname Regex tensor(-1.9404, device='cuda:0')\n",
      "find Path -name Regex tensor(-1.7023, device='cuda:0')\n",
      "find Path -type f -name Regex tensor(-1.6675, device='cuda:0')\n",
      "1.0\n",
      "\n",
      "text: set myvariable to the value of variable_name\n",
      "text cleaned: set myvari valu variablenam\n",
      "true: myVariable=$(env  | grep VARIABLE_NAME | grep -oe '[^=]*$');\n",
      "true cleaned: env | grep Regex | grep -o -e Regex\n",
      "echo Regex | cut -f Number -d Regex | rev tensor(-5.7752, device='cuda:0')\n",
      "echo Regex | cut -f Number tensor(-4.5846, device='cuda:0')\n",
      "echo Regex | cut -d Regex -f Number tensor(-4.2235, device='cuda:0')\n",
      "echo Regex | rename Regex tensor(-4.0193, device='cuda:0')\n",
      "echo Regex | tee File tensor(-2.1091, device='cuda:0')\n",
      "-1.0\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print()\n",
    "        print('text:', valid_data.invocation.iloc[i])\n",
    "        print('text cleaned:', valid_data.text_cleaned.iloc[i])\n",
    "        print('true:', valid_data.cmd.iloc[i])\n",
    "        print('true cleaned:', valid_data.cmd_cleaned.iloc[i])\n",
    "\n",
    "        src = torch.tensor(valid_ds[i][0])\n",
    "        pred = beam_search_engine.get_result(model, src)\n",
    "        \n",
    "        #print('greedy decode:', cmd_tokenizer.decode(list(map(int, greedy_decode(src, model)))))\n",
    "        scores = []\n",
    "        for x, proba in pred[:5]:\n",
    "            pred_cmd = cmd_tokenizer.decode(list(map(int, x)))\n",
    "            score = compute_metric(pred_cmd, 1, valid_data.cmd.iloc[i])\n",
    "            scores.append(score)\n",
    "            print(pred_cmd, proba)\n",
    "        print(max(scores))\n",
    "        all_scores.append(max(scores))\n",
    "        \n",
    "print(np.mean(all_scores))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i = 1\n",
    "print(valid_data.iloc[i])\n",
    "input_text_tokens = torch.tensor(valid_ds[i][0])\n",
    "out = beam_search_engine.get_result(model, input_text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Дополните функцию для подсчёта качества. Посчитайте качество вашей модели на валидационном и тестовых датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_scores(model, df, beam_engine):\n",
    "    all_scores = []\n",
    "\n",
    "    for i, (text, target_cmd) in tqdm(enumerate(zip(df.text_cleaned.values, df.cmd.values)), total=len(df)):\n",
    "        \n",
    "        input_tokens = text_tokenizer.tokenize(text, add_bos=True, add_eos=True)\n",
    "        if len(input_tokens) > MAX_TEXT_LENGTH:\n",
    "            input_tokens = input_tokens[:MAX_TEXT_LENGTH-1] + input_tokens[-1:]\n",
    "        input_tokens = torch.tensor(input_tokens)\n",
    "        \n",
    "        predictions = beam_engine.get_result(model, input_tokens)\n",
    "        \n",
    "        # get only 5 top results\n",
    "        predictions = predictions[:5]\n",
    "        object_scores = []\n",
    "        for output_tokens, proba in predictions:\n",
    "            output_cmd = cmd_tokenizer.decode(list(map(int, output_tokens)))\n",
    "            score = compute_metric(output_cmd, 1, target_cmd)\n",
    "            object_scores.append(score)\n",
    "        \n",
    "        all_scores.append(max(object_scores))\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы всё реализовали правильно, подобрали параметры BeamSearch то ваш средний скор на валидации должен быть >= 0.25, а скор на `handcrafted` части теста >= 0.13. На `mined` части датасета скор может быть низкий, т.к. некоторых команд из датасета нет в обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_temperature(model, valid_data):\n",
    "    scores = []\n",
    "    temperatures = np.arange(0.5, 2.1, 0.1)\n",
    "    for temperature in temperatures:\n",
    "        beam_search_engine = BeamSearchGenerator(\n",
    "            pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "            max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "            temperature=temperature, device=DEVICE\n",
    "        )\n",
    "        scores.append(np.mean(compute_all_scores(model, valid_data, beam_search_engine)))\n",
    "    return temperatures[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temperature = find_best_temperature(model, valid_data)\n",
    "print(temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./searching_temperature.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_engine = BeamSearchGenerator(\n",
    "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "    max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "    temperature=1.5, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26862499999999995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_scores = compute_all_scores(model, valid_data, beam_search_engine)\n",
    "print(np.mean(val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [00:11<00:00, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1491279069767442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores = compute_all_scores(model, test_data[test_data[\"origin\"] == \"handcrafted\"], beam_search_engine)\n",
    "print(np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 592/592 [00:49<00:00, 11.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.27850573788073785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mined_scores = compute_all_scores(model, test_data[test_data[\"origin\"] == \"mined\"], beam_search_engine)\n",
    "print(np.mean(mined_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение модели (4 балла)\n",
    "\n",
    "Вы реализовали бейзлайн, пришло время улучшить качество модели. Т.к. это последнее задание, мы не будем предлагать конкретные шаги, а только дадим несколько советов.\n",
    "\n",
    "1. Большой источник информации о работе командной строке — её документация, man. Один из способов улучшения модели - использование мана для генерации новых примеров. Структурированный ман можно найти по ссылке https://github.com/IBM/clai/blob/nlc2cmd/docs/manpage-data.md.\n",
    "2. Ещё один способ улучшить модель, разделить предсказание утилит и флагов. Т.к. задача предсказания утилит более важная, вы можете натренировать модель, которая предсказывает последовательность утилит, а затем к каждой утилите генерировать флаги.\n",
    "3. Можно аугментировать данные, чтобы увеличить выборку.\n",
    "4. Можно в качество входа подавать не только текстовый запрос, но и описание из мана. Т.к. всё описание достаточно большое, нужно сделать дополнительную модель, которая будет выбирать команды, для которых нужно вытащить описание.\n",
    "5. Найти дополнительные данные, улучшающие обучение\n",
    "6. Как всегда можно просто сделать больше слоёв, увеличить размер скрытого слоя и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От вас ожидается скор на `mined` >= 0 при скоре на `handrafted` >= 0.16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусные баллы (до 3 баллов)\n",
    "\n",
    "При существенном улучшении качества будут назначаться бонусные баллы. На тестовых датасетах реально выбить качество >= 0.3 на каждом, но усилий потребуется немало..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
