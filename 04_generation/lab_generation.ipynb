{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 4\n",
    "# Генерация bash команды по текстовому запросу\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "### ФИО: Смирнов Александр Львович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы построите систему, выдающую пользователю последовательность утилит командной строки linux (с нужными флагами) по его текстовому запросу. Вам дан набор пар текстовый запрос - команда на выходе. \n",
    "\n",
    "Решение этого задания будет построено на encoder-decoder архитектуре и модели transformer.\n",
    "\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    "* pytorch\n",
    "* transformers\n",
    "* sentencepiece (bpe токенизация)\n",
    "* clai utils (скачать с гитхаба отсюда https://github.com/IBM/clai/tree/nlc2cmd/utils) \n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "В качестве обучающей выборке используются данные, сгенерированные автоматически по запросам с сайта stack overflow. В качестве тестовых данных используются пары запросов, размеченные асессорами.\n",
    "\n",
    "Данные можно скачать по ссылке: https://drive.google.com/file/d/1n457AAgrMwd5VbT6mGZ_rws3g2wwdEfX/view?usp=sharing\n",
    "\n",
    "### Метрика качества\n",
    "\n",
    "Ваш алгоритм должен выдавать пять вариантов ответа для каждого запроса. \n",
    "Для упрощения задачи метрика качества будет учитывать утилиты и флаги ответа, но не учитывать подставленные значения. Пусть $\\{ u_1, \\ldots, u_T \\}$, $\\{ f_1, \\ldots, f_T \\}$ --- список утилит и множества их флагов ответа алгоритма, $\\{v_1, \\ldots, v_T \\}$, $\\{ \\phi_1, \\ldots, \\phi_T \\}$ --- список утилит и множества их флагов эталонного ответа. Если ответы отличаются по длине, они дополняются `None` утилитой. \n",
    "\n",
    "$$ S = \\frac{1}{T} \\sum_{i=1}^{T} \\left(\\mathbb{I}[u_i = v_i]\\left( 1 + \\frac{1}{2}s(f_i, \\phi_i)\\right) - 1\\right)$$\n",
    "\n",
    "$$ s(f, \\phi) = 1 + \\frac{2 |f \\cap \\phi| - |f \\cup \\phi|}{\\max(|f|, |\\phi|)} $$\n",
    "\n",
    "Метрика учитывает, что предсказать правильную утилиту важнее чем правильный флаг. При этом порядок флагов не важен (однако, чтобы корректно "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "PATH_TO_CLAI_UTILS = \"./utils/\"\n",
    "sys.path.append(PATH_TO_CLAI_UTILS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting bashlex grammar using file: /home/furiousteabag/Projects/NLP/04_generation/./utils/bashlint/grammar/grammar100.txt\n",
      "Bashlint grammar set up (148 utilities)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bashlint.data_tools import bash_parser, pretty_print, cmd2template\n",
    "from metric.metric_utils import compute_metric\n",
    "from functools import partial\n",
    "\n",
    "from collections import Counter\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем данные. В столбце `invocation` находится текстовый запрос, в столбце `cmd` находится релевантная команда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>copy loadable kernel module \"mymodule.ko\" to t...</td>\n",
       "      <td>sudo cp mymodule.ko /lib/modules/$(uname -r)/k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>display all lines containing \"ip_mroute\" in th...</td>\n",
       "      <td>cat /boot/config-`uname -r` | grep IP_MROUTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>display current running kernel's compile-time ...</td>\n",
       "      <td>cat /boot/config-`uname -r`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find all loadable modules for current kernel, ...</td>\n",
       "      <td>find /lib/modules/`uname -r` -regex .*perf.*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look for any instance of \"highmem\" in the curr...</td>\n",
       "      <td>grep “HIGHMEM” /boot/config-`uname -r`</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          invocation  \\\n",
       "0  copy loadable kernel module \"mymodule.ko\" to t...   \n",
       "1  display all lines containing \"ip_mroute\" in th...   \n",
       "2  display current running kernel's compile-time ...   \n",
       "3  find all loadable modules for current kernel, ...   \n",
       "4  look for any instance of \"highmem\" in the curr...   \n",
       "\n",
       "                                                 cmd  \n",
       "0  sudo cp mymodule.ko /lib/modules/$(uname -r)/k...  \n",
       "1       cat /boot/config-`uname -r` | grep IP_MROUTE  \n",
       "2                        cat /boot/config-`uname -r`  \n",
       "3       find /lib/modules/`uname -r` -regex .*perf.*  \n",
       "4             grep “HIGHMEM” /boot/config-`uname -r`  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тестовых данных столбец `origin` отвечает за источник данных, значения `handrafted` соответствуют парам, составленными людьми, а `mined` парам, собранным автоматически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>create ssh connection to specified ip from spe...</td>\n",
       "      <td>ssh user123@176.0.13.154</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>search for commands containing string \"zeppeli...</td>\n",
       "      <td>history | grep zeppelin</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>search for location of specified file or appli...</td>\n",
       "      <td>whereis python3</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grant all rights to root folder</td>\n",
       "      <td>sudo chmod 777 -R /</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>search in running processes for specified name</td>\n",
       "      <td>ps -aux | grep zepp</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          invocation  \\\n",
       "0  create ssh connection to specified ip from spe...   \n",
       "1  search for commands containing string \"zeppeli...   \n",
       "2  search for location of specified file or appli...   \n",
       "3                    grant all rights to root folder   \n",
       "4     search in running processes for specified name   \n",
       "\n",
       "                        cmd       origin  \n",
       "0  ssh user123@176.0.13.154  handcrafted  \n",
       "1   history | grep zeppelin  handcrafted  \n",
       "2           whereis python3  handcrafted  \n",
       "3       sudo chmod 777 -R /  handcrafted  \n",
       "4       ps -aux | grep zepp  handcrafted  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Проведите предобработку текста. Рекомендуется:\n",
    "* перевести всё в нижний регистр\n",
    "* удалить стоп-слова (специфичные для выборки)\n",
    "* провести стемминг токенов\n",
    "* удалить все символы кроме латинских букв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'the', 'in', 'current', 'all', 'file', 'directori'}\n"
     ]
    }
   ],
   "source": [
    "# looking up for specific stop words\n",
    "tfidf = TfidfVectorizer(max_df=0.3, min_df=1)\n",
    "tfidf.fit(train_data[\"invocation\"].apply(lambda x: \" \".join([stemmer.stem(t) for t in x.split()])))\n",
    "specific_stop_words = tfidf.stop_words_\n",
    "print(specific_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(token):\n",
    "    token = token.lower()\n",
    "    token = regex.sub(r'[^\\p{Latin}]', '', token)\n",
    "    token = stemmer.stem(token)\n",
    "    return token if token not in specific_stop_words else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return \" \".join([clean_token(token) for token in tokenizer.tokenize(text) if clean_token(token) != \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_cleaned'] = train_data['invocation'].apply(clean_text)\n",
    "test_data['text_cleaned'] = test_data['invocation'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки кода воспользуемся функцией `cmd2template`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cmd_cleaned'] = train_data['cmd'].apply(partial(cmd2template, loose_constraints=True))\n",
    "test_data['cmd_cleaned'] = test_data['cmd'].apply(partial(cmd2template, loose_constraints=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>cmd_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>copy loadable kernel module \"mymodule.ko\" to t...</td>\n",
       "      <td>sudo cp mymodule.ko /lib/modules/$(uname -r)/k...</td>\n",
       "      <td>copi loadabl kernel modul mymodul ko to driver...</td>\n",
       "      <td>cp File $( uname -r )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>display all lines containing \"ip_mroute\" in th...</td>\n",
       "      <td>cat /boot/config-`uname -r` | grep IP_MROUTE</td>\n",
       "      <td>display line contain ipmrout kernel s compil t...</td>\n",
       "      <td>cat $( uname -r ) | grep Regex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>display current running kernel's compile-time ...</td>\n",
       "      <td>cat /boot/config-`uname -r`</td>\n",
       "      <td>display run kernel s compil time config</td>\n",
       "      <td>cat $( uname -r )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find all loadable modules for current kernel, ...</td>\n",
       "      <td>find /lib/modules/`uname -r` -regex .*perf.*</td>\n",
       "      <td>find loadabl modul for kernel whose name inclu...</td>\n",
       "      <td>find Path $( uname -r ) -regex Regex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look for any instance of \"highmem\" in the curr...</td>\n",
       "      <td>grep “HIGHMEM” /boot/config-`uname -r`</td>\n",
       "      <td>look for ani instanc of highmem kernel s compi...</td>\n",
       "      <td>grep Regex $( uname -r )</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          invocation  \\\n",
       "0  copy loadable kernel module \"mymodule.ko\" to t...   \n",
       "1  display all lines containing \"ip_mroute\" in th...   \n",
       "2  display current running kernel's compile-time ...   \n",
       "3  find all loadable modules for current kernel, ...   \n",
       "4  look for any instance of \"highmem\" in the curr...   \n",
       "\n",
       "                                                 cmd  \\\n",
       "0  sudo cp mymodule.ko /lib/modules/$(uname -r)/k...   \n",
       "1       cat /boot/config-`uname -r` | grep IP_MROUTE   \n",
       "2                        cat /boot/config-`uname -r`   \n",
       "3       find /lib/modules/`uname -r` -regex .*perf.*   \n",
       "4             grep “HIGHMEM” /boot/config-`uname -r`   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  copi loadabl kernel modul mymodul ko to driver...   \n",
       "1  display line contain ipmrout kernel s compil t...   \n",
       "2            display run kernel s compil time config   \n",
       "3  find loadabl modul for kernel whose name inclu...   \n",
       "4  look for ani instanc of highmem kernel s compi...   \n",
       "\n",
       "                            cmd_cleaned  \n",
       "0                 cp File $( uname -r )  \n",
       "1        cat $( uname -r ) | grep Regex  \n",
       "2                     cat $( uname -r )  \n",
       "3  find Path $( uname -r ) -regex Regex  \n",
       "4              grep Regex $( uname -r )  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучение и валидацию. Т.к. данных очень мало, то для валидационной выборки выделим только 100 примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = train_data.iloc[-100:]\n",
    "train_data = train_data.iloc[:-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Стандартный формат входных данных для трансформеров — BPE токены. Воспользуйтесь библиотекой sentencepiece для обучения токенайзеров для текста и кода. Используйте небольшое число токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "\n",
    "MAX_TEXT_LENGTH = 256\n",
    "MAX_CODE_LENGTH = 40\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mkdir -p ./tokenizers\n",
    "\n",
    "text_tokenizer = io.BytesIO()\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(train_data[\"text_cleaned\"]),\n",
    "    model_writer=text_tokenizer,\n",
    "    #model_prefix='./tokenizers/sp_text',\n",
    "    vocab_size=2400,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")\n",
    "text_tokenizer = spm.SentencePieceProcessor(model_proto=text_tokenizer.getvalue())\n",
    "\n",
    "cmd_tokenizer = io.BytesIO()\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=iter(train_data[\"cmd_cleaned\"]),\n",
    "    model_writer=cmd_tokenizer,\n",
    "    #model_prefix='./tokenizers/sp_cmd',\n",
    "    vocab_size=500,\n",
    "    pad_id=PAD_ID,                \n",
    "    bos_id=BOS_ID,\n",
    "    eos_id=EOS_ID,\n",
    "    unk_id=UNK_ID\n",
    ")\n",
    "cmd_tokenizer = spm.SentencePieceProcessor(model_proto=cmd_tokenizer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text:\n",
      "copi loadabl kernel modul mymodul ko to driver modul matchig kernel\n",
      "\n",
      "\n",
      "Indexes:\n",
      "[1, 96, 11, 1442, 158, 567, 447, 20, 509, 473, 200, 618, 473, 343, 79, 7, 820, 42, 509, 473, 61, 11, 135, 567, 447, 20, 2]\n",
      "\n",
      "\n",
      "Decoded version:\n",
      "copi loadabl kernel modul mymodul ko to driver modul matchig kernel\n",
      "\n",
      "\n",
      "Pieces:\n",
      "<s> ▁cop i ▁load abl ▁ker ne l ▁mod ul ▁my mod ul ▁k o ▁to ▁drive r ▁mod ul ▁match i g ▁ker ne l </s>\n"
     ]
    }
   ],
   "source": [
    "text = train_data[\"text_cleaned\"][0]\n",
    "print(\"Initial text:\")\n",
    "print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "tokenized = text_tokenizer.tokenize(text, add_bos=True, add_eos=True)\n",
    "print(\"Indexes:\")\n",
    "print(tokenized)\n",
    "print(\"\\n\")\n",
    "print(\"Decoded version:\")\n",
    "print(text_tokenizer.decode(tokenized))\n",
    "print(\"\\n\")\n",
    "print(\"Pieces:\")\n",
    "print(*text_tokenizer.IdToPiece(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cmd:\n",
      "cp File $( uname -r )\n",
      "\n",
      "\n",
      "Indexes:\n",
      "[1, 101, 4, 15, 4, 66, 41, 4, 18, 13, 5, 53, 4, 42, 2]\n",
      "\n",
      "\n",
      "Decoded version:\n",
      "cp File $( uname -r )\n",
      "\n",
      "\n",
      "Pieces:\n",
      "<s> ▁cp ▁ File ▁ $ ( ▁ u name ▁- r ▁ ) </s>\n"
     ]
    }
   ],
   "source": [
    "cmd = train_data[\"cmd_cleaned\"][0]\n",
    "print(\"Initial cmd:\")\n",
    "print(cmd)\n",
    "print(\"\\n\")\n",
    "\n",
    "tokenized = cmd_tokenizer.tokenize(cmd, add_bos=True, add_eos=True)\n",
    "print(\"Indexes:\")\n",
    "print(tokenized)\n",
    "print(\"\\n\")\n",
    "print(\"Decoded version:\")\n",
    "print(cmd_tokenizer.decode(tokenized))\n",
    "print(\"\\n\")\n",
    "print(\"Pieces:\")\n",
    "print(*cmd_tokenizer.IdToPiece(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Задайте датасеты и лоадеры для ваших данных. Каждая последовательность должна начинаться с BOS токена и заканчиваться EOS токеном. Рекомендуется ограничить длину входных и выходных последовательностей!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToBashDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, cmds, text_tokenizer, cmd_tokenizer,\n",
    "                 max_text_length=MAX_TEXT_LENGTH, max_code_length=MAX_CODE_LENGTH):\n",
    "        \n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.cmd_tokenizer = cmd_tokenizer\n",
    "        self.max_text_length = max_text_length\n",
    "        self.max_code_length = max_code_length\n",
    "        \n",
    "        self.items = []\n",
    "        \n",
    "        for text, cmd in zip(texts, cmds):\n",
    "            text_tokenized = text_tokenizer.tokenize(text, add_bos=True, add_eos=True)\n",
    "            cmd_tokenized = cmd_tokenizer.tokenize(cmd, add_bos=True, add_eos=True)\n",
    "            if len(text_tokenized) > max_text_length:\n",
    "                text_tokenized = text_tokenized[:max_text_length-1] + text_tokenized[-1:]\n",
    "            if len(cmd_tokenized) > max_code_length:\n",
    "                cmd_tokenized = cmd_tokenized[:max_code_length-1] + cmd_tokenized[-1:]\n",
    "            self.items.append((text_tokenized, cmd_tokenized))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextToBashDataset(\n",
    "    texts=train_data[\"text_cleaned\"],\n",
    "    cmds=train_data[\"cmd_cleaned\"],\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)\n",
    "\n",
    "valid_ds = TextToBashDataset(\n",
    "    texts=valid_data[\"text_cleaned\"],\n",
    "    cmds=valid_data[\"cmd_cleaned\"],\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    cmd_tokenizer=cmd_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invocation      searches through the root filesystem (\"/\") for...\n",
      "cmd                          find / -name Chapter1 -type f -print\n",
      "text_cleaned    search through root filesystem for name chapte...\n",
      "cmd_cleaned                  find Path -name Regex -type f -print\n",
      "Name: 9843, dtype: object\n",
      "\n",
      "\n",
      "[1, 18, 422, 125, 264, 312, 13, 15, 1051, 148, 192, 17, 254, 2]\n",
      "search through root filesystem for name chapter print locat\n",
      "find Path -name Regex -type f -print\n"
     ]
    }
   ],
   "source": [
    "text_idx, cmd_idx = valid_ds[0]\n",
    "print(valid_data.iloc[0])\n",
    "print(\"\\n\")\n",
    "print(text_idx)\n",
    "print(text_tokenizer.decode(text_idx))\n",
    "print(cmd_tokenizer.decode(cmd_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    text_idxs = [torch.tensor(item[0]) for item in batch]\n",
    "    cmd_idxs = [torch.tensor(item[1]) for item in batch]\n",
    "    \n",
    "    text_idxs = pad_sequence(text_idxs, padding_value=PAD_ID, batch_first=True)\n",
    "    cmd_idxs = pad_sequence(cmd_idxs, padding_value=PAD_ID, batch_first=True)\n",
    "    \n",
    "    return text_idxs, cmd_idxs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train': DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn),\n",
    "    'valid': DataLoader(valid_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1,  18, 422,  ...,   0,   0,   0],\n",
       "         [  1,  18, 422,  ...,   0,   0,   0],\n",
       "         [  1,  18, 422,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [  1, 307,  23,  ...,   0,   0,   0],\n",
       "         [  1, 307, 181,  ...,   0,   0,   0],\n",
       "         [  1, 307, 181,  ...,   0,   0,   0]]),\n",
       " tensor([[ 1,  9, 14,  ...,  0,  0,  0],\n",
       "         [ 1,  9, 14,  ...,  0,  0,  0],\n",
       "         [ 1,  9, 14,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 1,  4, 62,  ...,  0,  0,  0],\n",
       "         [ 1, 64, 11,  ...,  0,  0,  0],\n",
       "         [ 1, 64, 11,  ...,  0,  0,  0]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loaders[\"valid\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение бейзлайна (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel, EncoderDecoderConfig, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание.** Реализуйте модель encoder-decoder ниже. В качестве моделей энкодера и декодера рекомендуется использовать BertModel из библиотеки transformers, заданную через BertConfig. В случае декодера необходимо выставить параметры is_decoder=True и add_cross_attention=True. В качестве модели, <<сцепляющей>> энкодер и декодер, в одну архитектуру рекомендуется использовать EncoderDecoderModel.\n",
    "\n",
    "**Обратите внимание!** EncoderDecoderModel поддерживает использование кэшированных результатов при последовательной генерации. Это пригодится при реализации beam-search ниже.\n",
    "\n",
    "Для того, чтобы удобнее задавать модели, рекомендуется реализовать задание модели через конфиг. Ниже представлены базовые параметры, при которых модель должна работать быстро и с приемлемым качеством."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "text_model_config = BertConfig(\n",
    "    vocab_size = text_tokenizer.vocab_size(),\n",
    "    hidden_size = 256,\n",
    "    num_hidden_layers = 2,\n",
    "    num_attention_heads = 8,\n",
    "    intermediate_size = 256 * 4,\n",
    "    hidden_dropout_prob = 0.1,\n",
    "    pad_token_id = PAD_ID,\n",
    ")\n",
    "\n",
    "cmd_model_config = BertConfig(\n",
    "    vocab_size = cmd_tokenizer.vocab_size(),\n",
    "    hidden_size = 256,\n",
    "    num_hidden_layers = 2,\n",
    "    num_attention_heads = 8,\n",
    "    intermediate_size = 256 * 4,\n",
    "    hidden_dropout_prob = 0.1,\n",
    "    pad_token_id = PAD_ID,\n",
    "    is_decoder = True,\n",
    "    add_cross_attention = True\n",
    ")\n",
    "\n",
    "print(cmd_model_config.is_decoder)\n",
    "print(cmd_model_config.add_cross_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(text_model_config, cmd_model_config)\n",
    "model = EncoderDecoderModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 26])\n",
      "torch.Size([64, 39])\n",
      "torch.Size([64, 39])\n"
     ]
    }
   ],
   "source": [
    "text_idxs, cmd_idxs = next(iter(loaders[\"valid\"]))\n",
    "decoder_input = cmd_idxs[..., :-1]\n",
    "target = cmd_idxs[..., 1:]\n",
    "\n",
    "print(text_idxs.shape)\n",
    "print(decoder_input.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idxs_mask = torch.where(text_idxs != PAD_ID, 1, 0)\n",
    "decoder_input_mask = torch.where(decoder_input != PAD_ID, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  9, 14,  ...,  0,  0,  0],\n",
       "        [ 1,  9, 14,  ...,  0,  0,  0],\n",
       "        [ 1,  9, 14,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 1,  4, 62,  ...,  0,  0,  0],\n",
       "        [ 1, 64, 11,  ...,  0,  0,  0],\n",
       "        [ 1, 64, 11,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_idxs[..., :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 39, 500])\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "    attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "\n",
    "logits = out.logits\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(text_tokenized, model, max_len=20):\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    text_tokenized = text_tokenized.unsqueeze(0).to(device)\n",
    "    cmd_prediction = torch.tensor([BOS_ID]).unsqueeze(0).to(device)\n",
    "    next_piece = BOS_ID\n",
    "    \n",
    "    while next_piece != EOS_ID and cmd_prediction.shape[-1] < max_len:\n",
    "        out = model(input_ids=text_tokenized, decoder_input_ids=cmd_prediction)\n",
    "        next_piece = torch.argmax(out.logits.squeeze(0)[-1]).item()\n",
    "        cmd_prediction = torch.cat((cmd_prediction, torch.tensor([[next_piece]])), dim=1)\n",
    "        \n",
    "    return cmd_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_decode(i, data, ds, model, max_len=20):\n",
    "    \n",
    "    for k, v in dict(data.iloc[i]).items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    text_tokenized = torch.tensor(ds[i][0])\n",
    "    cmd_prediction = greedy_decode(text_tokenized, model, max_len)\n",
    "    cmd_prediction = [x.item() for x in cmd_prediction]\n",
    "    print(f\"cmd_predicted: {cmd_tokenizer.decode(cmd_prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_eval(data, ds, model, sample_size=5, max_len=20):\n",
    "    indexes = random.sample(range(len(ds)), sample_size)\n",
    "    for i in indexes:\n",
    "        eval_decode(i, data, ds, model, max_len)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invocation: find all loadable modules for current kernel, whose name includes \"perf\"\n",
      "cmd: find /lib/modules/`uname -r` -regex .*perf.*\n",
      "text_cleaned: find loadabl modul for kernel whose name includ perf\n",
      "cmd_cleaned: find Path $( uname -r ) -regex Regex\n",
      "cmd_predicted: cptpt{ commsTYngfin wpan ciceyCheckingskipgeoutilepan\n"
     ]
    }
   ],
   "source": [
    "eval_decode(3, train_data, train_ds, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invocation: gets ip address of 'en0' selected network interface.\n",
      "cmd: ifconfig en0 | awk '/inet addr/ {gsub(\"addr:\", \"\", $2); print $2}'\n",
      "text_cleaned: get ip address of en select network interfac\n",
      "cmd_cleaned: ifconfig Regex | awk Program\n",
      "cmd_predicted: cptpt{ commskipiletor-1mountemp ciceyCheckingskipgeouticct\n",
      "\n",
      "invocation: change all directories under \"./storage/\" to owner \"apache\" and group \"laravel\"\n",
      "cmd: sudo find ./storage/ -type d -exec chown apache:laravel {} \\;\n",
      "text_cleaned: chang under storag to owner apach group laravel\n",
      "cmd_cleaned: find Path -type d -exec chown Regex {} \\;\n",
      "cmd_predicted: cptpt{ commsTYngfin wpan ciceyCheckingskipgeoutilepan\n",
      "\n",
      "invocation: calculate the md5 sum of all files in \"/your/dir\" including content and filenames\n",
      "cmd: grep -ar -e . /your/dir | md5sum | cut -c-32\n",
      "text_cleaned: calcul md sum of your dir includ content filenam\n",
      "cmd_cleaned: grep -a -r -e Regex Regex | md5sum | cut -c -Number\n",
      "cmd_predicted: cptpt{ commsTYngfinpruneout ciceyCheckingskipgeoutilepan\n",
      "\n",
      "invocation: find suspicious php files\n",
      "cmd: find . -type f -name \"*.php\" -exec grep --with-filename \"eval(\\|exec(\\|base64_decode(\" {} \\;\n",
      "text_cleaned: find suspici php\n",
      "cmd_cleaned: find Path -type f -name Regex -exec grep --with-filename Regex {} \\;\n",
      "cmd_predicted: cptpt{ commskip \"\\{ commsTYir\\icemp c +% cfiles\n",
      "\n",
      "invocation: search the directory $path recursively for regular files with the given $extension\n",
      "cmd: find $path -type f -name \"*.$extension\"\n",
      "text_cleaned: search path recurs for regular with given extens\n",
      "cmd_cleaned: find Path -type f -name Regex\n",
      "cmd_predicted: cptpt{ commsTYngio 1.1,1.2,1.3,s ciceyCheckingskipgeoutilepan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_eval(train_data, train_ds, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invocation: this find command substitute space with underscore in the file name replaces space in all the *.mp3 files with _\n",
      "cmd: find . -type f -iname '*.mp3' -exec rename '/ /_/'  {} \\;\n",
      "text_cleaned: thi find command substitut space with underscor name replac space mp with\n",
      "cmd_cleaned: find Path -type f -iname Regex -exec rename Regex {} \\;\n",
      "cmd_predicted: cptpt{ commsTYngfin wpan ciceyCheckingskipgeouticct\n",
      "\n",
      "invocation: show the sum of disk used by all the files that belong to the user \"test1\" in the entire file system\n",
      "cmd: find / -user test1 -exec du -sm {} \\;|awk '{s+=$1}END{print s}'\n",
      "text_cleaned: show sum of disk use by that belong to user test entir system\n",
      "cmd_cleaned: find Path -user Regex -exec du -s -m {} \\; | awk Program\n",
      "cmd_predicted: cptpt{ commsTYngfin wpan ciceyCheckingskipgeoutilepan\n",
      "\n",
      "invocation: test if the 2nd column in file a is in the reverse order of the second column of file b\n",
      "cmd: diff --brief <(awk '{print $2}' A) <(tac B | awk '{print $2}')\n",
      "text_cleaned: test if nd column a is revers order of second column of b\n",
      "cmd_cleaned: diff --brief <( awk Program Program ) <( tac File | awk Program )\n",
      "cmd_predicted: cptpt{ commskip \"\\{ commsTYir\\icemp c +% cfiles\n",
      "\n",
      "invocation: split the file \"file\" into pieces per 2 lines\n",
      "cmd: split -n2 infile\n",
      "text_cleaned: split into piec per line\n",
      "cmd_cleaned: split -n Quantity File\n",
      "cmd_predicted: cptpt{ commskip \"\\{ commsTYir\\icemp c +% cfiles\n",
      "\n",
      "invocation: ssh into \"hostname\" as user \"buck\"\n",
      "cmd: ssh -l buck hostname\n",
      "text_cleaned: ssh into hostnam as user buck\n",
      "cmd_cleaned: ssh -l Regex Regex\n",
      "cmd_predicted: cptpt{ commskipiletor-1mountemp ciceyCheckingskipgeouticct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_eval(valid_data, valid_ds, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Обучите вашу модель ниже.\n",
    "\n",
    "Рекомендуется:\n",
    "* в качестве лосса использовать стандартную кросс-энтропию, не забывайте игнорировать PAD токены\n",
    "* использовать Adam для оптимизации\n",
    "* не использовать scheduler для бейзлайна (модель легко переобучается с ним)\n",
    "* использовать early stopping по валидационному лоссу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.01):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "#from apex import amp\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            criterion,\n",
    "            optimizer, \n",
    "            pad_token_id,\n",
    "            device,\n",
    "            stopper,\n",
    "            logdir=None,\n",
    "            max_grad_norm=-1\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        self._criterion = criterion\n",
    "        self._optimizer = optimizer\n",
    "        self._pad_token_id = pad_token_id\n",
    "        self._device = device\n",
    "        self._logdir = logdir\n",
    "        self._stopper = stopper\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self._model = model.to(self._device)\n",
    "        \n",
    "        if self._logdir is not None:\n",
    "            self._writer = SummaryWriter(log_dir=f\"{logdir}/{datetime.now()}/\", flush_secs=1)\n",
    "\n",
    "        self._n_epoch = 0\n",
    "        self._n_iter = 0\n",
    "\n",
    "    def train(self, dataloaders, n_epochs):\n",
    "        for epoch in trange(n_epochs):\n",
    "            train_loss = self._train_step(dataloaders[\"train\"])\n",
    "            val_loss = self._val_step(dataloaders[\"valid\"])\n",
    "            self._n_epoch += 1\n",
    "            print(f\"Epoch: {self._n_epoch} | train_loss: {train_loss:.3f} | val_loss: {val_loss:.3f}\")\n",
    "            \n",
    "            if self._logdir is not None:\n",
    "                self._writer.add_scalar(\"loss/train\", train_loss, global_step=self._n_epoch)\n",
    "                self._writer.add_scalar(\"loss/val\", val_loss, global_step=self._n_epoch)\n",
    "                \n",
    "            self._stopper(val_loss)\n",
    "            if self._stopper.early_stop:\n",
    "                break\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        self._model.train()\n",
    "        epoch_loss = 0\n",
    "        for text_idxs, cmd_idxs in dataloader:\n",
    "            \n",
    "            text_idxs = text_idxs.to(self._device)\n",
    "            decoder_input = cmd_idxs[..., :-1].to(self._device)\n",
    "            target = cmd_idxs[..., 1:].to(self._device)\n",
    "            \n",
    "            text_idxs_mask = torch.where(text_idxs != self._pad_token_id, 1, 0).to(self._device)\n",
    "            decoder_input_mask = torch.where(decoder_input != self._pad_token_id, 1, 0).to(self._device)\n",
    "            \n",
    "            self._optimizer.zero_grad()        \n",
    "            \n",
    "            out = self._model(\n",
    "                input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "                attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "\n",
    "            logits = out.logits\n",
    "\n",
    "            loss = self._criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if self._max_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n",
    "                \n",
    "            self._optimizer.step()\n",
    "        return epoch_loss / len(dataloader)\n",
    "    \n",
    "    def _val_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        self._model.eval()\n",
    "        epoch_loss = 0\n",
    "        for text_idxs, cmd_idxs in dataloader:\n",
    "            \n",
    "            text_idxs = text_idxs.to(self._device)\n",
    "            decoder_input = cmd_idxs[..., :-1].to(self._device)\n",
    "            target = cmd_idxs[..., 1:].to(self._device)\n",
    "            \n",
    "            text_idxs_mask = torch.where(text_idxs != self._pad_token_id, 1, 0).to(self._device)\n",
    "            decoder_input_mask = torch.where(decoder_input != self._pad_token_id, 1, 0).to(self._device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = self._model(\n",
    "                    input_ids=text_idxs, decoder_input_ids=decoder_input,\n",
    "                    attention_mask=text_idxs_mask, decoder_attention_mask=decoder_input_mask)\n",
    "                logits = out.logits\n",
    "                loss = self._criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(logits.shape)\n",
    "print(target.shape)\n",
    "criterion(logits.reshape(-1, cmd_tokenizer.vocab_size()), target.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "stopper = EarlyStopping(patience=2, min_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    pad_token_id=PAD_ID,\n",
    "    device=device,\n",
    "    logdir=\"./runs\",\n",
    "    stopper=stopper,\n",
    "    max_grad_norm=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:13<04:16, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.603 | val_loss: 1.237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:26<04:01, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.784 | val_loss: 0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:40<03:48, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.626 | val_loss: 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:53<03:33, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.545 | val_loss: 0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [01:07<03:21, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.483 | val_loss: 0.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [01:20<03:06, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 0.434 | val_loss: 0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [01:33<02:53, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss: 0.397 | val_loss: 0.845\n",
      "INFO: Early stopping counter 1 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [01:46<03:17, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | train_loss: 0.364 | val_loss: 0.835\n",
      "INFO: Early stopping counter 2 of 2\n",
      "INFO: Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train(loaders, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты обучения на тренировочной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invocation: display a long listing of the oldest file under '/hom/backups' directory tree\n",
      "cmd: find /home/backups -printf \"%T@ %p\\n\" | sort -n | head -1 | cut -d\" \" -f2- | xargs ls -al\n",
      "text_cleaned: display a long list of oldest under hom backup tree\n",
      "cmd_cleaned: find Path -printf \"%T@ %p\\n\" | sort -n | head - Quantity | cut -d Regex -f Number | xargs -I {} ls -a -l {}\n",
      "cmd_predicted: find Path -printf \"%T@ %p\\n\" | sort\n",
      "\n",
      "invocation: calculate the md5 sum of \"password\"\n",
      "cmd: echo \"password\" | md5sum\n",
      "text_cleaned: calcul md sum of password\n",
      "cmd_cleaned: echo Regex | md5sum\n",
      "cmd_predicted: md5sum File\n",
      "\n",
      "invocation: run ls command on *.pl files\n",
      "cmd: find . -name \"*.pl\" -exec ls -ld {} \\;\n",
      "text_cleaned: run ls command on pl\n",
      "cmd_cleaned: find Path -name Regex -exec ls -l -d {} \\;\n",
      "cmd_predicted: ls -l | xargs -I {} ls -l -d {}\n",
      "\n",
      "invocation: display all the files in the home folder which begin with \"arrow\"\n",
      "cmd: find ~ -name 'arrow*'\n",
      "text_cleaned: display home folder which begin with arrow\n",
      "cmd_cleaned: find Path -name Regex\n",
      "cmd_predicted: find Path -name Regex\n",
      "\n",
      "invocation: list \".java\" files that have the same contents\n",
      "cmd: md5sum *.java | sort | uniq -d -w32\n",
      "text_cleaned: list java that have same content\n",
      "cmd_cleaned: md5sum File | sort | uniq -d -w Quantity\n",
      "cmd_predicted: md5sum <( sort File ) <( sort\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_eval(train_data, train_ds, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты на тестовой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invocation: ssh into \"ssh.myhost.net\" as user \"myusername\" and run command \"mkdir -p $2\"\n",
      "cmd: ssh myusername@ssh.myhost.net \"mkdir -p $2\"\n",
      "text_cleaned: ssh into ssh myhost net as user myusernam run command mkdir p\n",
      "cmd_cleaned: ssh Regex command\n",
      "cmd_predicted: ssh Regex command\n",
      "\n",
      "invocation: split listing of the current directory into pieces per 500 lines named \"outputxyznnn\"\n",
      "cmd: ls | split -l 500 - outputXYZ.\n",
      "text_cleaned: split list of into piec per line name outputxyznnn\n",
      "cmd_cleaned: ls | split -l Quantity File Regex\n",
      "cmd_predicted: split File Regex\n",
      "\n",
      "invocation: tar all the regular java files to myfile.tar\n",
      "cmd: find . -type f -name \"*.java\" | xargs tar cvf myfile.tar\n",
      "text_cleaned: tar regular java to myfil tar\n",
      "cmd_cleaned: find Path -type f -name Regex | xargs -I {} tar -c -v -f File {}\n",
      "cmd_predicted: find Path -type f -name Regex -exec tar -r -\n",
      "\n",
      "invocation: split processed content of the file inout_file into pieces per 2000000 named as \"out-prefix-nnn\"\n",
      "cmd: sed 's/\\(.....\\)\\(.....\\)/\\1\\n\\2/' input_file | split -l 2000000 - out-prefix-\n",
      "text_cleaned: split process content of inoutfil into piec per name as out prefix nnn\n",
      "cmd_cleaned: sed Program File | split -l Quantity File Regex\n",
      "cmd_predicted: split --lines Quantity File\n",
      "\n",
      "invocation: split file \"$file into pieces named with 5 character suffix\n",
      "cmd: split -a 5 $file\n",
      "text_cleaned: split into piec name with charact suffix\n",
      "cmd_cleaned: split -a Quantity File\n",
      "cmd_predicted: split --lines Quantity --suffix Quanti\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_eval(valid_data, valid_ds, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация команд (2 балла)\n",
    "\n",
    "**Задание**. Реализуйте алгоритм beam-search в классе BeamSearchGenerator ниже. Ваша реализация должна поддерживать задание температуры софтмакса. Выходы модели, полученные на предыдущих итерациях, необходимо кэшировать для повышения скорости алгоритма. Вместо подсчёта произведения любых вероятностей необходимо считать сумму их логарифмов.\n",
    "\n",
    "Алгоритм должен возвращать список пар из получившихся выходных последовательностей и логарифмов их вероятностей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchGenerator:\n",
    "    def __init__(\n",
    "            self, pad_id, eos_id, bos_id,\n",
    "            max_length=20, beam_width=5, temperature=1,\n",
    "            device='cuda',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        pad_id : int\n",
    "        eos_id : int\n",
    "        bos_id : int\n",
    "        max_length : int\n",
    "            Maximum length of output sequence\n",
    "        beam_width : int\n",
    "            Width of the beam\n",
    "        temperature : float\n",
    "            Softmax temperature\n",
    "        device : torch.device\n",
    "            Your model device\n",
    "        \"\"\"\n",
    "        self.pad_id = pad_id\n",
    "        self.eos_id = eos_id\n",
    "        self.bos_id = bos_id\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.beam_width = beam_width\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def get_result(self, model, input_text_tokens):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : TextToBashModel\n",
    "        input_text_tokens : torch.tensor\n",
    "            One object input tensor\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте на нескольких примерах работу вашего алгоритма. Если всё реализовано правильно, то как минимум на трёх примерах из 5 всё должно работать правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_enginge = BeamSearchGenerator(\n",
    "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "    max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "    temperature=1, device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print()\n",
    "        print('text:', valid_data.invocation.iloc[i])\n",
    "        print('true:', valid_data.cmd.iloc[i])\n",
    "        print('true cleaned:', valid_data.cmd_cleaned.iloc[i])\n",
    "\n",
    "        src = valid_ds.src[i]\n",
    "        pred = beam_search_enginge.get_result(model, src)\n",
    "        \n",
    "        scores = []\n",
    "        for x, proba in pred:\n",
    "            pred_cmd = cmd_tokenizer.decode(list(map(int, x)))\n",
    "            score = compute_metric(pred_cmd, 1, valid_data.cmd.iloc[i])\n",
    "            scores.append(score)\n",
    "            print(pred_cmd, proba)\n",
    "        print(max(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Дополните функцию для подсчёта качества. Посчитайте качество вашей модели на валидационном и тестовых датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_scores(model, df, beam_engine):\n",
    "    all_scores = []\n",
    "\n",
    "    for i, (text, target_cmd) in enumerate(zip(df.text_cleaned.values, df.cmd.values)):\n",
    "        input_tokens = ## YOUR CODE HERE ##\n",
    "        predictions = beam_search_enginge.get_result(model, input_tokens)\n",
    "        \n",
    "        # get only 5 top results\n",
    "        predictions = predictions[:5]\n",
    "        object_scores = []\n",
    "        for output_tokens, proba in predictions:\n",
    "            output_cmd = cmd_tokenizer.decode(list(map(int, output_tokens)))\n",
    "            score = compute_metric(output_cmd, 1, target_cmd)\n",
    "            object_scores.append(score)\n",
    "        \n",
    "        all_scores.append(max(object_scores))\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы всё реализовали правильно, подобрали параметры BeamSearch то ваш средний скор на валидации должен быть >= 0.25, а скор на `handcrafted` части теста >= 0.13. На `mined` части датасета скор может быть низкий, т.к. некоторых команд из датасета нет в обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение модели (4 балла)\n",
    "\n",
    "Вы реализовали бейзлайн, пришло время улучшить качество модели. Т.к. это последнее задание, мы не будем предлагать конкретные шаги, а только дадим несколько советов.\n",
    "\n",
    "1. Большой источник информации о работе командной строке — её документация, man. Один из способов улучшения модели - использование мана для генерации новых примеров. Структурированный ман можно найти по ссылке https://github.com/IBM/clai/blob/nlc2cmd/docs/manpage-data.md.\n",
    "2. Ещё один способ улучшить модель, разделить предсказание утилит и флагов. Т.к. задача предсказания утилит более важная, вы можете натренировать модель, которая предсказывает последовательность утилит, а затем к каждой утилите генерировать флаги.\n",
    "3. Можно аугментировать данные, чтобы увеличить выборку.\n",
    "4. Можно в качество входа подавать не только текстовый запрос, но и описание из мана. Т.к. всё описание достаточно большое, нужно сделать дополнительную модель, которая будет выбирать команды, для которых нужно вытащить описание.\n",
    "5. Найти дополнительные данные, улучшающие обучение\n",
    "6. Как всегда можно просто сделать больше слоёв, увеличить размер скрытого слоя и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От вас ожидается скор на `mined` >= 0 при скоре на `handrafted` >= 0.16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусные баллы (до 3 баллов)\n",
    "\n",
    "При существенном улучшении качества будут назначаться бонусные баллы. На тестовых датасетах реально выбить качество >= 0.3 на каждом, но усилий потребуется немало..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
